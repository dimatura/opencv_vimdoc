*cv-features2d.txt*	For Vim 0.0	Thu May 19 18:07:34 2011

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Feature detection and description~

===========================================================================
*cv-FAST*

void FAST(const Mat& image, vector<KeyPoint>& keypoints, int threshold, bool
    nonmaxSupression=true)~

    Detects corners using FAST algorithm by E. Rosten (‘’Machine learning for
    high-speed corner detection’‘, 2006).

                  • image – The image. Keypoints (corners) will be detected on
                    this.
                  • keypoints – Keypoints detected on the image.
    Parameters:   • threshold – Threshold on difference between intensity of
                    center pixel and pixels on circle around this pixel. See
                    description of the algorithm.
                  • nonmaxSupression – If it is true then non-maximum
                    supression will be applied to detected corners (keypoints).

===========================================================================
*cv-MSER*

===========================================================================


Maximally-Stable Extremal Region Extractor

class MSER : public CvMSERParams
{
public:
    // default constructor
    MSER();
    // constructor that initializes all the algorithm parameters
    MSER( int _delta, int _min_area, int _max_area,
          float _max_variation, float _min_diversity,
          int _max_evolution, double _area_threshold,
          double _min_margin, int _edge_blur_size );
    // runs the extractor on the specified image; returns the MSERs,
    // each encoded as a contour (vector<Point>, see findContours)
    // the optional mask marks the area where MSERs are searched for
    void operator()( const Mat& image, vector<vector<Point> >& msers, const Mat& mask ) const;
};

The class encapsulates all the parameters of MSER (see http://en.wikipedia.org/
wiki/Maximally_stable_extremal_regions ) extraction algorithm.

===========================================================================
*cv-StarDetector*

===========================================================================


Implements Star keypoint detector

class StarDetector : CvStarDetectorParams
{
public:
    // default constructor
    StarDetector();
    // the full constructor initialized all the algorithm parameters:
    // maxSize - maximum size of the features. The following
    //      values of the parameter are supported:
    //      4, 6, 8, 11, 12, 16, 22, 23, 32, 45, 46, 64, 90, 128
    // responseThreshold - threshold for the approximated laplacian,
    //      used to eliminate weak features. The larger it is,
    //      the less features will be retrieved
    // lineThresholdProjected - another threshold for the laplacian to
    //      eliminate edges
    // lineThresholdBinarized - another threshold for the feature
    //      size to eliminate edges.
    // The larger the 2 threshold, the more points you get.
    StarDetector(int maxSize, int responseThreshold,
                 int lineThresholdProjected,
                 int lineThresholdBinarized,
                 int suppressNonmaxSize);

    // finds keypoints in an image
    void operator()(const Mat& image, vector<KeyPoint>& keypoints) const;
};

The class implements a modified version of CenSurE keypoint detector described
in Agrawal08

===========================================================================
*cv-SIFT*

===========================================================================


Class for extracting keypoints and computing descriptors using approach named
Scale Invariant Feature Transform (SIFT).

class CV_EXPORTS SIFT
{
public:
    struct CommonParams
    {
        static const int DEFAULT_NOCTAVES = 4;
        static const int DEFAULT_NOCTAVE_LAYERS = 3;
        static const int DEFAULT_FIRST_OCTAVE = -1;
        enum{ FIRST_ANGLE = 0, AVERAGE_ANGLE = 1 };

        CommonParams();
        CommonParams( int _nOctaves, int _nOctaveLayers, int _firstOctave,
                                          int _angleMode );
        int nOctaves, nOctaveLayers, firstOctave;
        int angleMode;
    };

    struct DetectorParams
    {
        static double GET_DEFAULT_THRESHOLD()
          { return 0.04 / SIFT::CommonParams::DEFAULT_NOCTAVE_LAYERS / 2.0; }
        static double GET_DEFAULT_EDGE_THRESHOLD() { return 10.0; }

        DetectorParams();
        DetectorParams( double _threshold, double _edgeThreshold );
        double threshold, edgeThreshold;
    };

    struct DescriptorParams
    {
        static double GET_DEFAULT_MAGNIFICATION() { return 3.0; }
        static const bool DEFAULT_IS_NORMALIZE = true;
        static const int DESCRIPTOR_SIZE = 128;

        DescriptorParams();
        DescriptorParams( double _magnification, bool _isNormalize,
                                                  bool _recalculateAngles );
        double magnification;
        bool isNormalize;
        bool recalculateAngles;
    };

    SIFT();
    //! sift-detector constructor
    SIFT( double _threshold, double _edgeThreshold,
          int _nOctaves=CommonParams::DEFAULT_NOCTAVES,
          int _nOctaveLayers=CommonParams::DEFAULT_NOCTAVE_LAYERS,
          int _firstOctave=CommonParams::DEFAULT_FIRST_OCTAVE,
          int _angleMode=CommonParams::FIRST_ANGLE );
    //! sift-descriptor constructor
    SIFT( double _magnification, bool _isNormalize=true,
          bool _recalculateAngles = true,
          int _nOctaves=CommonParams::DEFAULT_NOCTAVES,
          int _nOctaveLayers=CommonParams::DEFAULT_NOCTAVE_LAYERS,
          int _firstOctave=CommonParams::DEFAULT_FIRST_OCTAVE,
          int _angleMode=CommonParams::FIRST_ANGLE );
    SIFT( const CommonParams& _commParams,
          const DetectorParams& _detectorParams = DetectorParams(),
          const DescriptorParams& _descriptorParams = DescriptorParams() );

    //! returns the descriptor size in floats (128)
    int descriptorSize() const { return DescriptorParams::DESCRIPTOR_SIZE; }
    //! finds the keypoints using SIFT algorithm
    void operator()(const Mat& img, const Mat& mask,
                    vector<KeyPoint>& keypoints) const;
    //! finds the keypoints and computes descriptors for them using SIFT algorithm.
    //! Optionally it can compute descriptors for the user-provided keypoints
    void operator()(const Mat& img, const Mat& mask,
                    vector<KeyPoint>& keypoints,
                    Mat& descriptors,
                    bool useProvidedKeypoints=false) const;

    CommonParams getCommonParams () const { return commParams; }
    DetectorParams getDetectorParams () const { return detectorParams; }
    DescriptorParams getDescriptorParams () const { return descriptorParams; }
protected:
    ...
};

===========================================================================
*cv-SURF*

===========================================================================


Class for extracting Speeded Up Robust Features from an image.

class SURF : public CvSURFParams
{
public:
    // default constructor
    SURF();
    // constructor that initializes all the algorithm parameters
    SURF(double _hessianThreshold, int _nOctaves=4,
         int _nOctaveLayers=2, bool _extended=false);
    // returns the number of elements in each descriptor (64 or 128)
    int descriptorSize() const;
    // detects keypoints using fast multi-scale Hessian detector
    void operator()(const Mat& img, const Mat& mask,
                    vector<KeyPoint>& keypoints) const;
    // detects keypoints and computes the SURF descriptors for them;
    // output vector "descriptors" stores elements of descriptors and has size
    // equal descriptorSize()*keypoints.size() as each descriptor is
    // descriptorSize() elements of this vector.
    void operator()(const Mat& img, const Mat& mask,
                    vector<KeyPoint>& keypoints,
                    vector<float>& descriptors,
                    bool useProvidedKeypoints=false) const;
};

The class SURF implements Speeded Up Robust Features descriptor Bay06 . There
is fast multi-scale Hessian keypoint detector that can be used to find the
keypoints (which is the default option), but the descriptors can be also
computed for the user-specified keypoints. The function can be used for object
tracking and localization, image stitching etc. See the find_obj.cpp demo in
OpenCV samples directory.

===========================================================================
*cv-RandomizedTree*

===========================================================================


The class contains base structure for RTreeClassifier

class CV_EXPORTS RandomizedTree
{
public:
        friend class RTreeClassifier;

        RandomizedTree();
        ~RandomizedTree();

        void train(std::vector<BaseKeypoint> const& base_set,
                 cv::RNG &rng, int depth, int views,
                 size_t reduced_num_dim, int num_quant_bits);
        void train(std::vector<BaseKeypoint> const& base_set,
                 cv::RNG &rng, PatchGenerator &make_patch, int depth,
                 int views, size_t reduced_num_dim, int num_quant_bits);

        // following two funcs are EXPERIMENTAL
        //(do not use unless you know exactly what you do)
        static void quantizeVector(float *vec, int dim, int N, float bnds[2],
                 int clamp_mode=0);
        static void quantizeVector(float *src, int dim, int N, float bnds[2],
                 uchar *dst);

        // patch_data must be a 32x32 array (no row padding)
        float* getPosterior(uchar* patch_data);
        const float* getPosterior(uchar* patch_data) const;
        uchar* getPosterior2(uchar* patch_data);

        void read(const char* file_name, int num_quant_bits);
        void read(std::istream &is, int num_quant_bits);
        void write(const char* file_name) const;
        void write(std::ostream &os) const;

        int classes() { return classes_; }
        int depth() { return depth_; }

        void discardFloatPosteriors() { freePosteriors(1); }

        inline void applyQuantization(int num_quant_bits)
                 { makePosteriors2(num_quant_bits); }

private:
        int classes_;
        int depth_;
        int num_leaves_;
        std::vector<RTreeNode> nodes_;
        float **posteriors_;        // 16-bytes aligned posteriors
        uchar **posteriors2_;     // 16-bytes aligned posteriors
        std::vector<int> leaf_counts_;

        void createNodes(int num_nodes, cv::RNG &rng);
        void allocPosteriorsAligned(int num_leaves, int num_classes);
        void freePosteriors(int which);
                 // which: 1=posteriors_, 2=posteriors2_, 3=both
        void init(int classes, int depth, cv::RNG &rng);
        void addExample(int class_id, uchar* patch_data);
        void finalize(size_t reduced_num_dim, int num_quant_bits);
        int getIndex(uchar* patch_data) const;
        inline float* getPosteriorByIndex(int index);
        inline uchar* getPosteriorByIndex2(int index);
        inline const float* getPosteriorByIndex(int index) const;
        void convertPosteriorsToChar();
        void makePosteriors2(int num_quant_bits);
        void compressLeaves(size_t reduced_num_dim);
        void estimateQuantPercForPosteriors(float perc[2]);
};

===========================================================================
*cv-RandomizedTree::train*

void train(std::vector<BaseKeypoint> const& base_set, cv::RNG &rng,
    PatchGenerator &make_patch, int depth, int views, size_t reduced_num_dim,
    int num_quant_bits)¶
    Trains a randomized tree using input set of keypoints

void train(std::vector<BaseKeypoint> const& base_set, cv::RNG &rng,
    PatchGenerator &make_patch, int depth, int views, size_t reduced_num_dim,
    int num_quant_bits)

    {Vector of BaseKeypoint type. Contains keypoints from the image are used
    for training} {Random numbers generator is used for training} {Patch
    generator is used for training} {Maximum tree depth}

    {Number of dimensions are used in compressed signature} {Number of bits are
    used for quantization}

===========================================================================
*cv-RandomizedTree::read*

read(const char* file_name, int num_quant_bits)¶
    Reads pre-saved randomized tree from file or stream

read(std::istream &is, int num_quant_bits)

                  • file_name – Filename of file contains randomized tree data
    Parameters:   • is – Input stream associated with file contains randomized
                    tree data

    {Number of bits are used for quantization}

===========================================================================
*cv-RandomizedTree::write*

void write(const char* file_name) const¶
    Writes current randomized tree to a file or stream

void write(std::ostream &os) const

                  • file_name – Filename of file where randomized tree data
    Parameters:     will be stored
                  • is – Output stream associated with file where randomized
                    tree data will be stored

===========================================================================
*cv-RandomizedTree::applyQuantization*

void applyQuantization(int num_quant_bits)~

    Applies quantization to the current randomized tree

    {Number of bits are used for quantization}

===========================================================================
*cv-RTreeNode*

===========================================================================


The class contains base structure for RandomizedTree

struct RTreeNode
{
        short offset1, offset2;

        RTreeNode() {}

        RTreeNode(uchar x1, uchar y1, uchar x2, uchar y2)
                : offset1(y1*PATCH_SIZE + x1),
                offset2(y2*PATCH_SIZE + x2)
        {}

        //! Left child on 0, right child on 1
        inline bool operator() (uchar* patch_data) const
        {
                return patch_data[offset1] > patch_data[offset2];
        }
};

===========================================================================
*cv-RTreeClassifier*

===========================================================================


The class contains RTreeClassifier . It represents calonder descriptor which
was originally introduced by Michael Calonder

class CV_EXPORTS RTreeClassifier
{
public:
        static const int DEFAULT_TREES = 48;
        static const size_t DEFAULT_NUM_QUANT_BITS = 4;

        RTreeClassifier();

        void train(std::vector<BaseKeypoint> const& base_set,
                cv::RNG &rng,
                int num_trees = RTreeClassifier::DEFAULT_TREES,
                int depth = DEFAULT_DEPTH,
                int views = DEFAULT_VIEWS,
                size_t reduced_num_dim = DEFAULT_REDUCED_NUM_DIM,
                int num_quant_bits = DEFAULT_NUM_QUANT_BITS,
                         bool print_status = true);
        void train(std::vector<BaseKeypoint> const& base_set,
                cv::RNG &rng,
                PatchGenerator &make_patch,
                int num_trees = RTreeClassifier::DEFAULT_TREES,
                int depth = DEFAULT_DEPTH,
                int views = DEFAULT_VIEWS,
                size_t reduced_num_dim = DEFAULT_REDUCED_NUM_DIM,
                int num_quant_bits = DEFAULT_NUM_QUANT_BITS,
                 bool print_status = true);

        // sig must point to a memory block of at least
        //classes()*sizeof(float|uchar) bytes
        void getSignature(IplImage *patch, uchar *sig);
        void getSignature(IplImage *patch, float *sig);
        void getSparseSignature(IplImage *patch, float *sig,
                 float thresh);

        static int countNonZeroElements(float *vec, int n, double tol=1e-10);
        static inline void safeSignatureAlloc(uchar **sig, int num_sig=1,
                        int sig_len=176);
        static inline uchar* safeSignatureAlloc(int num_sig=1,
                         int sig_len=176);

        inline int classes() { return classes_; }
        inline int original_num_classes()
                 { return original_num_classes_; }

        void setQuantization(int num_quant_bits);
        void discardFloatPosteriors();

        void read(const char* file_name);
        void read(std::istream &is);
        void write(const char* file_name) const;
        void write(std::ostream &os) const;

        std::vector<RandomizedTree> trees_;

private:
        int classes_;
        int num_quant_bits_;
        uchar **posteriors_;
        ushort *ptemp_;
        int original_num_classes_;
        bool keep_floats_;
};

===========================================================================
*cv-RTreeClassifier::train*

void train(std::vector<BaseKeypoint> const& base_set, cv::RNG &rng, int
    num_trees = RTreeClassifier::DEFAULT_TREES, int depth = DEFAULT_DEPTH, int
    views = DEFAULT_VIEWS, size_t reduced_num_dim = DEFAULT_REDUCED_NUM_DIM,
    int num_quant_bits = DEFAULT_NUM_QUANT_BITS, bool print_status = true)
    Trains a randomized tree classificator using input set of keypoints

void train(std::vector<BaseKeypoint> const& base_set, cv::RNG &rng,
    PatchGenerator &make_patch, int num_trees = RTreeClassifier::DEFAULT_TREES,
    int depth = DEFAULT_DEPTH, int views = DEFAULT_VIEWS, size_t
    reduced_num_dim = DEFAULT_REDUCED_NUM_DIM, int num_quant_bits =
    DEFAULT_NUM_QUANT_BITS, bool print_status = true)

    {Vector of BaseKeypoint type. Contains keypoints from the image are used
    for training} {Random numbers generator is used for training} {Patch
    generator is used for training} {Number of randomized trees used in
    RTreeClassificator} {Maximum tree depth}

    {Number of dimensions are used in compressed signature} {Number of bits are
    used for quantization} {Print current status of training on the console}

===========================================================================
*cv-RTreeClassifier::getSignature*

void getSignature(IplImage *patch, uchar *sig)¶
    Returns signature for image patch

void getSignature(IplImage *patch, float *sig)
    {Image patch to calculate signature for} {Output signature (array dimension
    is reduced_num_dim) }

===========================================================================
*cv-RTreeClassifier::getSparseSignature*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

void getSparseSignature(IplImage *patch, float *sig, float thresh)~

    The function is simular to getSignaturebut uses the threshold for removing
    all signature elements less than the threshold. So that the signature is
    compressed

    {Image patch to calculate signature for} {Output signature (array dimension
    is reduced_num_dim) } {The threshold that is used for compressing the
    signature}

===========================================================================
*cv-RTreeClassifier::countNonZeroElements*

static int countNonZeroElements(float *vec, int n, double tol=1e-10)~

    The function returns the number of non-zero elements in the input array.

    Parameters:   • vec – Input vector contains float elements
                  • n – Input vector size

    {The threshold used for elements counting. We take all elements are less
    than tol as zero elements}

===========================================================================
*cv-RTreeClassifier::read*

read(const char* file_name)
    Reads pre-saved RTreeClassifier from file or stream

read(std::istream &is)

                  • file_name – Filename of file contains randomized tree data
    Parameters:   • is – Input stream associated with file contains randomized
                    tree data

===========================================================================
*cv-RTreeClassifier::write*

void write(const char* file_name) const
    Writes current RTreeClassifier to a file or stream

void write(std::ostream &os) const

                  • file_name – Filename of file where randomized tree data
    Parameters:     will be stored
                  • is – Output stream associated with file where randomized
                    tree data will be stored

===========================================================================
*cv-RTreeClassifier::setQuantization*

void setQuantization(int num_quant_bits)~

    Applies quantization to the current randomized tree

    {Number of bits are used for quantization}

Below there is an example of RTreeClassifier usage for feature matching. There
are test and train images and we extract features from both with SURF. Output
is best\_corr and best\_corr\_idx arrays which keep the best probabilities and
corresponding features indexes for every train feature.

CvMemStorage* storage = cvCreateMemStorage(0);
CvSeq *objectKeypoints = 0, *objectDescriptors = 0;
CvSeq *imageKeypoints = 0, *imageDescriptors = 0;
CvSURFParams params = cvSURFParams(500, 1);
cvExtractSURF( test_image, 0, &imageKeypoints, &imageDescriptors,
                 storage, params );
cvExtractSURF( train_image, 0, &objectKeypoints, &objectDescriptors,
                 storage, params );

cv::RTreeClassifier detector;
int patch_width = cv::PATCH_SIZE;
iint patch_height = cv::PATCH_SIZE;
vector<cv::BaseKeypoint> base_set;
int i=0;
CvSURFPoint* point;
for (i=0;i<(n_points > 0 ? n_points : objectKeypoints->total);i++)
{
        point=(CvSURFPoint*)cvGetSeqElem(objectKeypoints,i);
        base_set.push_back(
                cv::BaseKeypoint(point->pt.x,point->pt.y,train_image));
}

        //Detector training
 cv::RNG rng( cvGetTickCount() );
cv::PatchGenerator gen(0,255,2,false,0.7,1.3,-CV_PI/3,CV_PI/3,
                        -CV_PI/3,CV_PI/3);

printf("RTree Classifier training...n");
detector.train(base_set,rng,gen,24,cv::DEFAULT_DEPTH,2000,
        (int)base_set.size(), detector.DEFAULT_NUM_QUANT_BITS);
printf("Donen");

float* signature = new float[detector.original_num_classes()];
float* best_corr;
int* best_corr_idx;
if (imageKeypoints->total > 0)
{
        best_corr = new float[imageKeypoints->total];
        best_corr_idx = new int[imageKeypoints->total];
}

for(i=0; i < imageKeypoints->total; i++)
{
        point=(CvSURFPoint*)cvGetSeqElem(imageKeypoints,i);
        int part_idx = -1;
        float prob = 0.0f;

        CvRect roi = cvRect((int)(point->pt.x) - patch_width/2,
                (int)(point->pt.y) - patch_height/2,
                 patch_width, patch_height);
        cvSetImageROI(test_image, roi);
        roi = cvGetImageROI(test_image);
        if(roi.width != patch_width || roi.height != patch_height)
        {
                best_corr_idx[i] = part_idx;
                best_corr[i] = prob;
        }
        else
        {
                cvSetImageROI(test_image, roi);
                IplImage* roi_image =
                         cvCreateImage(cvSize(roi.width, roi.height),
                         test_image->depth, test_image->nChannels);
                cvCopy(test_image,roi_image);

                detector.getSignature(roi_image, signature);
                for (int j = 0; j< detector.original_num_classes();j++)
                {
                        if (prob < signature[j])
                        {
                                part_idx = j;
                                prob = signature[j];
                        }
                }

                best_corr_idx[i] = part_idx;
                best_corr[i] = prob;


                if (roi_image)
                        cvReleaseImage(&roi_image);
        }
        cvResetImageROI(test_image);
}

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Feature detection and description
      □ cv::FAST
      □ MSER
      □ StarDetector
      □ SIFT
      □ SURF
      □ RandomizedTree
      □ cv::RandomizedTree::train
      □ cv::RandomizedTree::read
      □ cv::RandomizedTree::write
      □ cv::RandomizedTree::applyQuantization
      □ RTreeNode
      □ RTreeClassifier
      □ cv::RTreeClassifier::train
      □ cv::RTreeClassifier::getSignature
      □ cv::RTreeClassifier::getSparseSignature
      □ cv::RTreeClassifier::countNonZeroElements
      □ cv::RTreeClassifier::read
      □ cv::RTreeClassifier::write
      □ cv::RTreeClassifier::setQuantization

Previous topic

features2d. Feature Detection and Descriptor Extraction

Next topic

Common Interfaces of Feature Detectors

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Common Interfaces of Feature Detectors~

Feature detectors in OpenCV have wrappers with common interface that enables to
switch easily between different algorithms solving the same problem. All
objects that implement keypoint detectors inherit FeatureDetector() interface.

===========================================================================
*cv-KeyPoint*

===========================================================================


Data structure for salient point detectors.

class KeyPoint
{
public:
    // the default constructor
    KeyPoint() : pt(0,0), size(0), angle(-1), response(0), octave(0),
                 class_id(-1) {}
    // the full constructor
    KeyPoint(Point2f _pt, float _size, float _angle=-1,
            float _response=0, int _octave=0, int _class_id=-1)
            : pt(_pt), size(_size), angle(_angle), response(_response),
              octave(_octave), class_id(_class_id) {}
    // another form of the full constructor
    KeyPoint(float x, float y, float _size, float _angle=-1,
            float _response=0, int _octave=0, int _class_id=-1)
            : pt(x, y), size(_size), angle(_angle), response(_response),
              octave(_octave), class_id(_class_id) {}
    // converts vector of keypoints to vector of points
    static void convert(const std::vector<KeyPoint>& keypoints,
                        std::vector<Point2f>& points2f,
                        const std::vector<int>& keypointIndexes=std::vector<int>());
    // converts vector of points to the vector of keypoints, where each
    // keypoint is assigned the same size and the same orientation
    static void convert(const std::vector<Point2f>& points2f,
                        std::vector<KeyPoint>& keypoints,
                        float size=1, float response=1, int octave=0,
                        int class_id=-1);

    // computes overlap for pair of keypoints;
    // overlap is a ratio between area of keypoint regions intersection and
    // area of keypoint regions union (now keypoint region is circle)
    static float overlap(const KeyPoint& kp1, const KeyPoint& kp2);

    Point2f pt; // coordinates of the keypoints
    float size; // diameter of the meaningfull keypoint neighborhood
    float angle; // computed orientation of the keypoint (-1 if not applicable)
    float response; // the response by which the most strong keypoints
                    // have been selected. Can be used for the further sorting
                    // or subsampling
    int octave; // octave (pyramid layer) from which the keypoint has been extracted
    int class_id; // object class (if the keypoints need to be clustered by
                  // an object they belong to)
};

// writes vector of keypoints to the file storage
void write(FileStorage& fs, const string& name, const vector<KeyPoint>& keypoints);
// reads vector of keypoints from the specified file storage node
void read(const FileNode& node, CV_OUT vector<KeyPoint>& keypoints);

===========================================================================
*cv-FeatureDetector*

===========================================================================


Abstract base class for 2D image feature detectors.

class CV_EXPORTS FeatureDetector
{
public:
    virtual ~FeatureDetector();

    void detect( const Mat& image, vector<KeyPoint>& keypoints,
                 const Mat& mask=Mat() ) const;

    void detect( const vector<Mat>& images,
                 vector<vector<KeyPoint> >& keypoints,
                 const vector<Mat>& masks=vector<Mat>() ) const;

    virtual void read(const FileNode&);
    virtual void write(FileStorage&) const;

    static Ptr<FeatureDetector> create( const string& detectorType );

protected:
...
};

===========================================================================
*cv-FeatureDetector::detect*

void FeatureDetector::detect(const Mat& image, vector<KeyPoint>& keypoints,
    const Mat& mask=Mat()) const~

    Detect keypoints in an image (first variant) or image set (second variant).

                  • image – The image.
                  • keypoints – The detected keypoints.
    Parameters:   • mask – Mask specifying where to look for keypoints
                    (optional). Must be a char matrix with non-zero values in
                    the region of interest.

void FeatureDetector::detect(const vector<Mat>& images, vector<vector<KeyPoint>
    >& keypoints, const vector<Mat>& masks=vector<Mat>()) const
      □ images Images set.

      □ keypoints Collection of keypoints detected in an input images.
        keypoints[i] is a set of keypoints detected in an images[i].

      □ 
        masks Masks for each input image specifying where to look for keypoints
            (optional). masks[i] is a mask for images[i].

            Each element of masks vector must be a char matrix with non-zero
            values in the region of interest.

===========================================================================
*cv-FeatureDetector::read*

void FeatureDetector::read(const FileNode& fn)~

    Read feature detector object from file node.

    Parameter: fn – File node from which detector will be read.

===========================================================================
*cv-FeatureDetector::write*

void FeatureDetector::write(FileStorage& fs) const~

    Write feature detector object to file storage.

    Parameter: fs – File storage in which detector will be written.

===========================================================================
*cv-FeatureDetector::create*

FeatureDetector()

..

    :param :

~

FastFeatureDetector() ```` StarFeatureDetector() ```` SiftFeatureDetector() ``
`` SurfFeatureDetector() ```` MserFeatureDetector() ```` GfttFeatureDetector()
```` HarrisFeatureDetector() ```` GridAdaptedFeatureDetector() ````
PyramidAdaptedFeatureDetector() ```` ````

===========================================================================
*cv-FastFeatureDetector*

===========================================================================


Wrapping class for feature detection using FAST() method.

class FastFeatureDetector : public FeatureDetector
{
public:
    FastFeatureDetector( int threshold=1, bool nonmaxSuppression=true );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-GoodFeaturesToTrackDetector*

===========================================================================


Wrapping class for feature detection using goodFeaturesToTrack() function.

class GoodFeaturesToTrackDetector : public FeatureDetector
{
public:
    class Params
    {
    public:
        Params( int maxCorners=1000, double qualityLevel=0.01,
                double minDistance=1., int blockSize=3,
                bool useHarrisDetector=false, double k=0.04 );
        void read( const FileNode& fn );
        void write( FileStorage& fs ) const;

        int maxCorners;
        double qualityLevel;
        double minDistance;
        int blockSize;
        bool useHarrisDetector;
        double k;
    };

    GoodFeaturesToTrackDetector( const GoodFeaturesToTrackDetector::Params& params=
                                            GoodFeaturesToTrackDetector::Params() );
    GoodFeaturesToTrackDetector( int maxCorners, double qualityLevel,
                                 double minDistance, int blockSize=3,
                                 bool useHarrisDetector=false, double k=0.04 );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-MserFeatureDetector*

===========================================================================


Wrapping class for feature detection using MSER() class.

class MserFeatureDetector : public FeatureDetector
{
public:
    MserFeatureDetector( CvMSERParams params=cvMSERParams() );
    MserFeatureDetector( int delta, int minArea, int maxArea,
                         double maxVariation, double minDiversity,
                         int maxEvolution, double areaThreshold,
                         double minMargin, int edgeBlurSize );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-StarFeatureDetector*

===========================================================================


Wrapping class for feature detection using StarDetector() class.

class StarFeatureDetector : public FeatureDetector
{
public:
    StarFeatureDetector( int maxSize=16, int responseThreshold=30,
                         int lineThresholdProjected = 10,
                         int lineThresholdBinarized=8, int suppressNonmaxSize=5 );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-SiftFeatureDetector*

===========================================================================


Wrapping class for feature detection using SIFT() class.

class SiftFeatureDetector : public FeatureDetector
{
public:
    SiftFeatureDetector(
        const SIFT::DetectorParams& detectorParams=SIFT::DetectorParams(),
        const SIFT::CommonParams& commonParams=SIFT::CommonParams() );
    SiftFeatureDetector( double threshold, double edgeThreshold,
                         int nOctaves=SIFT::CommonParams::DEFAULT_NOCTAVES,
                         int nOctaveLayers=SIFT::CommonParams::DEFAULT_NOCTAVE_LAYERS,
                         int firstOctave=SIFT::CommonParams::DEFAULT_FIRST_OCTAVE,
                         int angleMode=SIFT::CommonParams::FIRST_ANGLE );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-SurfFeatureDetector*

===========================================================================


Wrapping class for feature detection using SURF() class.

class SurfFeatureDetector : public FeatureDetector
{
public:
    SurfFeatureDetector( double hessianThreshold = 400., int octaves = 3,
                         int octaveLayers = 4 );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-GridAdaptedFeatureDetector*

===========================================================================


Adapts a detector to partition the source image into a grid and detect points
in each cell.

class GridAdaptedFeatureDetector : public FeatureDetector
{
public:
    /*
     * detector            Detector that will be adapted.
     * maxTotalKeypoints   Maximum count of keypoints detected on the image.
     *                     Only the strongest keypoints will be keeped.
     * gridRows            Grid rows count.
     * gridCols            Grid column count.
     */
    GridAdaptedFeatureDetector( const Ptr<FeatureDetector>& detector,
                                int maxTotalKeypoints, int gridRows=4,
                                int gridCols=4 );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-PyramidAdaptedFeatureDetector*

===========================================================================


Adapts a detector to detect points over multiple levels of a Gaussian pyramid.
Useful for detectors that are not inherently scaled.

class PyramidAdaptedFeatureDetector : public FeatureDetector
{
public:
    PyramidAdaptedFeatureDetector( const Ptr<FeatureDetector>& detector,
                                   int levels=2 );
    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;
protected:
    ...
};

===========================================================================
*cv-DynamicAdaptedFeatureDetector*

===========================================================================


An adaptively adjusting detector that iteratively detects until the desired
number of features are found.

If the detector is persisted, it will “remember” the parameters used on the
last detection. In this way, the detector may be used for consistent numbers of
keypoints in a sets of images that are temporally related such as video streams
or panorama series.

The DynamicAdaptedFeatureDetector uses another detector such as FAST or SURF to
do the dirty work, with the help of an AdjusterAdapter. After a detection, and
an unsatisfactory number of features are detected, the AdjusterAdapter will
adjust the detection parameters so that the next detection will result in more
or less features. This is repeated until either the number of desired features
are found or the parameters are maxed out.

Adapters can easily be implemented for any detector via the AdjusterAdapter
interface.

Beware that this is not thread safe - as the adjustment of parameters breaks
the const of the detection routine...

Here is a sample of how to create a DynamicAdaptedFeatureDetector.

//sample usage:
//will create a detector that attempts to find
//100 - 110 FAST Keypoints, and will at most run
//FAST feature detection 10 times until that
//number of keypoints are found
Ptr<FeatureDetector> detector(new DynamicAdaptedFeatureDetector (100, 110, 10,
                              new FastAdjuster(20,true)));

class DynamicAdaptedFeatureDetector: public FeatureDetector
{
public:
    DynamicAdaptedFeatureDetector( const Ptr<AdjusterAdapter>& adjaster,
        int min_features=400, int max_features=500, int max_iters=5 );
    ...
};

===========================================================================
*cv-DynamicAdaptedFeatureDetector::DynamicAdaptedFeatureDetector*

DynamicAdaptedFeatureDetector::DynamicAdaptedFeatureDetector(const Ptr<
    AdjusterAdapter>& adjaster, int min_features, int max_features, int
    max_iters)~

    DynamicAdaptedFeatureDetector constructor.

                  • adjaster – An AdjusterAdapter() that will do the detection
                    and parameter adjustment
                  • min_features – This minimum desired number features.
                  • max_features – The maximum desired number of features.
    Parameters:   • max_iters – The maximum number of times to try to adjust
                    the feature detector parameters. For the FastAdjuster()
                    this number can be high, but with Star or Surf, many
                    iterations can get time consuming. At each iteration the
                    detector is rerun, so keep this in mind when choosing this
                    value.

===========================================================================
*cv-AdjusterAdapter*

===========================================================================


A feature detector parameter adjuster interface, this is used by the
DynamicAdaptedFeatureDetector() and is a wrapper for FeatureDetecto() r that
allow them to be adjusted after a detection.

See FastAdjuster() , StarAdjuster() , SurfAdjuster() for concrete
implementations.

class AdjusterAdapter: public FeatureDetector
{
public:
        virtual ~AdjusterAdapter() {}
        virtual void tooFew(int min, int n_detected) = 0;
        virtual void tooMany(int max, int n_detected) = 0;
        virtual bool good() const = 0;
};

===========================================================================
*cv-AdjusterAdapter::tooFew*

virtual void tooFew(int min, int n_detected) = 0

Too few features were detected so, adjust the detector parameters accordingly -
so that the next detection detects more features.

    param min: This minimum desired number features.
                   param n_detected:
               The actual number detected last run.

An example implementation of this is

void FastAdjuster::tooFew(int min, int n_detected)
{
        thresh_--;
}

===========================================================================
*cv-AdjusterAdapter::tooMany*

virtual void tooMany(int max, int n_detected) = 0
    Too many features were detected so, adjust the detector parameters
    accordingly - so that the next

detection detects less features.

    param max: This maximum desired number features.
                   param n_detected:
               The actual number detected last run.

An example implementation of this is

void FastAdjuster::tooMany(int min, int n_detected)
{
        thresh_++;
}

===========================================================================
*cv-AdjusterAdapter::good*

virtual bool good() const = 0
    Are params maxed out or still valid? Returns false if the parameters can’t
    be adjusted any more.

An example implementation of this is

bool FastAdjuster::good() const
{
        return (thresh_ > 1) && (thresh_ < 200);
}

===========================================================================
*cv-FastAdjuster*

===========================================================================


An AdjusterAdapter() for the FastFeatureDetector() . This will basically
decrement or increment the threshhold by 1

class FastAdjuster FastAdjuster: public AdjusterAdapter
{
public:
        FastAdjuster(int init_thresh = 20, bool nonmax = true);
        ...
};

===========================================================================
*cv-StarAdjuster*

===========================================================================


An AdjusterAdapter() for the StarFeatureDetector() . This adjusts the
responseThreshhold of StarFeatureDetector.

class StarAdjuster: public AdjusterAdapter
{
        StarAdjuster(double initial_thresh = 30.0);
        ...
};

===========================================================================
*cv-SurfAdjuster*

===========================================================================


An AdjusterAdapter() for the SurfFeatureDetector() . This adjusts the
hessianThreshold of SurfFeatureDetector.

class SurfAdjuster: public SurfAdjuster
{
        SurfAdjuster();
        ...
};

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Common Interfaces of Feature Detectors
      □ KeyPoint
      □ FeatureDetector
      □ cv::FeatureDetector::detect
      □ cv::FeatureDetector::read
      □ cv::FeatureDetector::write
      □ cv::FeatureDetector::create
          ☆  
      □ FastFeatureDetector
      □ GoodFeaturesToTrackDetector
      □ MserFeatureDetector
      □ StarFeatureDetector
      □ SiftFeatureDetector
      □ SurfFeatureDetector
      □ GridAdaptedFeatureDetector
      □ PyramidAdaptedFeatureDetector
      □ DynamicAdaptedFeatureDetector
      □ cv::DynamicAdaptedFeatureDetector::DynamicAdaptedFeatureDetector
      □ AdjusterAdapter
      □ cv::AdjusterAdapter::tooFew
      □ cv::AdjusterAdapter::tooMany
      □ cv::AdjusterAdapter::good
      □ FastAdjuster
      □ StarAdjuster
      □ SurfAdjuster

Previous topic

Feature detection and description

Next topic

Common Interfaces of Descriptor Extractors

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Common Interfaces of Descriptor Extractors~

Extractors of keypoint descriptors in OpenCV have wrappers with common
interface that enables to switch easily between different algorithms solving
the same problem. This section is devoted to computing descriptors that are
represented as vectors in a multidimensional space. All objects that implement
‘’vector’’ descriptor extractors inherit DescriptorExtractor() interface.

===========================================================================
*cv-DescriptorExtractor*

===========================================================================


Abstract base class for computing descriptors for image keypoints.

class CV_EXPORTS DescriptorExtractor
{
public:
    virtual ~DescriptorExtractor();

    void compute( const Mat& image, vector<KeyPoint>& keypoints,
                  Mat& descriptors ) const;
    void compute( const vector<Mat>& images, vector<vector<KeyPoint> >& keypoints,
                  vector<Mat>& descriptors ) const;

    virtual void read( const FileNode& );
    virtual void write( FileStorage& ) const;

    virtual int descriptorSize() const = 0;
    virtual int descriptorType() const = 0;

    static Ptr<DescriptorExtractor> create( const string& descriptorExtractorType );

protected:
    ...
};

In this interface we assume a keypoint descriptor can be represented as a
dense, fixed-dimensional vector of some basic type. Most descriptors used in
practice follow this pattern, as it makes it very easy to compute distances
between descriptors. Therefore we represent a collection of descriptors as a
Mat() , where each row is one keypoint descriptor.

===========================================================================
*cv-DescriptorExtractor::compute*

void DescriptorExtractor::compute(const Mat& image, vector<KeyPoint>& keypoints
    , Mat& descriptors) const¶
    Compute the descriptors for a set of keypoints detected in an image (first
    variant)

or image set (second variant).

      param    The image.
      image:
                                 param keypoints:
               The keypoints. Keypoints for which a descriptor cannot be
               computed are removed.
                                param descriptors:
               The descriptors. Row i is the descriptor for keypoint i.

void DescriptorExtractor::compute(const vector<Mat>& images, vector<vector<
    KeyPoint> >& keypoints, vector<Mat>& descriptors) const
      □ images The image set.

      □ 
        keypoints Input keypoints collection. keypoints[i] is keypoints

            detected in images[i]. Keypoints for which a descriptor can not be
            computed are removed.

      □ 
        descriptors Descriptor collection. descriptors[i] are descriptors
            computed for

            a set keypoints[i].

===========================================================================
*cv-DescriptorExtractor::read*

void DescriptorExtractor::read(const FileNode& fn)~

    Read descriptor extractor object from file node.

    Parameter: fn – File node from which detector will be read.

===========================================================================
*cv-DescriptorExtractor::write*

void DescriptorExtractor::write(FileStorage& fs) const~

    Write descriptor extractor object to file storage.

    Parameter: fs – File storage in which detector will be written.

===========================================================================
*cv-DescriptorExtractor::create*

DescriptorExtractor()

..

    :param :

~

SiftFeatureDetector() ```` SurfFeatureDetector() ```` BriefFeatureDetector() ``
`` OpponentColorDescriptorExtractor() ````

===========================================================================
*cv-SiftDescriptorExtractor*

===========================================================================


Wrapping class for descriptors computing using SIFT() class.

class SiftDescriptorExtractor : public DescriptorExtractor
{
public:
    SiftDescriptorExtractor(
        const SIFT::DescriptorParams& descriptorParams=SIFT::DescriptorParams(),
        const SIFT::CommonParams& commonParams=SIFT::CommonParams() );
    SiftDescriptorExtractor( double magnification, bool isNormalize=true,
        bool recalculateAngles=true, int nOctaves=SIFT::CommonParams::DEFAULT_NOCTAVES,
        int nOctaveLayers=SIFT::CommonParams::DEFAULT_NOCTAVE_LAYERS,
        int firstOctave=SIFT::CommonParams::DEFAULT_FIRST_OCTAVE,
        int angleMode=SIFT::CommonParams::FIRST_ANGLE );

    virtual void read (const FileNode &fn);
    virtual void write (FileStorage &fs) const;
    virtual int descriptorSize() const;
    virtual int descriptorType() const;
protected:
    ...
}

===========================================================================
*cv-SurfDescriptorExtractor*

===========================================================================


Wrapping class for descriptors computing using SURF() class.

class SurfDescriptorExtractor : public DescriptorExtractor
{
public:
    SurfDescriptorExtractor( int nOctaves=4,
                             int nOctaveLayers=2, bool extended=false );

    virtual void read (const FileNode &fn);
    virtual void write (FileStorage &fs) const;
    virtual int descriptorSize() const;
    virtual int descriptorType() const;
protected:
    ...
}

===========================================================================
*cv-CalonderDescriptorExtractor*

===========================================================================


Wrapping class for descriptors computing using RTreeClassifier() class.

template<typename T>
class CalonderDescriptorExtractor : public DescriptorExtractor
{
public:
    CalonderDescriptorExtractor( const string& classifierFile );

    virtual void read( const FileNode &fn );
    virtual void write( FileStorage &fs ) const;
    virtual int descriptorSize() const;
    virtual int descriptorType() const;
protected:
    ...
}

===========================================================================
*cv-OpponentColorDescriptorExtractor*

===========================================================================


Adapts a descriptor extractor to compute descripors in Opponent Color Space
(refer to van de Sande et al., CGIV 2008 “Color Descriptors for Object Category
Recognition”). Input RGB image is transformed in Opponent Color Space. Then
unadapted descriptor extractor (set in constructor) computes descriptors on
each of the three channel and concatenate them into a single color descriptor.

class OpponentColorDescriptorExtractor : public DescriptorExtractor
{
public:
    OpponentColorDescriptorExtractor( const Ptr<DescriptorExtractor>& dextractor );

    virtual void read( const FileNode& );
    virtual void write( FileStorage& ) const;
    virtual int descriptorSize() const;
    virtual int descriptorType() const;
protected:
    ...
};

===========================================================================
*cv-BriefDescriptorExtractor*

Class for computing BRIEF descriptors described in paper of Calonder M.,
Lepetit V., Strecha C., Fua P.: ‘’BRIEF: Binary Robust Independent Elementary
Features.’’ 11th European Conference on Computer Vision (ECCV), Heraklion,
Crete. LNCS Springer, September 2010.

class BriefDescriptorExtractor : public DescriptorExtractor
{
public:
    static const int PATCH_SIZE = 48;
    static const int KERNEL_SIZE = 9;

    // bytes is a length of descriptor in bytes. It can be equal 16, 32 or 64 bytes.
    BriefDescriptorExtractor( int bytes = 32 );

    virtual void read( const FileNode& );
    virtual void write( FileStorage& ) const;
    virtual int descriptorSize() const;
    virtual int descriptorType() const;
protected:
    ...
};

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Common Interfaces of Descriptor Extractors
      □ DescriptorExtractor
      □ cv::DescriptorExtractor::compute
      □ cv::DescriptorExtractor::read
      □ cv::DescriptorExtractor::write
      □ cv::DescriptorExtractor::create
          ☆  
      □ SiftDescriptorExtractor
      □ SurfDescriptorExtractor
      □ CalonderDescriptorExtractor
      □ OpponentColorDescriptorExtractor
      □ BriefDescriptorExtractor

Previous topic

Common Interfaces of Feature Detectors

Next topic

Common Interfaces of Descriptor Matchers

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Common Interfaces of Descriptor Matchers~

Matchers of keypoint descriptors in OpenCV have wrappers with common interface
that enables to switch easily between different algorithms solving the same
problem. This section is devoted to matching descriptors that are represented
as vectors in a multidimensional space. All objects that implement ‘’vector’’
descriptor matchers inherit DescriptorMatcher() interface.

===========================================================================
*cv-DMatch*

Match between two keypoint descriptors: query descriptor index, train
descriptor index, train image index and distance between descriptors.

struct DMatch
{
    DMatch() : queryIdx(-1), trainIdx(-1), imgIdx(-1),
               distance(std::numeric_limits<float>::max()) {}
    DMatch( int _queryIdx, int _trainIdx, float _distance ) :
            queryIdx(_queryIdx), trainIdx(_trainIdx), imgIdx(-1),
            distance(_distance) {}
    DMatch( int _queryIdx, int _trainIdx, int _imgIdx, float _distance ) :
            queryIdx(_queryIdx), trainIdx(_trainIdx), imgIdx(_imgIdx),
            distance(_distance) {}

    int queryIdx; // query descriptor index
    int trainIdx; // train descriptor index
    int imgIdx;   // train image index

    float distance;

    // less is better
    bool operator<( const DMatch &m ) const;
};

===========================================================================
*cv-DescriptorMatcher*

===========================================================================


Abstract base class for matching keypoint descriptors. It has two groups of
match methods: for matching descriptors of one image with other image or with
image set.

class DescriptorMatcher
{
public:
    virtual ~DescriptorMatcher();

    virtual void add( const vector<Mat>& descriptors );

    const vector<Mat>& getTrainDescriptors() const;
    virtual void clear();
    bool empty() const;
    virtual bool isMaskSupported() const = 0;

    virtual void train();

    /*
     * Group of methods to match descriptors from image pair.
     */
    void match( const Mat& queryDescriptors, const Mat& trainDescriptors,
                vector<DMatch>& matches, const Mat& mask=Mat() ) const;
    void knnMatch( const Mat& queryDescriptors, const Mat& trainDescriptors,
                   vector<vector<DMatch> >& matches, int k,
                   const Mat& mask=Mat(), bool compactResult=false ) const;
    void radiusMatch( const Mat& queryDescriptors, const Mat& trainDescriptors,
                      vector<vector<DMatch> >& matches, float maxDistance,
                      const Mat& mask=Mat(), bool compactResult=false ) const;
    /*
     * Group of methods to match descriptors from one image to image set.
     */
    void match( const Mat& queryDescriptors, vector<DMatch>& matches,
                const vector<Mat>& masks=vector<Mat>() );
    void knnMatch( const Mat& queryDescriptors, vector<vector<DMatch> >& matches,
                   int k, const vector<Mat>& masks=vector<Mat>(),
                   bool compactResult=false );
    void radiusMatch( const Mat& queryDescriptors, vector<vector<DMatch> >& matches,
                      float maxDistance, const vector<Mat>& masks=vector<Mat>(),
                      bool compactResult=false );

    virtual void read( const FileNode& );
    virtual void write( FileStorage& ) const;

    virtual Ptr<DescriptorMatcher> clone( bool emptyTrainData=false ) const = 0;

    static Ptr<DescriptorMatcher> create( const string& descriptorMatcherType );

protected:
    vector<Mat> trainDescCollection;
    ...
};

===========================================================================
*cv-DescriptorMatcher::add*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

void add(const vector<Mat>& descriptors)¶
    Add descriptors to train descriptor collection. If collection
    trainDescCollectionis not empty

the new descriptors are added to existing train descriptors.

                                param descriptors:
      Descriptors to add. Each descriptors[i] is a set of descriptors from the
      same (one) train image.

===========================================================================
*cv-DescriptorMatcher::getTrainDescriptors*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

const vector<Mat>& getTrainDescriptors() const¶
    Returns constant link to the train descriptor collection (i.e.
    trainDescCollection).

===========================================================================
*cv-DescriptorMatcher::clear*

void DescriptorMatcher::clear()¶
    Clear train descriptor collection.

===========================================================================
*cv-DescriptorMatcher::empty*

bool DescriptorMatcher::empty() const¶
    Return true if there are not train descriptors in collection.

===========================================================================
*cv-DescriptorMatcher::isMaskSupported*

bool DescriptorMatcher::isMaskSupported()¶
    Returns true if descriptor matcher supports masking permissible matches.

===========================================================================
*cv-DescriptorMatcher::train*

void DescriptorMatcher::train()¶
    Train descriptor matcher (e.g. train flann index). In all methods to match
    the method train()

is run every time before matching. Some descriptor matchers (e.g.
BruteForceMatcher) have empty implementation of this method, other matchers
realy train their inner structures (e.g. FlannBasedMatcher trains flann::Index)

===========================================================================
*cv-DescriptorMatcher::match*

void DescriptorMatcher::match(const Mat& queryDescriptors, const Mat&
    trainDescriptors, vector<DMatch>& matches, const Mat& mask=Mat()) const¶
    Find the best match for each descriptor from a query set with train
    descriptors.

Supposed that the query descriptors are of keypoints detected on the same query
image. In first variant of this method train descriptors are set as input
argument and supposed that they are of keypoints detected on the same train
image. In second variant of the method train descriptors collection that was
set using addmethod is used. Optional mask (or masks) can be set to describe
which descriptors can be matched. queryDescriptors[i]can be matched with
trainDescriptors[j]only if mask.at<uchar>(i,j)is non-zero.

void DescriptorMatcher::match(const Mat& queryDescriptors, vector<DMatch>&
    matches, const vector<Mat>& masks=vector<Mat>())

                  • queryDescriptors – Query set of descriptors.
                  • trainDescriptors – Train set of descriptors. This will not
                    be added to train descriptors collection stored in class
                    object.
                  • matches – Matches. If some query descriptor masked out in
                    mask no match will be added for this descriptor. So matches
    Parameters:     size may be less query descriptors count.
                  • mask – Mask specifying permissible matches between input
                    query and train matrices of descriptors.
                  • masks – The set of masks. Each masks[i] specifies
                    permissible matches between input query descriptors and
                    stored train descriptors from i-th image (i.e.
                    trainDescCollection[i]) .

===========================================================================
*cv-DescriptorMatcher::knnMatch*

DescriptorMatcher::match()

void DescriptorMatcher::knnMatch(const Mat& queryDescriptors, const Mat&
    trainDescriptors, vector<vector<DMatch> >& matches, int k, const Mat& mask=
    Mat(), bool compactResult=false) const¶
    Find the k best matches for each descriptor from a query set with train
    descriptors.

Found k (or less if not possible) matches are returned in distance increasing
order. Details about query and train descriptors see in .

void DescriptorMatcher::knnMatch(const Mat& queryDescriptors, vector<vector<
    DMatch> >& matches, int k, const vector<Mat>& masks=vector<Mat>(), bool
    compactResult=false)

                  • queryDescriptors, trainDescriptors, mask, masks – See in
                    DescriptorMatcher::match() .
                  • matches – Mathes. Each matches[i] is k or less matches for
                    the same query descriptor.
                  • k – Count of best matches will be found per each query
    Parameters:     descriptor (or less if it’s not possible).
                  • compactResult – It’s used when mask (or masks) is not
                    empty. If compactResult is false matches vector will have
                    the same size as queryDescriptors rows. If compactResult is
                    true matches vector will not contain matches for fully
                    masked out query descriptors.

===========================================================================
*cv-DescriptorMatcher::radiusMatch*

DescriptorMatcher::match()

void DescriptorMatcher::radiusMatch(const Mat& queryDescriptors, const Mat&
    trainDescriptors, vector<vector<DMatch> >& matches, float maxDistance,
    const Mat& mask=Mat(), bool compactResult=false) const¶
    Find the best matches for each query descriptor which have distance less
    than given threshold.

Found matches are returned in distance increasing order. Details about query
and train descriptors see in .

void DescriptorMatcher::radiusMatch(const Mat& queryDescriptors, vector<vector<
    DMatch> >& matches, float maxDistance, const vector<Mat>& masks=vector<Mat>
    (), bool compactResult=false)

                  • queryDescriptors, trainDescriptors, mask, masks – See in
                    DescriptorMatcher::match() .
    Parameters:   • matches, compactResult – See in DescriptorMatcher::knnMatch
                    () .
                  • maxDistance – The threshold to found match distances.

===========================================================================
*cv-DescriptorMatcher::clone*

Ptr<DescriptorMatcher> \DescriptorMatcher::clone(bool emptyTrainData) const~

    Clone the matcher.

               emptyTrainData – If emptyTrainData is false the method create
    Parameter: deep copy of the object, i.e. copies both parameters and train
               data. If emptyTrainData is true the method create object copy
               with current parameters but with empty train data..

===========================================================================
*cv-DescriptorMatcher::create*

DescriptorMatcher()

..

    :param :

===========================================================================
*cv-BruteForceMatcher*

===========================================================================


Brute-force descriptor matcher. For each descriptor in the first set, this
matcher finds the closest descriptor in the second set by trying each one. This
descriptor matcher supports masking permissible matches between descriptor
sets.

template<class Distance>
class BruteForceMatcher : public DescriptorMatcher
{
public:
    BruteForceMatcher( Distance d = Distance() );
    virtual ~BruteForceMatcher();

    virtual bool isMaskSupported() const;
    virtual Ptr<DescriptorMatcher> clone( bool emptyTrainData=false ) const;
protected:
    ...
}

For efficiency, BruteForceMatcher is templated on the distance metric. For
float descriptors, a common choice would be L2<float> . Class of supported
distances are:

template<typename T>
struct Accumulator
{
    typedef T Type;
};

template<> struct Accumulator<unsigned char>  { typedef unsigned int Type; };
template<> struct Accumulator<unsigned short> { typedef unsigned int Type; };
template<> struct Accumulator<char>   { typedef int Type; };
template<> struct Accumulator<short>  { typedef int Type; };

/*
 * Squared Euclidean distance functor
 */
template<class T>
struct L2
{
    typedef T ValueType;
    typedef typename Accumulator<T>::Type ResultType;

    ResultType operator()( const T* a, const T* b, int size ) const;
};

/*
 * Manhattan distance (city block distance) functor
 */
template<class T>
struct CV_EXPORTS L1
{
    typedef T ValueType;
    typedef typename Accumulator<T>::Type ResultType;

    ResultType operator()( const T* a, const T* b, int size ) const;
    ...
};

/*
 * Hamming distance (city block distance) functor
 */
struct HammingLUT
{
    typedef unsigned char ValueType;
    typedef int ResultType;

    ResultType operator()( const unsigned char* a, const unsigned char* b,
                           int size ) const;
    ...
};

struct Hamming
{
    typedef unsigned char ValueType;
    typedef int ResultType;

    ResultType operator()( const unsigned char* a, const unsigned char* b,
                           int size ) const;
    ...
};

===========================================================================
*cv-FlannBasedMatcher*

===========================================================================


Flann based descriptor matcher. This matcher trains flann::Index() on train
descriptor collection and calls it’s nearest search methods to find best
matches. So this matcher may be faster in cases of matching to large train
collection than brute force matcher. FlannBasedMatcher does not support masking
permissible matches between descriptor sets, because flann::Index() does not
support this.

class FlannBasedMatcher : public DescriptorMatcher
{
public:
    FlannBasedMatcher(
      const Ptr<flann::IndexParams>& indexParams=new flann::KDTreeIndexParams(),
      const Ptr<flann::SearchParams>& searchParams=new flann::SearchParams() );

    virtual void add( const vector<Mat>& descriptors );
    virtual void clear();

    virtual void train();
    virtual bool isMaskSupported() const;

    virtual Ptr<DescriptorMatcher> clone( bool emptyTrainData=false ) const;
protected:
    ...
};

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Common Interfaces of Descriptor Matchers
      □ DMatch
      □ DescriptorMatcher
      □ cv::DescriptorMatcher::add
      □ cv::DescriptorMatcher::getTrainDescriptors
      □ cv::DescriptorMatcher::clear
      □ cv::DescriptorMatcher::empty
      □ cv::DescriptorMatcher::isMaskSupported
      □ cv::DescriptorMatcher::train
      □ cv::DescriptorMatcher::match
      □ cv::DescriptorMatcher::knnMatch
      □ cv::DescriptorMatcher::radiusMatch
      □ cv::DescriptorMatcher::clone
      □ cv::DescriptorMatcher::create
      □ BruteForceMatcher
      □ FlannBasedMatcher

Previous topic

Common Interfaces of Descriptor Extractors

Next topic

Common Interfaces of Generic Descriptor Matchers

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Common Interfaces of Generic Descriptor Matchers~

Matchers of keypoint descriptors in OpenCV have wrappers with common interface
that enables to switch easily between different algorithms solving the same
problem. This section is devoted to matching descriptors that can not be
represented as vectors in a multidimensional space. GenericDescriptorMatcher is
a more generic interface for descriptors. It does not make any assumptions
about descriptor representation. Every descriptor with DescriptorExtractor()
interface has a wrapper with GenericDescriptorMatcher interface (see
VectorDescriptorMatcher() ). There are descriptors such as One way descriptor
and Ferns that have GenericDescriptorMatcher interface implemented, but do not
support DescriptorExtractor() .

===========================================================================
*cv-GenericDescriptorMatcher*

===========================================================================


Abstract interface for a keypoint descriptor extracting and matching. There is
DescriptorExtractor() and DescriptorMatcher() for these purposes too, but their
interfaces are intended for descriptors represented as vectors in a
multidimensional space. GenericDescriptorMatcher is a more generic interface
for descriptors. As DescriptorMatcher() , GenericDescriptorMatcher has two
groups of match methods: for matching keypoints of one image with other image
or with image set.

class GenericDescriptorMatcher
{
public:
    GenericDescriptorMatcher();
    virtual ~GenericDescriptorMatcher();

    virtual void add( const vector<Mat>& images,
                      vector<vector<KeyPoint> >& keypoints );

    const vector<Mat>& getTrainImages() const;
    const vector<vector<KeyPoint> >& getTrainKeypoints() const;
    virtual void clear();

    virtual void train() = 0;

    virtual bool isMaskSupported() = 0;

    void classify( const Mat& queryImage,
                   vector<KeyPoint>& queryKeypoints,
                   const Mat& trainImage,
                   vector<KeyPoint>& trainKeypoints ) const;
    void classify( const Mat& queryImage,
                   vector<KeyPoint>& queryKeypoints );

    /*
     * Group of methods to match keypoints from image pair.
     */
    void match( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                const Mat& trainImage, vector<KeyPoint>& trainKeypoints,
                vector<DMatch>& matches, const Mat& mask=Mat() ) const;
    void knnMatch( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                   const Mat& trainImage, vector<KeyPoint>& trainKeypoints,
                   vector<vector<DMatch> >& matches, int k,
                   const Mat& mask=Mat(), bool compactResult=false ) const;
    void radiusMatch( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                      const Mat& trainImage, vector<KeyPoint>& trainKeypoints,
                      vector<vector<DMatch> >& matches, float maxDistance,
                      const Mat& mask=Mat(), bool compactResult=false ) const;
    /*
     * Group of methods to match keypoints from one image to image set.
     */
    void match( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                vector<DMatch>& matches, const vector<Mat>& masks=vector<Mat>() );
    void knnMatch( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                   vector<vector<DMatch> >& matches, int k,
                   const vector<Mat>& masks=vector<Mat>(), bool compactResult=false );
    void radiusMatch( const Mat& queryImage, vector<KeyPoint>& queryKeypoints,
                      vector<vector<DMatch> >& matches, float maxDistance,
                      const vector<Mat>& masks=vector<Mat>(), bool compactResult=false );

    virtual void read( const FileNode& );
    virtual void write( FileStorage& ) const;

    virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const = 0;

protected:
    ...
};

===========================================================================
*cv-GenericDescriptorMatcher::add*

void GenericDescriptorMatcher::add(const vector<Mat>& images, vector<vector<
    KeyPoint> >& keypoints)¶
    Adds images and keypoints from them to the train collection (descriptors
    are supposed to be calculated here).

If train collection is not empty new image and keypoints from them will be
added to existing data.

      param   Image collection.
     images:
                                 param keypoints:
              Point collection. Assumes that keypoints[i] are keypoints
              detected in an image images[i] .

===========================================================================
*cv-GenericDescriptorMatcher::getTrainImages*

..

GenericDescriptorMatcher::add()

void GenericDescriptorMatcher::classify(const Mat& queryImage, vector<KeyPoint>
    & queryKeypoints, const Mat& trainImage, vector<KeyPoint>& trainKeypoints)
    const¶
    Classifies query keypoints under keypoints of one train image qiven as
    input argument

(first version of the method) or train image collection that set using (second
version).

void GenericDescriptorMatcher::classify(const Mat& queryImage, vector<KeyPoint>
    & queryKeypoints)

                  • queryImage – The query image.
    Parameters:   • queryKeypoints – Keypoints from the query image.
                  • trainImage – The train image.
                  • trainKeypoints – Keypoints from the train image.

===========================================================================
*cv-GenericDescriptorMatcher::match*

GenericDescriptorMatcher::add() DescriptorMatcher::match()

void GenericDescriptorMatcher::match(const Mat& queryImage, vector<KeyPoint>&
    queryKeypoints, const Mat& trainImage, vector<KeyPoint>& trainKeypoints,
    vector<DMatch>& matches, const Mat& mask=Mat()) const¶
    Find best match for query keypoints to the training set. In first version
    of method

one train image and keypoints detected on it - are input arguments. In second
version query keypoints are matched to training collectin that set using . As
in the mask can be set.

void GenericDescriptorMatcher::match(const Mat& queryImage, vector<KeyPoint>&
    queryKeypoints, vector<DMatch>& matches, const vector<Mat>& masks=vector
    <Mat>())

                  • queryImage – Query image.
                  • queryKeypoints – Keypoints detected in queryImage .
                  • trainImage – Train image. This will not be added to train
                    image collection stored in class object.
                  • trainKeypoints – Keypoints detected in trainImage . They
                    will not be added to train points collection stored in
                    class object.
                  • matches –

    Parameters:     Matches. If some query descriptor (keypoint) masked out in
                    mask no match will be added for this descriptor.

                        So matches size may be less query keypoints count.

                  • mask – Mask specifying permissible matches between input
                    query and train keypoints.
                  • masks – The set of masks. Each masks[i] specifies
                    permissible matches between input query keypoints and
                    stored train keypointss from i-th image.

===========================================================================
*cv-GenericDescriptorMatcher::knnMatch*

GenericDescriptorMatcher::match() DescriptorMatcher::knnMatch()

void GenericDescriptorMatcher::knnMatch(const Mat& queryImage, vector<KeyPoint>
    & queryKeypoints, const Mat& trainImage, vector<KeyPoint>& trainKeypoints,
    vector<vector<DMatch> >& matches, int k, const Mat& mask=Mat(), bool
    compactResult=false) const¶
    Find the knn best matches for each keypoint from a query set with train
    keypoints.

Found knn (or less if not possible) matches are returned in distance increasing
order. Details see in and .

void GenericDescriptorMatcher::knnMatch(const Mat& queryImage, vector<KeyPoint>
    & queryKeypoints, vector<vector<DMatch> >& matches, int k, const vector<Mat
    >& masks=vector<Mat>(), bool compactResult=false)

===========================================================================
*cv-GenericDescriptorMatcher::radiusMatch*

GenericDescriptorMatcher::match() DescriptorMatcher::radiusMatch()

void GenericDescriptorMatcher::radiusMatch(const Mat& queryImage, vector<
    KeyPoint>& queryKeypoints, const Mat& trainImage, vector<KeyPoint>&
    trainKeypoints, vector<vector<DMatch> >& matches, float maxDistance, const 
    Mat& mask=Mat(), bool compactResult=false) const¶
    Find the best matches for each query keypoint which have distance less than
    given threshold.

Found matches are returned in distance increasing order. Details see in and .

void GenericDescriptorMatcher::radiusMatch(const Mat& queryImage, vector<
    KeyPoint>& queryKeypoints, vector<vector<DMatch> >& matches, float
    maxDistance, const vector<Mat>& masks=vector<Mat>(), bool compactResult=
    false)

===========================================================================
*cv-GenericDescriptorMatcher::read*

void GenericDescriptorMatcher::read(const FileNode& fn)¶
    Reads matcher object from a file node.

===========================================================================
*cv-GenericDescriptorMatcher::write*

void GenericDescriptorMatcher::write(FileStorage& fs) const¶
    Writes match object to a file storage

===========================================================================
*cv-GenericDescriptorMatcher::clone*

Ptr<GenericDescriptorMatcher>\GenericDescriptorMatcher::clone(bool
    emptyTrainData) const~

    Clone the matcher.

               emptyTrainData – If emptyTrainData is false the method create
    Parameter: deep copy of the object, i.e. copies both parameters and train
               data. If emptyTrainData is true the method create object copy
               with current parameters but with empty train data.

===========================================================================
*cv-OneWayDescriptorMatcher*

===========================================================================


Wrapping class for computing, matching and classification of descriptors using
OneWayDescriptorBase() class.

class OneWayDescriptorMatcher : public GenericDescriptorMatcher
{
public:
    class Params
    {
    public:
        static const int POSE_COUNT = 500;
        static const int PATCH_WIDTH = 24;
        static const int PATCH_HEIGHT = 24;
        static float GET_MIN_SCALE() { return 0.7f; }
        static float GET_MAX_SCALE() { return 1.5f; }
        static float GET_STEP_SCALE() { return 1.2f; }

        Params( int poseCount = POSE_COUNT,
                Size patchSize = Size(PATCH_WIDTH, PATCH_HEIGHT),
                string pcaFilename = string(),
                string trainPath = string(), string trainImagesList = string(),
                float minScale = GET_MIN_SCALE(), float maxScale = GET_MAX_SCALE(),
                float stepScale = GET_STEP_SCALE() );

        int poseCount;
        Size patchSize;
        string pcaFilename;
        string trainPath;
        string trainImagesList;

        float minScale, maxScale, stepScale;
    };

    OneWayDescriptorMatcher( const Params& params=Params() );
    virtual ~OneWayDescriptorMatcher();

    void initialize( const Params& params, const Ptr<OneWayDescriptorBase>& base=Ptr<OneWayDescriptorBase>() );

    // Clears keypoints storing in collection and OneWayDescriptorBase
    virtual void clear();

    virtual void train();

    virtual bool isMaskSupported();

    virtual void read( const FileNode &fn );
    virtual void write( FileStorage& fs ) const;

    virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const;
protected:
    ...
};

===========================================================================
*cv-FernDescriptorMatcher*

===========================================================================


Wrapping class for computing, matching and classification of descriptors using
FernClassifier() class.

class FernDescriptorMatcher : public GenericDescriptorMatcher
{
public:
    class Params
    {
    public:
        Params( int nclasses=0,
                int patchSize=FernClassifier::PATCH_SIZE,
                int signatureSize=FernClassifier::DEFAULT_SIGNATURE_SIZE,
                int nstructs=FernClassifier::DEFAULT_STRUCTS,
                int structSize=FernClassifier::DEFAULT_STRUCT_SIZE,
                int nviews=FernClassifier::DEFAULT_VIEWS,
                int compressionMethod=FernClassifier::COMPRESSION_NONE,
                const PatchGenerator& patchGenerator=PatchGenerator() );

        Params( const string& filename );

        int nclasses;
        int patchSize;
        int signatureSize;
        int nstructs;
        int structSize;
        int nviews;
        int compressionMethod;
        PatchGenerator patchGenerator;

        string filename;
    };

    FernDescriptorMatcher( const Params& params=Params() );
    virtual ~FernDescriptorMatcher();

    virtual void clear();

    virtual void train();

    virtual bool isMaskSupported();

    virtual void read( const FileNode &fn );
    virtual void write( FileStorage& fs ) const;

    virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const;

protected:
        ...
};

===========================================================================
*cv-VectorDescriptorMatcher*

===========================================================================


Class used for matching descriptors that can be described as vectors in a
finite-dimensional space.

class CV_EXPORTS VectorDescriptorMatcher : public GenericDescriptorMatcher
{
public:
    VectorDescriptorMatcher( const Ptr<DescriptorExtractor>& extractor, const Ptr<DescriptorMatcher>& matcher );
    virtual ~VectorDescriptorMatcher();

    virtual void add( const vector<Mat>& imgCollection,
                      vector<vector<KeyPoint> >& pointCollection );
    virtual void clear();
    virtual void train();
    virtual bool isMaskSupported();

    virtual void read( const FileNode& fn );
    virtual void write( FileStorage& fs ) const;

    virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const;

protected:
    ...
};

Example of creating:

VectorDescriptorMatcher matcher( new SurfDescriptorExtractor,
                                 new BruteForceMatcher<L2<float> > );

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Common Interfaces of Generic Descriptor Matchers
      □ GenericDescriptorMatcher
      □ cv::GenericDescriptorMatcher::add
      □ cv::GenericDescriptorMatcher::getTrainImages
      □ cv::
      □ cv::
      □ cv::
      □ cv::
      □ cv::
      □ cv::GenericDescriptorMatcher::match
      □ cv::GenericDescriptorMatcher::knnMatch
      □ cv::GenericDescriptorMatcher::radiusMatch
      □ cv::GenericDescriptorMatcher::read
      □ cv::GenericDescriptorMatcher::write
      □ cv::GenericDescriptorMatcher::clone
      □ OneWayDescriptorMatcher
      □ FernDescriptorMatcher
      □ VectorDescriptorMatcher

Previous topic

Common Interfaces of Descriptor Matchers

Next topic

Drawing Function of Keypoints and Matches

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Drawing Function of Keypoints and Matches~

===========================================================================
*cv-drawMatches*

void drawMatches(const Mat& img1, const vector<KeyPoint>& keypoints1, const Mat
    & img2, const vector<KeyPoint>& keypoints2, const vector<DMatch>&
    matches1to2, Mat& outImg, const Scalar& matchColor=Scalar::all(-1), const
    Scalar& singlePointColor=Scalar::all(-1), const vector<char>& matchesMask=
    vector<char>(), int flags=DrawMatchesFlags::DEFAULT)¶
    This function draws matches of keypints from two images on output image.

Match is a line connecting two keypoints (circles).

void drawMatches(const Mat& img1, const vector<KeyPoint>& keypoints1, const Mat
    & img2, const vector<KeyPoint>& keypoints2, const vector<vector<DMatch> >&
    matches1to2, Mat& outImg, const Scalar& matchColor=Scalar::all(-1), const
    Scalar& singlePointColor=Scalar::all(-1), const vector<vector<char>>&
    matchesMask= vector<vector<char> >(), int flags=DrawMatchesFlags::DEFAULT)

                  • img1 – First source image.
                  • keypoints1 – Keypoints from first source image.
                  • img2 – Second source image.
                  • keypoints2 – Keypoints from second source image.
                  • matches – Matches from first image to second one, i.e.
                    keypoints1[i] has corresponding point keypoints2[matches
                    [i]] .
                  • outImg – Output image. Its content depends on flags value
                    what is drawn in output image. See below possible flags bit
                    values.
    Parameters:   • matchColor – Color of matches (lines and connected
                    keypoints). If matchColor==Scalar::all(-1) color will be
                    generated randomly.
                  • singlePointColor – Color of single keypoints (circles),
                    i.e. keypoints not having the matches. If singlePointColor=
                    =Scalar::all(-1) color will be generated randomly.
                  • matchesMask – Mask determining which matches will be drawn.
                    If mask is empty all matches will be drawn.
                  • flags – Each bit of flags sets some feature of drawing.
                    Possible flags bit values is defined by DrawMatchesFlags ,
                    see below.

struct DrawMatchesFlags
{
    enum{ DEFAULT = 0, // Output image matrix will be created (Mat::create),
                       // i.e. existing memory of output image may be reused.
                       // Two source image, matches and single keypoints
                       // will be drawn.
                       // For each keypoint only the center point will be
                       // drawn (without the circle around keypoint with
                       // keypoint size and orientation).
          DRAW_OVER_OUTIMG = 1, // Output image matrix will not be
                       // created (Mat::create). Matches will be drawn
                       // on existing content of output image.
          NOT_DRAW_SINGLE_POINTS = 2, // Single keypoints will not be drawn.
          DRAW_RICH_KEYPOINTS = 4 // For each keypoint the circle around
                       // keypoint with keypoint size and orientation will
                       // be drawn.
        };
};

===========================================================================
*cv-drawKeypoints*

void drawKeypoints(const Mat& image, const vector<KeyPoint>& keypoints, Mat&
    outImg, const Scalar& color=Scalar::all(-1), int flags=
    DrawMatchesFlags::DEFAULT)~

    Draw keypoints.

                  • image – Source image.
                  • keypoints – Keypoints from source image.
    Parameters:   • outImg – Output image. Its content depends on flags value
                    what is drawn in output image. See possible flags bit
                    values.
                  • color – Color of keypoints

    .

               flags – Each bit of flags sets some feature of drawing. Possible
    Parameter: flags bit values is defined by DrawMatchesFlags , see above in
               drawMatches() .

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Drawing Function of Keypoints and Matches
      □ cv::drawMatches
      □ cv::drawKeypoints

Previous topic

Common Interfaces of Generic Descriptor Matchers

Next topic

Object Categorization

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

Object Categorization~

Some approaches based on local 2D features and used to object categorization
are described in this section.

===========================================================================
*cv-BOWTrainer*

===========================================================================


Abstract base class for training ‘’bag of visual words’’ vocabulary from a set
of descriptors. See e.g. ‘’Visual Categorization with Bags of Keypoints’’ of
Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric
Bray, 2004.

class BOWTrainer
{
public:
    BOWTrainer(){}
    virtual ~BOWTrainer(){}

    void add( const Mat& descriptors );
    const vector<Mat>& getDescriptors() const;
    int descripotorsCount() const;

    virtual void clear();

    virtual Mat cluster() const = 0;
    virtual Mat cluster( const Mat& descriptors ) const = 0;

protected:
    ...
};

===========================================================================
*cv-BOWTrainer::add*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

void BOWTrainer::add(const Mat& descriptors)~

    Add descriptors to training set. The training set will be clustered using
    clustermethod to construct vocabulary.

    Parameter: descriptors – Descriptors to add to training set. Each row of
               descriptors matrix is a one descriptor.

===========================================================================
*cv-BOWTrainer::getDescriptors*

const vector<Mat>& BOWTrainer::getDescriptors() const¶
    Returns training set of descriptors.

===========================================================================
*cv-BOWTrainer::descripotorsCount*

const vector<Mat>& BOWTrainer::descripotorsCount() const¶
    Returns count of all descriptors stored in the training set.

===========================================================================
*cv-BOWTrainer::cluster*

Mat BOWTrainer::cluster() const¶
    Cluster train descriptors. Vocabulary consists from cluster centers. So
    this method

returns vocabulary. In first method variant the stored in object train
descriptors will be clustered, in second variant – input descriptors will be
clustered.

Mat BOWTrainer::cluster(const Mat& descriptors) const

               descriptors – Descriptors to cluster. Each row of descriptors
    Parameter: matrix is a one descriptor. Descriptors will not be added to the
               inner train descriptor set.

===========================================================================
*cv-BOWKMeansTrainer*

===========================================================================


kmeans() based class to train visual vocabulary using the ‘’bag of visual
words’’ approach.

class BOWKMeansTrainer : public BOWTrainer
{
public:
    BOWKMeansTrainer( int clusterCount, const TermCriteria& termcrit=TermCriteria(),
                      int attempts=3, int flags=KMEANS_PP_CENTERS );
    virtual ~BOWKMeansTrainer(){}

    // Returns trained vocabulary (i.e. cluster centers).
    virtual Mat cluster() const;
    virtual Mat cluster( const Mat& descriptors ) const;

protected:
    ...
};

To gain an understanding of constructor parameters see kmeans() function
arguments.

===========================================================================
*cv-BOWImgDescriptorExtractor*

===========================================================================


Class to compute image descriptor using ‘’bad of visual words’‘. In few,
    such computing consists from the following steps: 1. Compute descriptors
    for given image and it’s keypoints set,

2. Find nearest visual words from vocabulary for each keypoint descriptor, 3.
Image descriptor is a normalized histogram of vocabulary words encountered in
the image. I.e.

i -bin of the histogram is a frequency of i -word of vocabulary in the given
image.

class BOWImgDescriptorExtractor
{
public:
    BOWImgDescriptorExtractor( const Ptr<DescriptorExtractor>& dextractor,
                               const Ptr<DescriptorMatcher>& dmatcher );
    virtual ~BOWImgDescriptorExtractor(){}

    void setVocabulary( const Mat& vocabulary );
    const Mat& getVocabulary() const;
    void compute( const Mat& image, vector<KeyPoint>& keypoints,
                  Mat& imgDescriptor,
                  vector<vector<int> >* pointIdxsOfClusters=0,
                  Mat* descriptors=0 );
    int descriptorSize() const;
    int descriptorType() const;

protected:
    ...
};

===========================================================================
*cv-BOWImgDescriptorExtractor::BOWImgDescriptorExtractor*

BOWImgDescriptorExtractor::BOWImgDescriptorExtractor(const Ptr<
    DescriptorExtractor>& dextractor, const Ptr<DescriptorMatcher>& dmatcher)~

    Constructor.

                  • dextractor – Descriptor extractor that will be used to
                    compute descriptors for input image and it’s keypoints.
    Parameters:   • dmatcher – Descriptor matcher that will be used to find
                    nearest word of trained vocabulary to each keupoints
                    descriptor of the image.

===========================================================================
*cv-BOWImgDescriptorExtractor::setVocabulary*

void BOWImgDescriptorExtractor::setVocabulary(const Mat& vocabulary)~

    Method to set visual vocabulary.

               vocabulary – Vocabulary (can be trained using inheritor of
    Parameter: BOWTrainer() ). Each row of vocabulary is a one visual word
               (cluster center).

===========================================================================
*cv-BOWImgDescriptorExtractor::getVocabulary*

const Mat& BOWImgDescriptorExtractor::getVocabulary() const¶
    Returns set vocabulary.

===========================================================================
*cv-BOWImgDescriptorExtractor::compute*

void BOWImgDescriptorExtractor::compute(const Mat& image, vector<KeyPoint>&
    keypoints, Mat& imgDescriptor, vector<vector<int> >* pointIdxsOfClusters=0,
    Mat* descriptors=0)~

    Compute image descriptor using set visual vocabulary.

                  • image – The image. Image descriptor will be computed for
                    this.
                  • keypoints – Keypoints detected in the input image.
                  • imgDescriptor – This is output, i.e. computed image
                    descriptor.
                  • pointIdxsOfClusters –

    Parameters:     Indices of keypoints which belong to the cluster, i.e.
                        pointIdxsOfClusters[i] is keypoint indices which belong

                    to the i- cluster (word of vocabulary) (returned if it is
                    not 0.)

                  • descriptors – Descriptors of the image keypoints (returned
                    if it is not 0.)

===========================================================================
*cv-BOWImgDescriptorExtractor::descriptorSize*

int BOWImgDescriptorExtractor::descriptorSize() const¶
    Returns image discriptor size, if vocabulary was set, and 0 otherwise.

===========================================================================
*cv-BOWImgDescriptorExtractor::descriptorType*

int BOWImgDescriptorExtractor::descriptorType() const¶
    Returns image descriptor type.

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Object Categorization
      □ BOWTrainer
      □ cv::BOWTrainer::add
      □ cv::BOWTrainer::getDescriptors
      □ cv::BOWTrainer::descripotorsCount
      □ cv::BOWTrainer::cluster
      □ BOWKMeansTrainer
      □ BOWImgDescriptorExtractor
      □ cv::BOWImgDescriptorExtractor::BOWImgDescriptorExtractor
      □ cv::BOWImgDescriptorExtractor::setVocabulary
      □ cv::BOWImgDescriptorExtractor::getVocabulary
      □ cv::BOWImgDescriptorExtractor::compute
      □ cv::BOWImgDescriptorExtractor::descriptorSize
      □ cv::BOWImgDescriptorExtractor::descriptorType

Previous topic

Drawing Function of Keypoints and Matches

Next topic

flann. Clustering and Search in Multi-Dimensional Spaces

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • features2d. Feature Detection and Descriptor Extraction »

© Copyright 2010, authors. Created using Sphinx 0.6.2.


vim:tw=78:ts=4:ft=help:norl:

*cv-imgproc.txt*	For Vim 0.0	Thu May 19 18:07:34 2011

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

===========================================================================
*cv-Histograms*

===========================================================================
*cv-calcHist*

void calcHist(const Mat* arrays, int narrays, const int* channels, const Mat&
    mask, MatND& hist, int dims, const int* histSize, const float** ranges,
    bool uniform=true, bool accumulate=false)~

void calcHist(const Mat* arrays, int narrays, const int* channels, const Mat&
    mask, SparseMat& hist, int dims, const int* histSize, const float** ranges,
    bool uniform=true, bool accumulate=false)

    Calculates histogram of a set of arrays

                  • arrays – Source arrays. They all should have the same
                    depth, CV_8U or CV_32F , and the same size. Each of them
                    can have an arbitrary number of channels
                  • narrays – The number of source arrays
                  • channels – The list of dims channels that are used to
                    compute the histogram. The first array channels are
                    numerated from 0 to arrays[0].channels()-1 , the second
                    array channels are counted from arrays[0].channels() to
                    arrays[0].channels() + arrays[1].channels()-1 etc.
                  • mask – The optional mask. If the matrix is not empty, it
                    must be 8-bit array of the same size as arrays[i] . The
                    non-zero mask elements mark the array elements that are
                    counted in the histogram
                  • hist – The output histogram, a dense or sparse dims
                    -dimensional array
                  • dims – The histogram dimensionality; must be positive and
                    not greater than CV_MAX_DIMS (=32 in the current OpenCV
                    version)
                  • histSize – The array of histogram sizes in each dimension
    Parameters:   • ranges – The array of dims arrays of the histogram bin
                    boundaries in each dimension. When the histogram is uniform
                    ( uniform =true), then for each dimension i it’s enough to
                    specify the lower (inclusive) boundary L_0 of the 0-th
                    histogram bin and the upper (exclusive) boundary U_{\texttt
                    {histSize}[i]-1} for the last histogram bin histSize[i]-1 .
                    That is, in the case of uniform histogram each of ranges[i]
                    is an array of 2 elements. When the histogram is not
                    uniform ( uniform=false ), then each of ranges[i] contains
                    histSize[i]+1 elements: L_0, U_0=L_1, U_1=L_2, ..., U_{\
                    texttt{histSize[i]}-2}=L_{histSize[i]-1}, U_{\
                    texttt{histSize[i]}-1} . The array elements, which are not
                    between L_0 and U_{histSize[i]-1} , are not
                    counted in the histogram
                  • uniform – Indicates whether the histogram is uniform or
                    not, see above
                  • accumulate – Accumulation flag. If it is set, the histogram
                    is not cleared in the beginning (when it is allocated).
                    This feature allows user to compute a single histogram from
                    several sets of arrays, or to update the histogram in time

The functions calcHist calculate the histogram of one or more arrays. The
elements of a tuple that is used to increment a histogram bin are taken at the
same location from the corresponding input arrays. The sample below shows how
to compute 2D Hue-Saturation histogram for a color imag

#include <cv.h>
#include <highgui.h>

using namespace cv;

int main( int argc, char** argv )
{
    Mat src, hsv;
    if( argc != 2 || !(src=imread(argv[1], 1)).data )
        return -1;

    cvtColor(src, hsv, CV_BGR2HSV);

    // let's quantize the hue to 30 levels
    // and the saturation to 32 levels
    int hbins = 30, sbins = 32;
    int histSize[] = {hbins, sbins};
    // hue varies from 0 to 179, see cvtColor
    float hranges[] = { 0, 180 };
    // saturation varies from 0 (black-gray-white) to
    // 255 (pure spectrum color)
    float sranges[] = { 0, 256 };
    const float* ranges[] = { hranges, sranges };
    MatND hist;
    // we compute the histogram from the 0-th and 1-st channels
    int channels[] = {0, 1};

    calcHist( &hsv, 1, channels, Mat(), // do not use mask
             hist, 2, histSize, ranges,
             true, // the histogram is uniform
             false );
    double maxVal=0;
    minMaxLoc(hist, 0, &maxVal, 0, 0);

    int scale = 10;
    Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3);

    for( int h = 0; h < hbins; h++ )
        for( int s = 0; s < sbins; s++ )
        {
            float binVal = hist.at<float>(h, s);
            int intensity = cvRound(binVal*255/maxVal);
            rectangle( histImg, Point(h*scale, s*scale),
                        Point( (h+1)*scale - 1, (s+1)*scale - 1),
                        Scalar::all(intensity),
                        CV_FILLED );
        }

    namedWindow( "Source", 1 );
    imshow( "Source", src );

    namedWindow( "H-S Histogram", 1 );
    imshow( "H-S Histogram", histImg );
    waitKey();
}

===========================================================================
*cv-calcBackProject*

void calcBackProject(const Mat* arrays, int narrays, const int* channels, const
    MatND& hist, Mat& backProject, const float** ranges, double scale=1, bool
    uniform=true)~

void calcBackProject(const Mat* arrays, int narrays, const int* channels, const
    SparseMat& hist, Mat& backProject, const float** ranges, double scale=1,
    bool uniform=true)

    Calculates the back projection of a histogram.

                  • arrays – Source arrays. They all should have the same
                    depth, CV_8U or CV_32F , and the same size. Each of them
                    can have an arbitrary number of channels
                  • narrays – The number of source arrays
                  • channels – The list of channels that are used to compute
                    the back projection. The number of channels must match the
                    histogram dimensionality. The first array channels are
                    numerated from 0 to arrays[0].channels()-1 , the second
                    array channels are counted from arrays[0].channels() to
    Parameters:     arrays[0].channels() + arrays[1].channels()-1 etc.
                  • hist – The input histogram, a dense or sparse
                  • backProject – Destination back projection aray; will be a
                    single-channel array of the same size and the same depth as
                    arrays[0]
                  • ranges – The array of arrays of the histogram bin
                    boundaries in each dimension. See calcHist()
                  • scale – The optional scale factor for the output back
                    projection
                  • uniform – Indicates whether the histogram is uniform or
                    not, see above

The functions calcBackProject calculate the back project of the histogram. That
is, similarly to calcHist , at each location (x, y) the function collects the
values from the selected channels in the input images and finds the
corresponding histogram bin. But instead of incrementing it, the function reads
the bin value, scales it by scale and stores in backProject(x,y) . In terms of
statistics, the function computes probability of each element value in respect
with the empirical probability distribution represented by the histogram. Here
is how, for example, you can find and track a bright-colored object in a scene:

 1. Before the tracking, show the object to the camera such that covers almost
    the whole frame. Calculate a hue histogram. The histogram will likely have
    a strong maximums, corresponding to the dominant colors in the object.
 2. During the tracking, calculate back projection of a hue plane of each input
    video frame using that pre-computed histogram. Threshold the back
    projection to suppress weak colors. It may also have sense to suppress
    pixels with non sufficient color saturation and too dark or too bright
    pixels.
 3. Find connected components in the resulting picture and choose, for example,
    the largest component.

That is the approximate algorithm of CAMShift() color object tracker.

See also: calcHist()

===========================================================================
*cv-compareHist*

double compareHist(const MatND& H1, const MatND& H2, int method)~

double compareHist(const SparseMat& H1, const SparseMat& H2, int method)

    Compares two histograms

                  • H1 – The first compared histogram
                  • H2 – The second compared histogram of the same size as H1
                  • method –

    Parameters:     The comparison method, one of the following:

                      □ CV_COMP_CORREL Correlation
                      □ CV_COMP_CHISQR Chi-Square
                      □ CV_COMP_INTERSECT Intersection
                      □ CV_COMP_BHATTACHARYYA Bhattacharyya distance

The functions compareHist compare two dense or two sparse histograms using the
specified method:

  • Correlation (method=CV_COMP_CORREL)

        d(H_1,H_2) = \frac{\sum_I (H_1(I) - \bar{H_1}) (H_2(I) - \bar{H_2})}{\
        sqrt{\sum_I(H_1(I) - \bar{H_1})^2 \sum_I(H_2(I) - \bar{H_2})^2}}

        where

        \bar{H_k} = \frac{1}{N} \sum _J H_k(J)

        and N is the total number of histogram bins.

  • Chi-Square (method=CV_COMP_CHISQR)

        d(H_1,H_2) = \sum _I \frac{\left(H_1(I)-H_2(I)\right)^2}{H_1(I)+H_2(I)}

  • Intersection (method=CV_COMP_INTERSECT)

        d(H_1,H_2) = \sum _I \min (H_1(I), H_2(I))

  • Bhattacharyya distance (method=CV_COMP_BHATTACHARYYA)

        d(H_1,H_2) = \sqrt{1 - \frac{1}{\sqrt{\bar{H_1} \bar{H_2} N^2}} \sum_I
        \sqrt{H_1(I) * H_2(I)}}

The function returns d(H_1, H_2) .

While the function works well with 1-, 2-, 3-dimensional dense histograms, it
may not be suitable for high-dimensional sparse histograms, where, because of
aliasing and sampling problems the coordinates of non-zero histogram bins can
slightly shift. To compare such histograms or more general sparse
configurations of weighted points, consider using the calcEMD() function.

===========================================================================
*cv-equalizeHist*

void equalizeHist(const Mat& src, Mat& dst)~

    Equalizes the histogram of a grayscale image.

                  • src – The source 8-bit single channel image
    Parameters:   • dst – The destination image; will have the same size and
                    the same type as src

The function equalizes the histogram of the input image using the following
algorithm:

 1. calculate the histogram H for src .

 2. normalize the histogram so that the sum of histogram bins is 255.

 3. compute the integral of the histogram:

    H'_i = \sum _{0 \le j < i} H(j)

 4. transform the image using H' as a look-up table: dst(x,y) = H'(\
    texttt{src}(x,y))

The algorithm normalizes the brightness and increases the contrast of the
image.

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Histograms
      □ cv::calcHist
      □ cv::calcBackProject
      □ cv::compareHist
      □ cv::equalizeHist

Previous topic

imgproc. Image Processing

Next topic

Image Filtering

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Image Filtering~

Functions and classes described in this section are used to perform various
linear or non-linear filtering operations on 2D images (represented as Mat()
‘s), that is, for each pixel location (x,y) in the source image some its
(normally rectangular) neighborhood is considered and used to compute the
response. In case of a linear filter it is a weighted sum of pixel values, in
case of morphological operations it is the minimum or maximum etc. The computed
response is stored to the destination image at the same location (x,y) . It
means, that the output image will be of the same size as the input image.
Normally, the functions supports multi-channel arrays, in which case every
channel is processed independently, therefore the output image will also have
the same number of channels as the input one.

Another common feature of the functions and classes described in this section
is that, unlike simple arithmetic functions, they need to extrapolate values of
some non-existing pixels. For example, if we want to smooth an image using a
Gaussian 3 \times 3 filter, then during the processing of the left-most pixels
in each row we need pixels to the left of them, i.e. outside of the image. We
can let those pixels be the same as the left-most image pixels (i.e. use
“replicated border” extrapolation method), or assume that all the non-existing
pixels are zeros (“contant border” extrapolation method) etc. OpenCV let the
user to specify the extrapolation method; see the function borderInterpolate()
and discussion of borderType parameter in various functions below.

===========================================================================
*cv-BaseColumnFilter*

===========================================================================


Base class for filters with single-column kernels

class BaseColumnFilter
{
public:
    virtual ~BaseColumnFilter();

    // To be overriden by the user.
    //
    // runs filtering operation on the set of rows,
    // "dstcount + ksize - 1" rows on input,
    // "dstcount" rows on output,
    // each input and output row has "width" elements
    // the filtered rows are written into "dst" buffer.
    virtual void operator()(const uchar** src, uchar* dst, int dststep,
                            int dstcount, int width) = 0;
    // resets the filter state (may be needed for IIR filters)
    virtual void reset();

    int ksize; // the aperture size
    int anchor; // position of the anchor point,
                // normally not used during the processing
};

The class BaseColumnFilter is the base class for filtering data using
single-column kernels. The filtering does not have to be a linear operation. In
general, it could be written as following:

dst (x,y) = F( src [y](x), \; src [y+1](x), \; ...,
\; src [y+ ksize -1](x)

where F is the filtering function, but, as it is represented as a class, it can
produce any side effects, memorize previously processed data etc. The class
only defines the interface and is not used directly. Instead, there are several
functions in OpenCV (and you can add more) that return pointers to the derived
classes that implement specific filtering operations. Those pointers are then
passed to FilterEngine() constructor. While the filtering operation interface
uses uchar type, a particular implementation is not limited to 8-bit data.

See also: BaseRowFilter() , BaseFilter() , FilterEngine() ,

getColumnSumFilter() , getLinearColumnFilter() , getMorphologyColumnFilter()

===========================================================================
*cv-BaseFilter*

===========================================================================


Base class for 2D image filters

class BaseFilter
{
public:
    virtual ~BaseFilter();

    // To be overriden by the user.
    //
    // runs filtering operation on the set of rows,
    // "dstcount + ksize.height - 1" rows on input,
    // "dstcount" rows on output,
    // each input row has "(width + ksize.width-1)*cn" elements
    // each output row has "width*cn" elements.
    // the filtered rows are written into "dst" buffer.
    virtual void operator()(const uchar** src, uchar* dst, int dststep,
                            int dstcount, int width, int cn) = 0;
    // resets the filter state (may be needed for IIR filters)
    virtual void reset();
    Size ksize;
    Point anchor;
};

The class BaseFilter is the base class for filtering data using 2D kernels. The
filtering does not have to be a linear operation. In general, it could be
written as following:

\begin{array}{l} dst (x,y) = F( src [y](x), \; src
[y](x+1), \; ..., \; src [y](x+ ksize.width -1), \\ \texttt
{src} [y+1](x), \; src [y+1](x+1), \; ..., \; src [y+1](x+ \
texttt{ksize.width} -1), \\
.........................................................................................
\\ src [y+ \texttt{ksize.height-1} ](x), \\ src [y+ \texttt
{ksize.height-1} ](x+1), \\ ... src [y+ \texttt{ksize.height-1} ](x+ \
texttt{ksize.width} -1)) \end{array}

where F is the filtering function. The class only defines the interface and is
not used directly. Instead, there are several functions in OpenCV (and you can
add more) that return pointers to the derived classes that implement specific
filtering operations. Those pointers are then passed to FilterEngine()
constructor. While the filtering operation interface uses uchar type, a
particular implementation is not limited to 8-bit data.

See also: BaseColumnFilter() , BaseRowFilter() , FilterEngine() ,

getLinearFilter() , getMorphologyFilter()

===========================================================================
*cv-BaseRowFilter*

===========================================================================


Base class for filters with single-row kernels

class BaseRowFilter
{
public:
    virtual ~BaseRowFilter();

    // To be overriden by the user.
    //
    // runs filtering operation on the single input row
    // of "width" element, each element is has "cn" channels.
    // the filtered row is written into "dst" buffer.
    virtual void operator()(const uchar* src, uchar* dst,
                            int width, int cn) = 0;
    int ksize, anchor;
};

The class BaseRowFilter is the base class for filtering data using single-row
kernels. The filtering does not have to be a linear operation. In general, it
could be written as following:

dst (x,y) = F( src [y](x), \; src [y](x+1), \; ...,
\; src [y](x+ ksize.width -1))

where F is the filtering function. The class only defines the interface and is
not used directly. Instead, there are several functions in OpenCV (and you can
add more) that return pointers to the derived classes that implement specific
filtering operations. Those pointers are then passed to FilterEngine()
constructor. While the filtering operation interface uses uchar type, a
particular implementation is not limited to 8-bit data.

See also: BaseColumnFilter() , Filter() , FilterEngine() ,

getLinearRowFilter() , getMorphologyRowFilter() , getRowSumFilter()

===========================================================================
*cv-FilterEngine*

===========================================================================


Generic image filtering class

class FilterEngine
{
public:
    // empty constructor
    FilterEngine();
    // builds a 2D non-separable filter (!_filter2D.empty()) or
    // a separable filter (!_rowFilter.empty() && !_columnFilter.empty())
    // the input data type will be "srcType", the output data type will be "dstType",
    // the intermediate data type is "bufType".
    // _rowBorderType and _columnBorderType determine how the image
    // will be extrapolated beyond the image boundaries.
    // _borderValue is only used when _rowBorderType and/or _columnBorderType
    // == cv::BORDER_CONSTANT
    FilterEngine(const Ptr<BaseFilter>& _filter2D,
                 const Ptr<BaseRowFilter>& _rowFilter,
                 const Ptr<BaseColumnFilter>& _columnFilter,
                 int srcType, int dstType, int bufType,
                 int _rowBorderType=BORDER_REPLICATE,
                 int _columnBorderType=-1, // use _rowBorderType by default
                 const Scalar& _borderValue=Scalar());
    virtual ~FilterEngine();
    // separate function for the engine initialization
    void init(const Ptr<BaseFilter>& _filter2D,
              const Ptr<BaseRowFilter>& _rowFilter,
              const Ptr<BaseColumnFilter>& _columnFilter,
              int srcType, int dstType, int bufType,
              int _rowBorderType=BORDER_REPLICATE, int _columnBorderType=-1,
              const Scalar& _borderValue=Scalar());
    // starts filtering of the ROI in an image of size "wholeSize".
    // returns the starting y-position in the source image.
    virtual int start(Size wholeSize, Rect roi, int maxBufRows=-1);
    // alternative form of start that takes the image
    // itself instead of "wholeSize". Set isolated to true to pretend that
    // there are no real pixels outside of the ROI
    // (so that the pixels will be extrapolated using the specified border modes)
    virtual int start(const Mat& src, const Rect& srcRoi=Rect(0,0,-1,-1),
                      bool isolated=false, int maxBufRows=-1);
    // processes the next portion of the source image,
    // "srcCount" rows starting from "src" and
    // stores the results to "dst".
    // returns the number of produced rows
    virtual int proceed(const uchar* src, int srcStep, int srcCount,
                        uchar* dst, int dstStep);
    // higher-level function that processes the whole
    // ROI or the whole image with a single call
    virtual void apply( const Mat& src, Mat& dst,
                        const Rect& srcRoi=Rect(0,0,-1,-1),
                        Point dstOfs=Point(0,0),
                        bool isolated=false);
    bool isSeparable() const { return filter2D.empty(); }
    // how many rows from the input image are not yet processed
    int remainingInputRows() const;
    // how many output rows are not yet produced
    int remainingOutputRows() const;
    ...
    // the starting and the ending rows in the source image
    int startY, endY;

    // pointers to the filters
    Ptr<BaseFilter> filter2D;
    Ptr<BaseRowFilter> rowFilter;
    Ptr<BaseColumnFilter> columnFilter;
};

The class FilterEngine can be used to apply an arbitrary filtering operation to
an image. It contains all the necessary intermediate buffers, it computes
extrapolated values of the “virtual” pixels outside of the image etc. Pointers
to the initialized FilterEngine instances are returned by various create*Filter
functions, see below, and they are used inside high-level functions such as
filter2D() , erode() , dilate() etc, that is, the class is the workhorse in
many of OpenCV filtering functions.

This class makes it easier (though, maybe not very easy yet) to combine
filtering operations with other operations, such as color space conversions,
thresholding, arithmetic operations, etc. By combining several operations
together you can get much better performance because your data will stay in
cache. For example, below is the implementation of Laplace operator for a
floating-point images, which is a simplified implementation of Laplacian() :

void laplace_f(const Mat& src, Mat& dst)
{
    CV_Assert( src.type() == CV_32F );
    dst.create(src.size(), src.type());

    // get the derivative and smooth kernels for d2I/dx2.
    // for d2I/dy2 we could use the same kernels, just swapped
    Mat kd, ks;
    getSobelKernels( kd, ks, 2, 0, ksize, false, ktype );

    // let's process 10 source rows at once
    int DELTA = std::min(10, src.rows);
    Ptr<FilterEngine> Fxx = createSeparableLinearFilter(src.type(),
        dst.type(), kd, ks, Point(-1,-1), 0, borderType, borderType, Scalar() );
    Ptr<FilterEngine> Fyy = createSeparableLinearFilter(src.type(),
        dst.type(), ks, kd, Point(-1,-1), 0, borderType, borderType, Scalar() );

    int y = Fxx->start(src), dsty = 0, dy = 0;
    Fyy->start(src);
    const uchar* sptr = src.data + y*src.step;

    // allocate the buffers for the spatial image derivatives;
    // the buffers need to have more than DELTA rows, because at the
    // last iteration the output may take max(kd.rows-1,ks.rows-1)
    // rows more than the input.
    Mat Ixx( DELTA + kd.rows - 1, src.cols, dst.type() );
    Mat Iyy( DELTA + kd.rows - 1, src.cols, dst.type() );

    // inside the loop we always pass DELTA rows to the filter
    // (note that the "proceed" method takes care of possibe overflow, since
    // it was given the actual image height in the "start" method)
    // on output we can get:
    //  * < DELTA rows (the initial buffer accumulation stage)
    //  * = DELTA rows (settled state in the middle)
    //  * > DELTA rows (then the input image is over, but we generate
    //                  "virtual" rows using the border mode and filter them)
    // this variable number of output rows is dy.
    // dsty is the current output row.
    // sptr is the pointer to the first input row in the portion to process
    for( ; dsty < dst.rows; sptr += DELTA*src.step, dsty += dy )
    {
        Fxx->proceed( sptr, (int)src.step, DELTA, Ixx.data, (int)Ixx.step );
        dy = Fyy->proceed( sptr, (int)src.step, DELTA, d2y.data, (int)Iyy.step );
        if( dy > 0 )
        {
            Mat dstripe = dst.rowRange(dsty, dsty + dy);
            add(Ixx.rowRange(0, dy), Iyy.rowRange(0, dy), dstripe);
        }
    }
}

If you do not need that much control of the filtering process, you can simply
use the FilterEngine::apply method. Here is how the method is actually
implemented:

void FilterEngine::apply(const Mat& src, Mat& dst,
    const Rect& srcRoi, Point dstOfs, bool isolated)
{
    // check matrix types
    CV_Assert( src.type() == srcType && dst.type() == dstType );

    // handle the "whole image" case
    Rect _srcRoi = srcRoi;
    if( _srcRoi == Rect(0,0,-1,-1) )
        _srcRoi = Rect(0,0,src.cols,src.rows);

    // check if the destination ROI is inside the dst.
    // and FilterEngine::start will check if the source ROI is inside src.
    CV_Assert( dstOfs.x >= 0 && dstOfs.y >= 0 &&
        dstOfs.x + _srcRoi.width <= dst.cols &&
        dstOfs.y + _srcRoi.height <= dst.rows );

    // start filtering
    int y = start(src, _srcRoi, isolated);

    // process the whole ROI. Note that "endY - startY" is the total number
    // of the source rows to process
    // (including the possible rows outside of srcRoi but inside the source image)
    proceed( src.data + y*src.step,
             (int)src.step, endY - startY,
             dst.data + dstOfs.y*dst.step +
             dstOfs.x*dst.elemSize(), (int)dst.step );
}

Unlike the earlier versions of OpenCV, now the filtering operations fully
support the notion of image ROI, that is, pixels outside of the ROI but inside
the image can be used in the filtering operations. For example, you can take a
ROI of a single pixel and filter it - that will be a filter response at that
particular pixel (however, it’s possible to emulate the old behavior by passing
isolated=false to FilterEngine::start or FilterEngine::apply ). You can pass
the ROI explicitly to FilterEngine::apply , or construct a new matrix headers:

// compute dI/dx derivative at src(x,y)

// method 1:
// form a matrix header for a single value
float val1 = 0;
Mat dst1(1,1,CV_32F,&val1);

Ptr<FilterEngine> Fx = createDerivFilter(CV_32F, CV_32F,
                        1, 0, 3, BORDER_REFLECT_101);
Fx->apply(src, Rect(x,y,1,1), Point(), dst1);

// method 2:
// form a matrix header for a single value
float val2 = 0;
Mat dst2(1,1,CV_32F,&val2);

Mat pix_roi(src, Rect(x,y,1,1));
Sobel(pix_roi, dst2, dst2.type(), 1, 0, 3, 1, 0, BORDER_REFLECT_101);

printf("method1 =

Note on the data types. As it was mentioned in BaseFilter() description, the
specific filters can process data of any type, despite that
Base*Filter::operator() only takes uchar pointers and no information about the
actual types. To make it all work, the following rules are used:

  • in case of separable filtering FilterEngine::rowFilter applied first. It
    transforms the input image data (of type srcType ) to the intermediate
    results stored in the internal buffers (of type bufType ). Then these
    intermediate results are processed as single-channel data with
    FilterEngine::columnFilter and stored in the output image (of type dstType
    ). Thus, the input type for rowFilter is srcType and the output type is
    bufType ; the input type for columnFilter is CV_MAT_DEPTH(bufType) and the
    output type is CV_MAT_DEPTH(dstType) .
  • in case of non-separable filtering bufType must be the same as srcType .
    The source data is copied to the temporary buffer if needed and then just
    passed to FilterEngine::filter2D . That is, the input type for filter2D is
    srcType (= bufType ) and the output type is dstType .

See also: BaseColumnFilter() , BaseFilter() , BaseRowFilter() , createBoxFilter
() , createDerivFilter() , createGaussianFilter() , createLinearFilter() ,
createMorphologyFilter() , createSeparableLinearFilter()

===========================================================================
*cv-bilateralFilter*

void bilateralFilter(const Mat& src, Mat& dst, int d, double sigmaColor, double
    sigmaSpace, int borderType=BORDER_DEFAULT)~

    Applies bilateral filter to the image

                  • src – The source 8-bit or floating-point, 1-channel or
                    3-channel image
                  • dst – The destination image; will have the same size and
                    the same type as src
                  • d – The diameter of each pixel neighborhood, that is used
                    during filtering. If it is non-positive, it’s computed from
                    sigmaSpace
                  • sigmaColor – Filter sigma in the color space. Larger value
    Parameters:     of the parameter means that farther colors within the pixel
                    neighborhood (see sigmaSpace ) will be mixed together,
                    resulting in larger areas of semi-equal color
                  • sigmaSpace – Filter sigma in the coordinate space. Larger
                    value of the parameter means that farther pixels will
                    influence each other (as long as their colors are close
                    enough; see sigmaColor ). Then d>0 , it specifies the
                    neighborhood size regardless of sigmaSpace , otherwise d is
                    proportional to sigmaSpace

The function applies bilateral filtering to the input image, as described in
http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/
Bilateral_Filtering.html

===========================================================================
*cv-blur*

void blur(const Mat& src, Mat& dst, Size ksize, Point anchor=Point(-1, -1), int
    borderType=BORDER_DEFAULT)~

    Smoothes image using normalized box filter

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same type as src
    Parameters:   • ksize – The smoothing kernel size
                  • anchor – The anchor point. The default value Point(-1,-1)
                    means that the anchor is at the kernel center
                  • borderType – The border mode used to extrapolate pixels
                    outside of the image

The function smoothes the image using the kernel:

K = \frac{1}{ksize.width*ksize.height} \begin{bmatrix} 1 & 1
& 1 & *s & 1 & 1 \\ 1 & 1 & 1 & *s & 1 & 1 \\ \hdotsfor{6} \\ 1 & 1 & 1
& *s & 1 & 1 \\ \end{bmatrix}

The call blur(src, dst, ksize, anchor, borderType) is equivalent to boxFilter
(src, dst, src.type(), anchor, true, borderType) .

See also: boxFilter() , bilateralFilter() , GaussianBlur() , medianBlur() .

===========================================================================
*cv-borderInterpolate*

int borderInterpolate(int p, int len, int borderType)~

    Computes source location of extrapolated pixel

                  • p – 0-based coordinate of the extrapolated pixel along one
                    of the axes, likely <0 or >= len
                  • len – length of the array along the corresponding axis
    Parameters:   • borderType – the border type, one of the BORDER_* , except
                    for BORDER_TRANSPARENT and BORDER_ISOLATED . When
                    borderType==BORDER_CONSTANT the function always returns -1,
                    regardless of p and len

The function computes and returns the coordinate of the donor pixel,
corresponding to the specified extrapolated pixel when using the specified
extrapolation border mode. For example, if we use BORDER_WRAP mode in the
horizontal direction, BORDER_REFLECT_101 in the vertical direction and want to
compute value of the “virtual” pixel Point(-5, 100) in a floating-point image
img , it will be

float val = img.at<float>(borderInterpolate(100, img.rows, BORDER_REFLECT_101),
                          borderInterpolate(-5, img.cols, BORDER_WRAP));

Normally, the function is not called directly; it is used inside FilterEngine()
and copyMakeBorder() to compute tables for quick extrapolation.

See also: FilterEngine() , copyMakeBorder()

===========================================================================
*cv-boxFilter*

void boxFilter(const Mat& src, Mat& dst, int ddepth, Size ksize, Point anchor=
    Point(-1, -1), bool normalize=true, int borderType=BORDER_DEFAULT)~

    Smoothes image using box filter

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same type as src
                  • ksize – The smoothing kernel size
    Parameters:   • anchor – The anchor point. The default value Point(-1,-1)
                    means that the anchor is at the kernel center
                  • normalize – Indicates, whether the kernel is normalized by
                    its area or not
                  • borderType – The border mode used to extrapolate pixels
                    outside of the image

The function smoothes the image using the kernel:

K = \alpha \begin{bmatrix} 1 & 1 & 1 & *s & 1 & 1 \\ 1 & 1 & 1 & \
cdots & 1 & 1 \\ \hdotsfor{6} \\ 1 & 1 & 1 & *s & 1 & 1 \end{bmatrix}

where

\alpha = \fork{\frac{1}{ksize.width*ksize.height}}{when \texttt
{normalize=true}}{1}{otherwise}

Unnormalized box filter is useful for computing various integral
characteristics over each pixel neighborhood, such as covariation matrices of
image derivatives (used in dense optical flow algorithms, etc.). If you need to
compute pixel sums over variable-size windows, use integral() .

See also: boxFilter() , bilateralFilter() , GaussianBlur() , medianBlur() ,
integral() .

===========================================================================
*cv-buildPyramid*

void buildPyramid(const Mat& src, vector<Mat>& dst, int maxlevel)~

    Constructs Gaussian pyramid for an image

                  • src – The source image; check pyrDown() for the list of
                    supported types
                  • dst – The destination vector of maxlevel+1 images of the
    Parameters:     same type as src ; dst[0] will be the same as src , dst[1]
                    is the next pyramid layer, a smoothed and down-sized src
                    etc.
                  • maxlevel – The 0-based index of the last (i.e. the
                    smallest) pyramid layer; it must be non-negative

The function constructs a vector of images and builds the gaussian pyramid by
recursively applying pyrDown() to the previously built pyramid layers, starting
from dst[0]==src .

===========================================================================
*cv-copyMakeBorder*

void copyMakeBorder(const Mat& src, Mat& dst, int top, int bottom, int left,
    int right, int borderType, const Scalar& value=Scalar())~

    Forms a border around the image

                  • src – The source image
                  • dst – The destination image; will have the same type as src
                    and the size Size(src.cols+left+right, src.rows+top+bottom)
                  • top, bottom, left, right – Specify how much pixels in each
    Parameters:     direction from the source image rectangle one needs to
                    extrapolate, e.g. top=1, bottom=1, left=1, right=1 mean
                    that 1 pixel-wide border needs to be built
                  • borderType – The border type; see borderInterpolate()
                  • value – The border value if borderType==BORDER_CONSTANT

The function copies the source image into the middle of the destination image.
The areas to the left, to the right, above and below the copied source image
will be filled with extrapolated pixels. This is not what FilterEngine() or
based on it filtering functions do (they extrapolate pixels on-fly), but what
other more complex functions, including your own, may do to simplify image
boundary handling.

The function supports the mode when src is already in the middle of dst . In
this case the function does not copy src itself, but simply constructs the
border, e.g.:

// let border be the same in all directions
int border=2;
// constructs a larger image to fit both the image and the border
Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());
// select the middle part of it w/o copying data
Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));
// convert image from RGB to grayscale
cvtColor(rgb, gray, CV_RGB2GRAY);
// form a border in-place
copyMakeBorder(gray, gray_buf, border, border,
               border, border, BORDER_REPLICATE);
// now do some custom filtering ...
...

See also: borderInterpolate()

===========================================================================
*cv-createBoxFilter*

Ptr<FilterEngine> createBoxFilter(int srcType, int dstType, Size ksize, Point
    anchor=Point(-1, -1), bool normalize=true, int borderType=BORDER_DEFAULT)~

Ptr<BaseRowFilter> getRowSumFilter(int srcType, int sumType, int ksize, int
    anchor=-1)~

Ptr<BaseColumnFilter> getColumnSumFilter(int sumType, int dstType, int ksize,
    int anchor=-1, double scale=1)~

    Returns box filter engine

                  • srcType – The source image type
                  • sumType – The intermediate horizontal sum type; must have
                    as many channels as srcType
                  • dstType – The destination image type; must have as many
                    channels as srcType
                  • ksize – The aperture size
    Parameters:   • anchor – The anchor position with the kernel; negative
                    values mean that the anchor is at the kernel center
                  • normalize – Whether the sums are normalized or not; see
                    boxFilter()
                  • scale – Another way to specify normalization in lower-level
                    getColumnSumFilter
                  • borderType – Which border type to use; see
                    borderInterpolate()

The function is a convenience function that retrieves horizontal sum primitive
filter with getRowSumFilter() , vertical sum filter with getColumnSumFilter() ,
constructs new FilterEngine() and passes both of the primitive filters there.
The constructed filter engine can be used for image filtering with normalized
or unnormalized box filter.

The function itself is used by blur() and boxFilter() .

See also: FilterEngine() , blur() , boxFilter() .

===========================================================================
*cv-createDerivFilter*

Ptr<FilterEngine> createDerivFilter(int srcType, int dstType, int dx, int dy,
    int ksize, int borderType=BORDER_DEFAULT)~

    Returns engine for computing image derivatives

                  • srcType – The source image type
                  • dstType – The destination image type; must have as many
                    channels as srcType
    Parameters:   • dx – The derivative order in respect with x
                  • dy – The derivative order in respect with y
                  • ksize – The aperture size; see getDerivKernels()
                  • borderType – Which border type to use; see
                    borderInterpolate()

The function createDerivFilter() is a small convenience function that retrieves
linear filter coefficients for computing image derivatives using
getDerivKernels() and then creates a separable linear filter with
createSeparableLinearFilter() . The function is used by Sobel() and Scharr() .

See also: createSeparableLinearFilter() , getDerivKernels() , Scharr() , Sobel
() .

===========================================================================
*cv-createGaussianFilter*

Ptr<FilterEngine> createGaussianFilter(int type, Size ksize, double sigmaX,
    double sigmaY=0, int borderType=BORDER_DEFAULT)~

    Returns engine for smoothing images with a Gaussian filter

                  • type – The source and the destination image type
                  • ksize – The aperture size; see getGaussianKernel()
                  • sigmaX – The Gaussian sigma in the horizontal direction;
    Parameters:     see getGaussianKernel()
                  • sigmaY – The Gaussian sigma in the vertical direction; if
                    0, then sigmaY\leftarrowsigmaX
                  • borderType – Which border type to use; see
                    borderInterpolate()

The function createGaussianFilter() computes Gaussian kernel coefficients and
then returns separable linear filter for that kernel. The function is used by
GaussianBlur() . Note that while the function takes just one data type, both
for input and output, you can pass by this limitation by calling
getGaussianKernel() and then createSeparableFilter() directly.

See also: createSeparableLinearFilter() , getGaussianKernel() , GaussianBlur()
.

===========================================================================
*cv-createLinearFilter*

Ptr<FilterEngine> createLinearFilter(int srcType, int dstType, const Mat&
    kernel, Point _anchor=Point(-1, -1), double delta=0, int rowBorderType=
    BORDER_DEFAULT, int columnBorderType=-1, const Scalar& borderValue=Scalar()
    )~

Ptr<BaseFilter> getLinearFilter(int srcType, int dstType, const Mat& kernel,
    Point anchor=Point(-1, -1), double delta=0, int bits=0)~

    Creates non-separable linear filter engine

                  • srcType – The source image type
                  • dstType – The destination image type; must have as many
                    channels as srcType
                  • kernel – The 2D array of filter coefficients
                  • anchor – The anchor point within the kernel; special value
                    Point(-1,-1) means that the anchor is at the kernel center
                  • delta – The value added to the filtered results before
    Parameters:     storing them
                  • bits – When the kernel is an integer matrix representing
                    fixed-point filter coefficients, the parameter specifies
                    the number of the fractional bits
                  • rowBorderType, columnBorderType – The pixel extrapolation
                    methods in the horizontal and the vertical directions; see
                    borderInterpolate()
                  • borderValue – Used in case of constant border

The function returns pointer to 2D linear filter for the specified kernel, the
source array type and the destination array type. The function is a
higher-level function that calls getLinearFilter and passes the retrieved 2D
filter to FilterEngine() constructor.

See also: createSeparableLinearFilter() , FilterEngine() , filter2D()

===========================================================================
*cv-createMorphologyFilter*

Ptr<FilterEngine> createMorphologyFilter(int op, int type, const Mat& element,
    Point anchor=Point(-1, -1), int rowBorderType=BORDER_CONSTANT, int
    columnBorderType=-1, const Scalar& borderValue=morphologyDefaultBorderValue
    ())~

Ptr<BaseFilter> getMorphologyFilter(int op, int type, const Mat& element, Point
    anchor=Point(-1, -1))~

Ptr<BaseRowFilter> getMorphologyRowFilter(int op, int type, int esize, int
    anchor=-1)~

Ptr<BaseColumnFilter> getMorphologyColumnFilter(int op, int type, int esize,
    int anchor=-1)~

static inline Scalar morphologyDefaultBorderValue(){ return Scalar::all
    (DBL_MAX) }

    Creates engine for non-separable morphological operations

                  • op – The morphology operation id, MORPH_ERODE or
                    MORPH_DILATE
                  • type – The input/output image type
                  • element – The 2D 8-bit structuring element for the
                    morphological operation. Non-zero elements indicate the
                    pixels that belong to the element
                  • esize – The horizontal or vertical structuring element size
                    for separable morphological operations
                  • anchor – The anchor position within the structuring
    Parameters:     element; negative values mean that the anchor is at the
                    center
                  • rowBorderType, columnBorderType – The pixel extrapolation
                    methods in the horizontal and the vertical directions; see
                    borderInterpolate()
                  • borderValue – The border value in case of a constant
                    border. The default value, morphologyDefaultBorderValue ,
                    has the special meaning. It is transformed +\inf for the
                    erosion and to -\inf for the dilation, which means that the
                    minimum (maximum) is effectively computed only over the
                    pixels that are inside the image.

The functions construct primitive morphological filtering operations or a
filter engine based on them. Normally it’s enough to use createMorphologyFilter
() or even higher-level erode() , dilate() or morphologyEx() , Note, that
createMorphologyFilter() analyses the structuring element shape and builds a
separable morphological filter engine when the structuring element is square.

See also: erode() , dilate() , morphologyEx() , FilterEngine()

===========================================================================
*cv-createSeparableLinearFilter*

Ptr<FilterEngine> createSeparableLinearFilter(int srcType, int dstType, const 
    Mat& rowKernel, const Mat& columnKernel, Point anchor=Point(-1, -1), double
    delta=0, int rowBorderType=BORDER_DEFAULT, int columnBorderType=-1, const
    Scalar& borderValue=Scalar())~

Ptr<BaseColumnFilter> getLinearColumnFilter(int bufType, int dstType, const Mat
    & columnKernel, int anchor, int symmetryType, double delta=0, int bits=0)~

Ptr<BaseRowFilter> getLinearRowFilter(int srcType, int bufType, const Mat&
    rowKernel, int anchor, int symmetryType)~

    Creates engine for separable linear filter

                  • srcType – The source array type
                  • dstType – The destination image type; must have as many
                    channels as srcType
                  • bufType – The inermediate buffer type; must have as many
                    channels as srcType
                  • rowKernel – The coefficients for filtering each row
                  • columnKernel – The coefficients for filtering each column
                  • anchor – The anchor position within the kernel; negative
                    values mean that anchor is positioned at the aperture
                    center
    Parameters:   • delta – The value added to the filtered results before
                    storing them
                  • bits – When the kernel is an integer matrix representing
                    fixed-point filter coefficients, the parameter specifies
                    the number of the fractional bits
                  • rowBorderType, columnBorderType – The pixel extrapolation
                    methods in the horizontal and the vertical directions; see
                    borderInterpolate()
                  • borderValue – Used in case of a constant border
                  • symmetryType – The type of each of the row and column
                    kernel; see getKernelType() .

The functions construct primitive separable linear filtering operations or a
filter engine based on them. Normally it’s enough to use
createSeparableLinearFilter() or even higher-level sepFilter2D() . The function
createMorphologyFilter() is smart enough to figure out the symmetryType for
each of the two kernels, the intermediate bufType , and, if the filtering can
be done in integer arithmetics, the number of bits to encode the filter
coefficients. If it does not work for you, it’s possible to call
getLinearColumnFilter , getLinearRowFilter directly and then pass them to
FilterEngine() constructor.

See also: sepFilter2D() , createLinearFilter() , FilterEngine() , getKernelType
()

===========================================================================
*cv-dilate*

void dilate(const Mat& src, Mat& dst, const Mat& element, Point anchor=Point(-1
    , -1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar&
    borderValue=morphologyDefaultBorderValue())~

    Dilates an image by using a specific structuring element.

                  • src – The source image
                  • dst – The destination image. It will have the same size and
                    the same type as src
                  • element – The structuring element used for dilation. If
                    element=Mat() , a 3\times 3 rectangular structuring element
                    is used
                  • anchor – Position of the anchor within the element. The
    Parameters:     default value (-1, -1) means that the anchor is at the
                    element center
                  • iterations – The number of times dilation is applied
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()
                  • borderValue – The border value in case of a constant
                    border. The default value has a special meaning, see
                    createMorphologyFilter()

The function dilates the source image using the specified structuring element
that determines the shape of a pixel neighborhood over which the maximum is
taken:

dst (x,y) = \max _{(x',y'): \, element (x',y') \ne0 } \texttt
{src} (x+x',y+y')

The function supports the in-place mode. Dilation can be applied several (
iterations ) times. In the case of multi-channel images each channel is
processed independently.

See also: erode() , morphologyEx() , createMorphologyFilter()

===========================================================================
*cv-erode*

void erode(const Mat& src, Mat& dst, const Mat& element, Point anchor=Point(-1,
    -1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar&
    borderValue=morphologyDefaultBorderValue())~

    Erodes an image by using a specific structuring element.

                  • src – The source image
                  • dst – The destination image. It will have the same size and
                    the same type as src
                  • element – The structuring element used for dilation. If
                    element=Mat() , a 3\times 3 rectangular structuring element
                    is used
                  • anchor – Position of the anchor within the element. The
    Parameters:     default value (-1, -1) means that the anchor is at the
                    element center
                  • iterations – The number of times erosion is applied
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()
                  • borderValue – The border value in case of a constant
                    border. The default value has a special meaning, see
                    createMorphoogyFilter()

The function erodes the source image using the specified structuring element
that determines the shape of a pixel neighborhood over which the minimum is
taken:

dst (x,y) = \min _{(x',y'): \, element (x',y') \ne0 } \texttt
{src} (x+x',y+y')

The function supports the in-place mode. Erosion can be applied several (
iterations ) times. In the case of multi-channel images each channel is
processed independently.

See also: dilate() , morphologyEx() , createMorphologyFilter()

===========================================================================
*cv-filter2D*

void filter2D(const Mat& src, Mat& dst, int ddepth, const Mat& kernel, Point
    anchor=Point(-1, -1), double delta=0, int borderType=BORDER_DEFAULT)~

    Convolves an image with the kernel

                  • src – The source image
                  • dst – The destination image. It will have the same size and
                    the same number of channels as src
                  • ddepth – The desired depth of the destination image. If it
                    is negative, it will be the same as src.depth()
                  • kernel – Convolution kernel (or rather a correlation
                    kernel), a single-channel floating point matrix. If you
                    want to apply different kernels to different channels,
                    split the image into separate color planes using split()
    Parameters:     and process them individually
                  • anchor – The anchor of the kernel that indicates the
                    relative position of a filtered point within the kernel.
                    The anchor should lie within the kernel. The special
                    default value (-1,-1) means that the anchor is at the
                    kernel center
                  • delta – The optional value added to the filtered pixels
                    before storing them in dst
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()

The function applies an arbitrary linear filter to the image. In-place
operation is supported. When the aperture is partially outside the image, the
function interpolates outlier pixel values according to the specified border
mode.

The function does actually computes correlation, not the convolution:

dst (x,y) = \sum _{ \stackrel{0\leq x' < kernel.cols,}{0\leq
y' < kernel.rows} } kernel (x',y')* src (x+x'- \
texttt{anchor.x} ,y+y'- anchor.y )

That is, the kernel is not mirrored around the anchor point. If you need a real
convolution, flip the kernel using flip() and set the new anchor to
(kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1) .

The function uses -based algorithm in case of sufficiently large kernels (~ 11\
times11 ) and the direct algorithm (that uses the engine retrieved by
createLinearFilter() ) for small kernels.

See also: sepFilter2D() , createLinearFilter() , dft() , matchTemplate()

===========================================================================
*cv-GaussianBlur*

void GaussianBlur(const Mat& src, Mat& dst, Size ksize, double sigmaX, double
    sigmaY=0, int borderType=BORDER_DEFAULT)~

    Smoothes image using a Gaussian filter

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same type as src
                  • ksize – The Gaussian kernel size; ksize.width and
                    ksize.height can differ, but they both must be positive and
                    odd. Or, they can be zero’s, then they are computed from
                    sigma*
                  • sigmaX, sigmaY – The Gaussian kernel standard deviations in
    Parameters:     X and Y direction. If sigmaY is zero, it is set to be equal
                    to sigmaX . If they are both zeros, they are computed from
                    ksize.width and ksize.height , respectively, see
                    getGaussianKernel() . To fully control the result
                    regardless of possible future modification of all this
                    semantics, it is recommended to specify all of ksize ,
                    sigmaX and sigmaY
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()

The function convolves the source image with the specified Gaussian kernel.
In-place filtering is supported.

See also: sepFilter2D() , filter2D() , blur() , boxFilter() , bilateralFilter()
, medianBlur()

===========================================================================
*cv-getDerivKernels*

void getDerivKernels(Mat& kx, Mat& ky, int dx, int dy, int ksize, bool
    normalize=false, int ktype=CV_32F)~

    Returns filter coefficients for computing spatial image derivatives

                  • kx – The output matrix of row filter coefficients; will
                    have type ktype
                  • ky – The output matrix of column filter coefficients; will
                    have type ktype
                  • dx – The derivative order in respect with x
                  • dy – The derivative order in respect with y
                  • ksize – The aperture size. It can be CV_SCHARR , 1, 3, 5 or
                    7
    Parameters:   • normalize – Indicates, whether to normalize (scale down)
                    the filter coefficients or not. In theory the coefficients
                    should have the denominator =2^{ksize*2-dx-dy-2} . If you
                    are going to filter floating-point images, you will likely
                    want to use the normalized kernels. But if you compute
                    derivatives of a 8-bit image, store the results in 16-bit
                    image and wish to preserve all the fractional bits, you may
                    want to set normalize=false .
                  • ktype – The type of filter coefficients. It can be CV_32f
                    or CV_64F

The function computes and returns the filter coefficients for spatial image
derivatives. When ksize=CV_SCHARR , the Scharr 3 \times 3 kernels are
generated, see Scharr() . Otherwise, Sobel kernels are generated, see Sobel() .
The filters are normally passed to sepFilter2D() or to
createSeparableLinearFilter() .

===========================================================================
*cv-getGaussianKernel*

Mat getGaussianKernel(int ksize, double sigma, int ktype=CV_64F)~

    Returns Gaussian filter coefficients

                  • ksize – The aperture size. It should be odd ( \texttt
                    {ksize} \mod 2 = 1 ) and positive.
                  • sigma – The Gaussian standard deviation. If it is
    Parameters:     non-positive, it is computed from ksize as sigma = 0.3*
                    (ksize/2 - 1) + 0.8
                  • ktype – The type of filter coefficients. It can be CV_32f
                    or CV_64F

The function computes and returns the ksize \times 1 matrix of
Gaussian filter coefficients:

G_i= \alpha *e^{-(i-( ksize -1)/2)^2/(2* sigma )^2},

where i=0..ksize-1 and \alpha is the scale factor chosen so that \
sum_i G_i=1 Two of such generated kernels can be passed to sepFilter2D() or to
createSeparableLinearFilter() that will automatically detect that these are
smoothing kernels and handle them accordingly. Also you may use the
higher-level GaussianBlur() .

See also: sepFilter2D() , createSeparableLinearFilter() , getDerivKernels() ,
getStructuringElement() , GaussianBlur() .

===========================================================================
*cv-getKernelType*

int getKernelType(const Mat& kernel, Point anchor)~

    Returns the kernel type

    Parameters:   • kernel – 1D array of the kernel coefficients to analyze
                  • anchor – The anchor position within the kernel

The function analyzes the kernel coefficients and returns the corresponding
kernel type:

      □ KERNEL_GENERAL Generic kernel - when there is no any type of symmetry
        or other properties
      □ KERNEL_SYMMETRICAL The kernel is symmetrical: kernel_i == \
        texttt{kernel}_{ksize-i-1} and the anchor is at the center
      □ KERNEL_ASYMMETRICAL The kernel is asymmetrical: kernel_i == -\
        texttt{kernel}_{ksize-i-1} and the anchor is at the center
      □ KERNEL_SMOOTH All the kernel elements are non-negative and sum to 1.
        E.g. the Gaussian kernel is both smooth kernel and symmetrical, so the
        function will return KERNEL_SMOOTH | KERNEL_SYMMETRICAL
      □ KERNEL_INTEGER Al the kernel coefficients are integer numbers. This
        flag can be combined with KERNEL_SYMMETRICAL or KERNEL_ASYMMETRICAL

===========================================================================
*cv-getStructuringElement*

Mat getStructuringElement(int shape, Size esize, Point anchor=Point(-1, -1))~

    Returns the structuring element of the specified size and shape for
    morphological operations

    Parameter: shape – The element shape, one of:

      □ MORPH_RECT - rectangular structuring element

        E_{ij}=1

      □ MORPH_ELLIPSE - elliptic structuring element, i.e. a filled ellipse
        inscribed into the rectangle

        Rect(0, 0, esize.width, 0.esize.height)

      □ MORPH_CROSS - cross-shaped structuring element:

        E_{ij} = \fork{1}{if i=anchor.y or j=anchor.x}{0}
        {otherwise}

                  • esize – Size of the structuring element
                  • anchor – The anchor position within the element. The
                    default value (-1, -1) means that the anchor is at the
    Parameters:     center. Note that only the cross-shaped element’s shape
                    depends on the anchor position; in other cases the anchor
                    just regulates by how much the result of the morphological
                    operation is shifted

The function constructs and returns the structuring element that can be then
passed to createMorphologyFilter() , erode() , dilate() or morphologyEx() . But
also you can construct an arbitrary binary mask yourself and use it as the
structuring element.

===========================================================================
*cv-medianBlur*

void medianBlur(const Mat& src, Mat& dst, int ksize)~

    Smoothes image using median filter

                  • src – The source 1-, 3- or 4-channel image. When ksize is 3
                    or 5, the image depth should be CV_8U , CV_16U or CV_32F .
                    For larger aperture sizes it can only be CV_8U
    Parameters:   • dst – The destination array; will have the same size and
                    the same type as src
                  • ksize – The aperture linear size. It must be odd and more
                    than 1, i.e. 3, 5, 7 ...

The function smoothes image using the median filter with ksize \times
ksize aperture. Each channel of a multi-channel image is processed
independently. In-place operation is supported.

See also: bilateralFilter() , blur() , boxFilter() , GaussianBlur()

===========================================================================
*cv-morphologyEx*

void morphologyEx(const Mat& src, Mat& dst, int op, const Mat& element, Point
    anchor=Point(-1, -1), int iterations=1, int borderType=BORDER_CONSTANT,
    const Scalar& borderValue=morphologyDefaultBorderValue())~

    Performs advanced morphological transformations

                  • src – Source image
                  • dst – Destination image. It will have the same size and the
                    same type as src
                  • element – Structuring element
                  • op –

                    Type of morphological operation, one of the following:

                      □ MORPH_OPEN opening
    Parameters:       □ MORPH_CLOSE closing
                      □ MORPH_GRADIENT morphological gradient
                      □ MORPH_TOPHAT “top hat”
                      □ MORPH_BLACKHAT “black hat”
                  • iterations – Number of times erosion and dilation are
                    applied
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()
                  • borderValue – The border value in case of a constant
                    border. The default value has a special meaning, see
                    createMorphoogyFilter()

The function can perform advanced morphological transformations using erosion
and dilation as basic operations.

Opening:

dst = open ( src , element )= \mathrm
{dilate} ( erode ( src , element ))

Closing:

dst = close ( src , element )= \mathrm
{erode} ( dilate ( src , element ))

Morphological gradient:

dst = \mathrm{morph\_grad} ( src , element )= \
mathrm{dilate} ( src , element )- erode ( \texttt
{src} , element )

“Top hat”:

dst = tophat ( src , element )= \texttt
{src} - open ( src , element )

“Black hat”:

dst = blackhat ( src , element )= \mathrm
{close} ( src , element )- src

Any of the operations can be done in-place.

See also: dilate() , erode() , createMorphologyFilter()

===========================================================================
*cv-Laplacian*

void Laplacian(const Mat& src, Mat& dst, int ddepth, int ksize=1, double scale=
    1, double delta=0, int borderType=BORDER_DEFAULT)~

    Calculates the Laplacian of an image

                  • src – Source image
                  • dst – Destination image; will have the same size and the
                    same number of channels as src
                  • ddepth – The desired depth of the destination image
                  • ksize – The aperture size used to compute the
                    second-derivative filters, see getDerivKernels() . It must
    Parameters:     be positive and odd
                  • scale – The optional scale factor for the computed
                    Laplacian values (by default, no scaling is applied, see
                    getDerivKernels() )
                  • delta – The optional delta value, added to the results
                    prior to storing them in dst
                  • borderType – The pixel extrapolation method, see
                    borderInterpolate()

The function calculates the Laplacian of the source image by adding up the
second x and y derivatives calculated using the Sobel operator:

dst = \Delta src = \frac{\partial^2 src}{\partial x^
2} + \frac{\partial^2 src}{\partial y^2}

This is done when ksize > 1 . When ksize == 1 , the Laplacian is computed by
filtering the image with the following 3 \times 3 aperture:

\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}

See also: Sobel() , Scharr()

===========================================================================
*cv-pyrDown*

void pyrDown(const Mat& src, Mat& dst, const Size& dstsize=Size())~

    Smoothes an image and downsamples it.

                  • src – The source image
                  • dst – The destination image. It will have the specified
                    size and the same type as src
                  • dstsize –

    Parameters:     Size of the destination image. By default it is computed as
                    Size((src.cols+1)/2, (src.rows+1)/2) . But in any case the
                    following conditions should be satisfied:

                    \begin{array}{l} | dstsize.width *2-src.cols| \leq
                    2 \\ | dstsize.height *2-src.rows| \leq 2 \end
                    {array}

The function performs the downsampling step of the Gaussian pyramid
construction. First it convolves the source image with the kernel:

\frac{1}{16} \begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\ 4 & 16 & 24 & 16 & 4 \\ 6 &
24 & 36 & 24 & 6 \\ 4 & 16 & 24 & 16 & 4 \\ 1 & 4 & 6 & 4 & 1 \end{bmatrix}

and then downsamples the image by rejecting even rows and columns.

===========================================================================
*cv-pyrUp*

void pyrUp(const Mat& src, Mat& dst, const Size& dstsize=Size())~

    Upsamples an image and then smoothes it

                  • src – The source image
                  • dst – The destination image. It will have the specified
                    size and the same type as src
                  • dstsize –

                    Size of the destination image. By default it is computed as
    Parameters:     Size(src.cols*2, (src.rows*2) . But in any case the
                    following conditions should be satisfied:

                    \begin{array}{l} | dstsize.width -src.cols*2| \leq
                    ( dstsize.width \mod 2) \\ | \texttt
                    {dstsize.height} -src.rows*2| \leq ( \texttt
                    {dstsize.height} \mod 2) \end{array}

The function performs the upsampling step of the Gaussian pyramid construction
(it can actually be used to construct the Laplacian pyramid). First it
upsamples the source image by injecting even zero rows and columns and then
convolves the result with the same kernel as in pyrDown() , multiplied by 4.

===========================================================================
*cv-sepFilter2D*

void sepFilter2D(const Mat& src, Mat& dst, int ddepth, const Mat& rowKernel,
    const Mat& columnKernel, Point anchor=Point(-1, -1), double delta=0, int
    borderType=BORDER_DEFAULT)~

    Applies separable linear filter to an image

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same number of channels as src
                  • ddepth – The destination image depth
                  • rowKernel – The coefficients for filtering each row
    Parameters:   • columnKernel – The coefficients for filtering each column
                  • anchor – The anchor position within the kernel; The default
                    value (-1, 1) means that the anchor is at the kernel center
                  • delta – The value added to the filtered results before
                    storing them
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()

The function applies a separable linear filter to the image. That is, first,
every row of src is filtered with 1D kernel rowKernel . Then, every column of
the result is filtered with 1D kernel columnKernel and the final result shifted
by delta is stored in dst .

See also: createSeparableLinearFilter() , filter2D() , Sobel() , GaussianBlur()
, boxFilter() , blur() .

===========================================================================
*cv-Sobel*

void Sobel(const Mat& src, Mat& dst, int ddepth, int xorder, int yorder, int
    ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT)~

    Calculates the first, second, third or mixed image derivatives using an
    extended Sobel operator

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same number of channels as src
                  • ddepth – The destination image depth
                  • xorder – Order of the derivative x
                  • yorder – Order of the derivative y
                  • ksize – Size of the extended Sobel kernel, must be 1, 3, 5
    Parameters:     or 7
                  • scale – The optional scale factor for the computed
                    derivative values (by default, no scaling is applied, see
                    getDerivKernels() )
                  • delta – The optional delta value, added to the results
                    prior to storing them in dst
                  • borderType – The pixel extrapolation method, see
                    borderInterpolate()

In all cases except 1, an ksize \timesksize separable kernel
will be used to calculate the derivative. When \texttt{ksize = 1} , a 3 \times
1 or 1 \times 3 kernel will be used (i.e. no Gaussian smoothing is done). ksize
= 1 can only be used for the first or the second x- or y- derivatives.

There is also the special value ksize = CV_SCHARR (-1) that corresponds to a 3\
times3 Scharr filter that may give more accurate results than a 3\times3 Sobel.
The Scharr aperture is

\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}

for the x-derivative or transposed for the y-derivative.

The function calculates the image derivative by convolving the image with the
appropriate kernel:

dst = \frac{\partial^{xorder+yorder} src}{\partial x^{xorder}
\partial y^{yorder}}

The Sobel operators combine Gaussian smoothing and differentiation, so the
result is more or less resistant to the noise. Most often, the function is
called with ( xorder = 1, yorder = 0, ksize = 3) or ( xorder = 0, yorder = 1,
ksize = 3) to calculate the first x- or y- image derivative. The first case
corresponds to a kernel of:

\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}

and the second one corresponds to a kernel of:

\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}

See also: Scharr() , Lapacian() , sepFilter2D() , filter2D() , GaussianBlur()

===========================================================================
*cv-Scharr*

void Scharr(const Mat& src, Mat& dst, int ddepth, int xorder, int yorder,
    double scale=1, double delta=0, int borderType=BORDER_DEFAULT)~

    Calculates the first x- or y- image derivative using Scharr operator

                  • src – The source image
                  • dst – The destination image; will have the same size and
                    the same number of channels as src
                  • ddepth – The destination image depth
                  • xorder – Order of the derivative x
                  • yorder – Order of the derivative y
    Parameters:   • scale – The optional scale factor for the computed
                    derivative values (by default, no scaling is applied, see
                    getDerivKernels() )
                  • delta – The optional delta value, added to the results
                    prior to storing them in dst
                  • borderType – The pixel extrapolation method, see
                    borderInterpolate()

The function computes the first x- or y- spatial image derivative using Scharr
operator. The call

Scharr(src, dst, ddepth, xorder, yorder, scale, delta, borderType)

is equivalent to

\texttt{Sobel(src, dst, ddepth, xorder, yorder, CV\_SCHARR, scale, delta,
borderType)} .

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Image Filtering
      □ BaseColumnFilter
      □ BaseFilter
      □ BaseRowFilter
      □ FilterEngine
      □ cv::bilateralFilter
      □ cv::blur
      □ cv::borderInterpolate
      □ cv::boxFilter
      □ cv::buildPyramid
      □ cv::copyMakeBorder
      □ cv::createBoxFilter
      □ cv::createDerivFilter
      □ cv::createGaussianFilter
      □ cv::createLinearFilter
      □ cv::createMorphologyFilter
      □ cv::createSeparableLinearFilter
      □ cv::dilate
      □ cv::erode
      □ cv::filter2D
      □ cv::GaussianBlur
      □ cv::getDerivKernels
      □ cv::getGaussianKernel
      □ cv::getKernelType
      □ cv::getStructuringElement
      □ cv::medianBlur
      □ cv::morphologyEx
      □ cv::Laplacian
      □ cv::pyrDown
      □ cv::pyrUp
      □ cv::sepFilter2D
      □ cv::Sobel
      □ cv::Scharr

Previous topic

Histograms

Next topic

Geometric Image Transformations

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Geometric Image Transformations~

The functions in this section perform various geometrical transformations of 2D
images. That is, they do not change the image content, but deform the pixel
grid, and map this deformed grid to the destination image. In fact, to avoid
sampling artifacts, the mapping is done in the reverse order, from destination
to the source. That is, for each pixel (x, y) of the destination image, the
functions compute coordinates of the corresponding “donor” pixel in the source
image and copy the pixel value, that is:

dst (x,y)= src (f_x(x,y), f_y(x,y))

In the case when the user specifies the forward mapping: \left<g_x, g_y\right>:
src \rightarrow dst , the OpenCV functions first compute the
corresponding inverse mapping: \left<f_x, f_y\right>: dst \rightarrow
src and then use the above formula.

The actual implementations of the geometrical transformations, from the most
generic Remap and to the simplest and the fastest Resize , need to solve the 2
main problems with the above formula:

 1. extrapolation of non-existing pixels. Similarly to the filtering functions,
    described in the previous section, for some (x,y) one of f_x(x,y) or f_y
    (x,y) , or they both, may fall outside of the image, in which case some
    extrapolation method needs to be used. OpenCV provides the same selection
    of the extrapolation methods as in the filtering functions, but also an
    additional method BORDER_TRANSPARENT , which means that the corresponding
    pixels in the destination image will not be modified at all.
 2. interpolation of pixel values. Usually f_x(x,y) and f_y(x,y) are
    floating-point numbers (i.e. \left<f_x, f_y\right> can be an affine or
    perspective transformation, or radial lens distortion correction etc.), so
    a pixel values at fractional coordinates needs to be retrieved. In the
    simplest case the coordinates can be just rounded to the nearest integer
    coordinates and the corresponding pixel used, which is called
    nearest-neighbor interpolation. However, a better result can be achieved by
    using more sophisticated interpolation methods , where a polynomial
    function is fit into some neighborhood of the computed pixel (f_x(x,y), f_y
    (x,y)) and then the value of the polynomial at (f_x(x,y), f_y(x,y)) is
    taken as the interpolated pixel value. In OpenCV you can choose between
    several interpolation methods, see Resize .

===========================================================================
*cv-convertMaps*

void convertMaps(const Mat& map1, const Mat& map2, Mat& dstmap1, Mat& dstmap2,
    int dstmap1type, bool nninterpolation=false)~

    Converts image transformation maps from one representation to another

                  • map1 – The first input map of type CV_16SC2 or CV_32FC1 or
                    CV_32FC2
                  • map2 – The second input map of type CV_16UC1 or CV_32FC1 or
                    none (empty matrix), respectively
                  • dstmap1 – The first output map; will have type dstmap1type
    Parameters:     and the same size as src
                  • dstmap2 – The second output map
                  • dstmap1type – The type of the first output map; should be
                    CV_16SC2 , CV_32FC1 or CV_32FC2
                  • nninterpolation – Indicates whether the fixed-point maps
                    will be used for nearest-neighbor or for more complex
                    interpolation

The function converts a pair of maps for remap() from one representation to
another. The following options ( (map1.type(), map2.type()) \rightarrow
(dstmap1.type(), dstmap2.type()) ) are supported:

 1. \texttt{(CV\_32FC1, CV\_32FC1)} \rightarrow \texttt{(CV\_16SC2, CV\_16UC1)}
    . This is the most frequently used conversion operation, in which the
    original floating-point maps (see remap() ) are converted to more compact
    and much faster fixed-point representation. The first output array will
    contain the rounded coordinates and the second array (created only when
    nninterpolation=false ) will contain indices in the interpolation tables.
 2. \texttt{(CV\_32FC2)} \rightarrow \texttt{(CV\_16SC2, CV\_16UC1)} . The same
    as above, but the original maps are stored in one 2-channel matrix.
 3. the reverse conversion. Obviously, the reconstructed floating-point maps
    will not be exactly the same as the originals.

See also: remap() , undisort() , initUndistortRectifyMap()

===========================================================================
*cv-getAffineTransform*

Mat getAffineTransform(const Point2f src[], const Point2f dst[])~

    Calculates the affine transform from 3 pairs of the corresponding points

                  • src – Coordinates of a triangle vertices in the source
    Parameters:     image
                  • dst – Coordinates of the corresponding triangle vertices in
                    the destination image

The function calculates the 2 \times 3 matrix of an affine transform such that:

\begin{bmatrix} x'_i \\ y'_i \end{bmatrix} = \texttt{map\_matrix} * \begin
{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}

where

dst(i)=(x'_i,y'_i),src(i)=(x_i, y_i),i=0,1,2

See also: warpAffine() , transform()

===========================================================================
*cv-getPerspectiveTransform*

Mat getPerspectiveTransform(const Point2f src[], const Point2f dst[])~

    Calculates the perspective transform from 4 pairs of the corresponding
    points

                  • src – Coordinates of a quadrange vertices in the source
    Parameters:     image
                  • dst – Coordinates of the corresponding quadrangle vertices
                    in the destination image

The function calculates the 3 \times 3 matrix of a perspective transform such
that:

\begin{bmatrix} t_i x'_i \\ t_i y'_i \\ t_i \end{bmatrix} = \texttt{map\
_matrix} * \begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}

where

dst(i)=(x'_i,y'_i),src(i)=(x_i, y_i),i=0,1,2

See also: findHomography() , warpPerspective() , perspectiveTransform()

===========================================================================
*cv-getRectSubPix*

void getRectSubPix(const Mat& image, Size patchSize, Point2f center, Mat& dst,
    int patchType=-1)~

    Retrieves the pixel rectangle from an image with sub-pixel accuracy

                  • src – Source image
                  • patchSize – Size of the extracted patch
                  • center – Floating point coordinates of the extracted
                    rectangle center within the source image. The center must
    Parameters:     be inside the image
                  • dst – The extracted patch; will have the size patchSize and
                    the same number of channels as src
                  • patchType – The depth of the extracted pixels. By default
                    they will have the same depth as src

The function getRectSubPix extracts pixels from src :

dst(x, y) = src(x + center.x - ( dst.cols -1)*0.5, y + \
texttt{center.y} - ( dst.rows -1)*0.5)

where the values of the pixels at non-integer coordinates are retrieved using
bilinear interpolation. Every channel of multiple-channel images is processed
independently. While the rectangle center must be inside the image, parts of
the rectangle may be outside. In this case, the replication border mode (see
borderInterpolate() ) is used to extrapolate the pixel values outside of the
image.

See also: warpAffine() , warpPerspective()

===========================================================================
*cv-getRotationMatrix2D*

Mat getRotationMatrix2D(Point2f center, double angle, double scale)~

    Calculates the affine matrix of 2d rotation.

                  • center – Center of the rotation in the source image
                  • angle – The rotation angle in degrees. Positive values mean
    Parameters:     counter-clockwise rotation (the coordinate origin is
                    assumed to be the top-left corner)
                  • scale – Isotropic scale factor

The function calculates the following matrix:

\begin{bmatrix} \alpha & \beta & (1- \alpha ) * center.x - \beta \
cdot center.y \\ - \beta & \alpha & \beta * center.x -
(1- \alpha ) * center.y \end{bmatrix}

where

\begin{array}{l} \alpha = scale * \cos angle , \\ \beta =
scale * \sin angle \end{array}

The transformation maps the rotation center to itself. If this is not the
purpose, the shift should be adjusted.

See also: getAffineTransform() , warpAffine() , transform()

===========================================================================
*cv-invertAffineTransform*

void invertAffineTransform(const Mat& M, Mat& iM)~

    Inverts an affine transformation

    Parameters:   • M – The original affine transformation
                  • iM – The output reverse affine transformation

The function computes inverse affine transformation represented by 2 \times 3
matrix M :

\begin{bmatrix} a_{11} & a_{12} & b_1 \\ a_{21} & a_{22} & b_2 \end{bmatrix}

The result will also be a 2 \times 3 matrix of the same type as M .

===========================================================================
*cv-remap*

void remap(const Mat& src, Mat& dst, const Mat& map1, const Mat& map2, int
    interpolation, int borderMode=BORDER_CONSTANT, const Scalar& borderValue=
    Scalar())~

    Applies a generic geometrical transformation to an image.

                  • src – Source image
                  • dst – Destination image. It will have the same size as map1
                    and the same type as src
                  • map1 – The first map of either (x,y) points or just x
                    values having type CV_16SC2 , CV_32FC1 or CV_32FC2 . See
                    convertMaps() for converting floating point representation
                    to fixed-point for speed.
                  • map2 – The second map of y values having type CV_16UC1 ,
                    CV_32FC1 or none (empty map if map1 is (x,y) points),
    Parameters:     respectively
                  • interpolation – The interpolation method, see resize() .
                    The method INTER_AREA is not supported by this function
                  • borderMode – The pixel extrapolation method, see
                    borderInterpolate() . When the borderMode=
                    BORDER_TRANSPARENT , it means that the pixels in the
                    destination image that corresponds to the “outliers” in the
                    source image are not modified by the function
                  • borderValue – A value used in the case of a constant
                    border. By default it is 0

The function remap transforms the source image using the specified map:

dst (x,y) = src (map_x(x,y),map_y(x,y))

Where values of pixels with non-integer coordinates are computed using one of
the available interpolation methods. map_x and map_y can be encoded as separate
floating-point maps in map_1 and map_2 respectively, or interleaved
floating-point maps of (x,y) in map_1 , or fixed-point maps made by using
convertMaps() . The reason you might want to convert from floating to
fixed-point representations of a map is that they can yield much faster (~2x)
remapping operations. In the converted case, map_1 contains pairs (cvFloor(x),
cvFloor(y)) and map_2 contains indices in a table of interpolation
coefficients.

This function can not operate in-place.

===========================================================================
*cv-resize*

void resize(const Mat& src, Mat& dst, Size dsize, double fx=0, double fy=0, int
    interpolation=INTER_LINEAR)~

    Resizes an image

                  • src – Source image
                  • dst – Destination image. It will have size dsize (when it
                    is non-zero) or the size computed from src.size() and fx
                    and fy . The type of dst will be the same as of src .
                  • dsize –

                    The destination image size. If it is zero, then it is
                    computed as:

                    \texttt{dsize = Size(round(fx*src.cols), round
                    (fy*src.rows))}

                    . Either dsize or both fx or fy must be non-zero.

                  • fx –

                    The scale factor along the horizontal axis. When 0, it is
                    computed as

                    \texttt{(double)dsize.width/src.cols}
    Parameters:
                  • fy –

                    The scale factor along the vertical axis. When 0, it is
                    computed as

                    \texttt{(double)dsize.height/src.rows}

                  • interpolation –

                    The interpolation method:

                      □ INTER_NEAREST nearest-neighbor interpolation
                      □ INTER_LINEAR bilinear interpolation (used by default)
                      □ INTER_AREA resampling using pixel area relation. It may
                        be the preferred method for image decimation, as it
                        gives moire-free results. But when the image is zoomed,
                        it is similar to the INTER_NEAREST method
                      □ INTER_CUBIC bicubic interpolation over 4x4 pixel
                        neighborhood
                      □ INTER_LANCZOS4 Lanczos interpolation over 8x8 pixel
                        neighborhood

The function resize resizes an image src down to or up to the specified size.
Note that the initial dst type or size are not taken into account. Instead the
size and type are derived from the src , dsize , fx and fy . If you want to
resize src so that it fits the pre-created dst , you may call the function as:

// explicitly specify dsize=dst.size(); fx and fy will be computed from that.
resize(src, dst, dst.size(), 0, 0, interpolation);

If you want to decimate the image by factor of 2 in each direction, you can
call the function this way:

// specify fx and fy and let the function to compute the destination image size.
resize(src, dst, Size(), 0.5, 0.5, interpolation);

See also: warpAffine() , warpPerspective() , remap() .

===========================================================================
*cv-warpAffine*

void warpAffine(const Mat& src, Mat& dst, const Mat& M, Size dsize, int flags=
    INTER_LINEAR, int borderMode=BORDER_CONSTANT, const Scalar& borderValue=
    Scalar())~

    Applies an affine transformation to an image.

                  • src – Source image
                  • dst – Destination image; will have size dsize and the same
                    type as src
                  • M – 2\times 3 transformation matrix
                  • dsize – Size of the destination image
                  • flags – A combination of interpolation methods, see resize
                    () , and the optional flag WARP_INVERSE_MAP that means that
    Parameters:     M is the inverse transformation ( dst\rightarrow\
                    texttt{src} )
                  • borderMode – The pixel extrapolation method, see
                    borderInterpolate() . When the borderMode=
                    BORDER_TRANSPARENT , it means that the pixels in the
                    destination image that corresponds to the “outliers” in the
                    source image are not modified by the function
                  • borderValue – A value used in case of a constant border. By
                    default it is 0

The function warpAffine transforms the source image using the specified matrix:

dst (x,y) = src ( M _{11} x + M _{12} y + \
texttt{M} _{13}, M _{21} x + M _{22} y + M _{23})

when the flag WARP_INVERSE_MAP is set. Otherwise, the transformation is first
inverted with invertAffineTransform() and then put in the formula above instead
of M . The function can not operate in-place.

See also: warpPerspective() , resize() , remap() , getRectSubPix() , transform
()

===========================================================================
*cv-warpPerspective*

void warpPerspective(const Mat& src, Mat& dst, const Mat& M, Size dsize, int
    flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, const Scalar&
    borderValue=Scalar())~

    Applies a perspective transformation to an image.

                  • src – Source image
                  • dst – Destination image; will have size dsize and the same
                    type as src
                  • M – 3\times 3 transformation matrix
                  • dsize – Size of the destination image
                  • flags – A combination of interpolation methods, see resize
                    () , and the optional flag WARP_INVERSE_MAP that means that
    Parameters:     M is the inverse transformation ( dst\rightarrow\
                    texttt{src} )
                  • borderMode – The pixel extrapolation method, see
                    borderInterpolate() . When the borderMode=
                    BORDER_TRANSPARENT , it means that the pixels in the
                    destination image that corresponds to the “outliers” in the
                    source image are not modified by the function
                  • borderValue – A value used in case of a constant border. By
                    default it is 0

The function warpPerspective transforms the source image using the specified
matrix:

dst (x,y) = src \left ( \frac{M_{11} x + M_{12} y + M_{13}}
{M_{31} x + M_{32} y + M_{33}} , \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x +
M_{32} y + M_{33}} \right )

when the flag WARP_INVERSE_MAP is set. Otherwise, the transformation is first
inverted with invert() and then put in the formula above instead of M . The
function can not operate in-place.

See also: warpAffine() , resize() , remap() , getRectSubPix() ,
perspectiveTransform()

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Geometric Image Transformations
      □ cv::convertMaps
      □ cv::getAffineTransform
      □ cv::getPerspectiveTransform
      □ cv::getRectSubPix
      □ cv::getRotationMatrix2D
      □ cv::invertAffineTransform
      □ cv::remap
      □ cv::resize
      □ cv::warpAffine
      □ cv::warpPerspective

Previous topic

Image Filtering

Next topic

Miscellaneous Image Transformations

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Miscellaneous Image Transformations~

===========================================================================
*cv-adaptiveThreshold*

void adaptiveThreshold(const Mat& src, Mat& dst, double maxValue, int
    adaptiveMethod, int thresholdType, int blockSize, double C)~

    Applies an adaptive threshold to an array.

                  • src – Source 8-bit single-channel image
                  • dst – Destination image; will have the same size and the
                    same type as src
                  • maxValue – The non-zero value assigned to the pixels for
                    which the condition is satisfied. See the discussion
                  • adaptiveMethod – Adaptive thresholding algorithm to use,
                    ADAPTIVE_THRESH_MEAN_C or ADAPTIVE_THRESH_GAUSSIAN_C (see
    Parameters:     the discussion)
                  • thresholdType – Thresholding type; must be one of
                    THRESH_BINARY or THRESH_BINARY_INV
                  • blockSize – The size of a pixel neighborhood that is used
                    to calculate a threshold value for the pixel: 3, 5, 7, and
                    so on
                  • C – The constant subtracted from the mean or weighted mean
                    (see the discussion); normally, it’s positive, but may be
                    zero or negative as well

The function transforms a grayscale image to a binary image according to the
formulas:

      □ THRESH_BINARY

            dst(x,y) = \fork{maxValue}{if $src(x,y) > T(x,y)$}{0}
            {otherwise}

      □ THRESH_BINARY_INV

            dst(x,y) = \fork{0}{if $src(x,y) > T(x,y)$}{maxValue}
            {otherwise}

where T(x,y) is a threshold calculated individually for each pixel.

 1. For the method ADAPTIVE_THRESH_MEAN_C the threshold value T(x,y) is the
    mean of a blockSize \times blockSize neighborhood of (x,
    y) , minus C .
 2. For the method ADAPTIVE_THRESH_GAUSSIAN_C the threshold value T(x, y) is
    the weighted sum (i.e. cross-correlation with a Gaussian window) of a \
    texttt{blockSize} \times blockSize neighborhood of (x, y) , minus
    C . The default sigma (standard deviation) is used for the specified
    blockSize , see getGaussianKernel() .

The function can process the image in-place.

See also: threshold() , blur() , GaussianBlur()

===========================================================================
*cv-cvtColor*

void cvtColor(const Mat& src, Mat& dst, int code, int dstCn=0)~

    Converts image from one color space to another

                  • src – The source image, 8-bit unsigned, 16-bit unsigned (
                    CV_16UC... ) or single-precision floating-point
                  • dst – The destination image; will have the same size and
    Parameters:     the same depth as src
                  • code – The color space conversion code; see the discussion
                  • dstCn – The number of channels in the destination image; if
                    the parameter is 0, the number of the channels will be
                    derived automatically from src and the code

The function converts the input image from one color space to another. In the
case of transformation to-from RGB color space the ordering of the channels
should be specified explicitly (RGB or BGR).

The conventional ranges for R, G and B channel values are:

  • 0 to 255 for CV_8U images
  • 0 to 65535 for CV_16U images and
  • 0 to 1 for CV_32F images.

Of course, in the case of linear transformations the range does not matter, but
in the non-linear cases the input RGB image should be normalized to the proper
value range in order to get the correct results, e.g. for RGB \rightarrow
L*u*v* transformation. For example, if you have a 32-bit floating-point image
directly converted from 8-bit image without any scaling, then it will have
0..255 value range, instead of the assumed by the function 0..1. So, before
calling cvtColor , you need first to scale the image down:

img *= 1./255;
cvtColor(img, img, CV_BGR2Luv);

The function can do the following transformations:

  • Transformations within RGB space like adding/removing the alpha channel,
    reversing the channel order, conversion to/from 16-bit RGB color (R5:G6:B5
    or R5:G5:B5), as well as conversion to/from grayscale using:

    \text{RGB[A] to Gray:} \quad Y \leftarrow 0.299 * R + 0.587 * G +
    0.114 * B

    and

    \text{Gray to RGB[A]:} \quad R \leftarrow Y, G \leftarrow Y, B \leftarrow
    Y, A \leftarrow 0

    The conversion from a RGB image to gray is done with:

    cvtColor(src, bwsrc, CV_RGB2GRAY);

    Some more advanced channel reordering can also be done with mixChannels() .

  • RGB \leftrightarrow CIE XYZ.Rec 709 with D65 white point ( CV_BGR2XYZ,
    CV_RGB2XYZ, CV_XYZ2BGR, CV_XYZ2RGB ):

    \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \leftarrow \begin{bmatrix}
    0.412453 & 0.357580 & 0.180423 \\ 0.212671 & 0.715160 & 0.072169 \\
    0.019334 & 0.119193 & 0.950227 \end{bmatrix} * \begin{bmatrix} R \\ G \
    \ B \end{bmatrix}

    \begin{bmatrix} R \\ G \\ B \end{bmatrix} \leftarrow \begin{bmatrix}
    3.240479 & -1.53715 & -0.498535 \\ -0.969256 & 1.875991 & 0.041556 \\
    0.055648 & -0.204043 & 1.057311 \end{bmatrix} * \begin{bmatrix} X \\ Y
    \\ Z \end{bmatrix}

    X , Y and Z cover the whole value range (in the case of floating-point
    images Z may exceed 1).

  • RGB \leftrightarrow YCrCb JPEG (a.k.a. YCC) ( CV_BGR2YCrCb, CV_RGB2YCrCb,
    CV_YCrCb2BGR, CV_YCrCb2RGB )

    Y \leftarrow 0.299 * R + 0.587 * G + 0.114 * B

    Cr \leftarrow (R-Y) * 0.713 + delta

    Cb \leftarrow (B-Y) * 0.564 + delta

    R \leftarrow Y + 1.403 * (Cr - delta)

    G \leftarrow Y - 0.344 * (Cr - delta) - 0.714 * (Cb - delta)

    B \leftarrow Y + 1.773 * (Cb - delta)

    where

    delta = \left \{ \begin{array}{l l} 128 & \mbox{for 8-bit images} \\ 32768
    & \mbox{for 16-bit images} \\ 0.5 & \mbox{for floating-point images} \end
    {array} \right .

    Y, Cr and Cb cover the whole value range.

  • RGB \leftrightarrow HSV ( CV_BGR2HSV, CV_RGB2HSV, CV_HSV2BGR, CV_HSV2RGB )

        in the case of 8-bit and 16-bit images R, G and B are converted to
        floating-point format and scaled to fit the 0 to 1 range

    V \leftarrow max(R,G,B)

    S \leftarrow \fork{\frac{V-min(R,G,B)}{V}}{if $V \neq 0$}{0}{otherwise}

    H \leftarrow \forkthree{{60(G - B)}/{S}}{if $V=R$}{{120+60(B - R)}/{S}}{if
    $V=G$}{{240+60(R - G)}/{S}}{if $V=B$}

    if H<0 then H \leftarrow H+360 On output 0 \leq V \leq 1 , 0 \leq S \leq 1
    , 0 \leq H \leq 360 .

    The values are then converted to the destination data type:

      □ 8-bit images

            V \leftarrow 255 V, S \leftarrow 255 S, H \leftarrow H/2 \text{(to
            fit to 0 to 255)}

      □ 16-bit images (currently not supported)

            V <- 65535 V, S <- 65535 S, H <- H

      □ 
        32-bit images

            H, S, V are left as is

  • RGB \leftrightarrow HLS ( CV_BGR2HLS, CV_RGB2HLS, CV_HLS2BGR, CV_HLS2RGB ).

        in the case of 8-bit and 16-bit images R, G and B are converted to
        floating-point format and scaled to fit the 0 to 1 range.

    V_{max} \leftarrow {max}(R,G,B)

    V_{min} \leftarrow {min}(R,G,B)

    L \leftarrow \frac{V_{max} + V_{min}}{2}

    S \leftarrow \fork { \frac{V_{max} - V_{min}}{V_{max} + V_{min}} }{if $L <
    0.5$ } { \frac{V_{max} - V_{min}}{2 - (V_{max} + V_{min})} }{if $L \ge 0.5$
    }

    H \leftarrow \forkthree {{60(G - B)}/{S}}{if $V_{max}=R$ } {{120+60(B - R)}
    /{S}}{if $V_{max}=G$ } {{240+60(R - G)}/{S}}{if $V_{max}=B$ }

    if H<0 then H \leftarrow H+360 On output 0 \leq L \leq 1 , 0 \leq S \leq 1
    , 0 \leq H \leq 360 .

    The values are then converted to the destination data type:

      □ 8-bit images

            V \leftarrow 255 * V, S \leftarrow 255 * S, H \leftarrow H/
            2 \; \text{(to fit to 0 to 255)}

      □ 16-bit images (currently not supported)

            V <- 65535 * V, S <- 65535 * S, H <- H

      □ 
        32-bit images

            H, S, V are left as is

  • RGB \leftrightarrow CIE L*a*b* ( CV_BGR2Lab, CV_RGB2Lab, CV_Lab2BGR,
    CV_Lab2RGB )

        in the case of 8-bit and 16-bit images R, G and B are converted to
        floating-point format and scaled to fit the 0 to 1 range

    \vecthree{X}{Y}{Z} \leftarrow \vecthreethree{0.412453}{0.357580}{0.180423}
    {0.212671}{0.715160}{0.072169}{0.019334}{0.119193}{0.950227} * \
    vecthree{R}{G}{B}

    X \leftarrow X/X_n, \text{where} X_n = 0.950456

    Z \leftarrow Z/Z_n, \text{where} Z_n = 1.088754

    L \leftarrow \fork{116*Y^{1/3}-16}{for $Y>0.008856$}{903.3*Y}{for $Y \le
    0.008856$}

    a \leftarrow 500 (f(X)-f(Y)) + delta

    b \leftarrow 200 (f(Y)-f(Z)) + delta

    where

    f(t)= \fork{t^{1/3}}{for $t>0.008856$}{7.787 t+16/116}{for $t\leq
    0.008856$}

    and

    delta = \fork{128}{for 8-bit images}{0}{for floating-point images}

    On output 0 \leq L \leq 100 , -127 \leq a \leq 127 , -127 \leq b \leq 127
    The values are then converted to the destination data type:

      □ 8-bit images

            L \leftarrow L*255/100, \; a \leftarrow a + 128, \; b \leftarrow b
            + 128

      □ 
        16-bit images

            currently not supported

      □ 
        32-bit images

            L, a, b are left as is

  • RGB \leftrightarrow CIE L*u*v* ( CV_BGR2Luv, CV_RGB2Luv, CV_Luv2BGR,
    CV_Luv2RGB )

        in the case of 8-bit and 16-bit images R, G and B are converted to
        floating-point format and scaled to fit 0 to 1 range

    \vecthree{X}{Y}{Z} \leftarrow \vecthreethree{0.412453}{0.357580}{0.180423}
    {0.212671}{0.715160}{0.072169}{0.019334}{0.119193}{0.950227} * \
    vecthree{R}{G}{B}

    L \leftarrow \fork{116 Y^{1/3}}{for $Y>0.008856$}{903.3 Y}{for $Y\leq
    0.008856$}

    u' \leftarrow 4*X/(X + 15*Y + 3 Z)

    v' \leftarrow 9*Y/(X + 15*Y + 3 Z)

    u \leftarrow 13*L*(u' - u_n) \quad \text{where} \quad u_n=0.19793943

    v \leftarrow 13*L*(v' - v_n) \quad \text{where} \quad v_n=0.46831096

    On output 0 \leq L \leq 100 , -134 \leq u \leq 220 , -140 \leq v \leq 122 .

    The values are then converted to the destination data type:

      □ 8-bit images

            L \leftarrow 255/100 L, \; u \leftarrow 255/354 (u + 134), \; v \
            leftarrow 255/256 (v + 140)

      □ 
        16-bit images

            currently not supported

      □ 
        32-bit images

            L, u, v are left as is

    The above formulas for converting RGB to/from various color spaces have
    been taken from multiple sources on Web, primarily from the Charles Poynton
    site http://www.poynton.com/ColorFAQ.html

  • Bayer \rightarrow RGB ( CV_BayerBG2BGR, CV_BayerGB2BGR, CV_BayerRG2BGR,
    CV_BayerGR2BGR, CV_BayerBG2RGB, CV_BayerGB2RGB, CV_BayerRG2RGB,
    CV_BayerGR2RGB ) The Bayer pattern is widely used in CCD and CMOS cameras.
    It allows one to get color pictures from a single plane where R,G and B
    pixels (sensors of a particular component) are interleaved like this:

    \newcommand{\Rcell}{\color{red}R} \newcommand{\Gcell}{\color{green}G} \
    newcommand{\Bcell}{\color{blue}B} \definecolor{BackGray}{rgb}{0.8,0.8,0.8}
    \begin{array}{ c c c c c } \Rcell & \Gcell & \Rcell & \Gcell & \Rcell \\ \
    Gcell & \colorbox{BackGray}{\Bcell} & \colorbox{BackGray}{\Gcell} & \Bcell
    & \Gcell \\ \Rcell & \Gcell & \Rcell & \Gcell & \Rcell \\ \Gcell & \Bcell &
    \Gcell & \Bcell & \Gcell \\ \Rcell & \Gcell & \Rcell & \Gcell & \Rcell \end
    {array}

    The output RGB components of a pixel are interpolated from 1, 2 or 4
    neighbors of the pixel having the same color. There are several
    modifications of the above pattern that can be achieved by shifting the
    pattern one pixel left and/or one pixel up. The two letters C_1 and C_2 in
    the conversion constants CV_Bayer C_1 C_2 2BGR and CV_Bayer C_1 C_2 2RGB
    indicate the particular pattern type - these are components from the second
    row, second and third columns, respectively. For example, the above pattern
    has very popular “BG” type.

===========================================================================
*cv-distanceTransform*

void distanceTransform(const Mat& src, Mat& dst, int distanceType, int maskSize
    )~

void distanceTransform(const Mat& src, Mat& dst, Mat& labels, int distanceType,
    int maskSize)

    Calculates the distance to the closest zero pixel for each pixel of the
    source image.

                  • src – 8-bit, single-channel (binary) source image
                  • dst – Output image with calculated distances; will be
                    32-bit floating-point, single-channel image of the same
                    size as src
                  • distanceType – Type of distance; can be CV_DIST_L1,
                    CV_DIST_L2 or CV_DIST_C
                  • maskSize – Size of the distance transform mask; can be 3, 5
    Parameters:     or CV_DIST_MASK_PRECISE (the latter option is only
                    supported by the first of the functions). In the case of
                    CV_DIST_L1 or CV_DIST_C distance type the parameter is
                    forced to 3, because a 3\times 3 mask gives the same result
                    as a 5\times 5 or any larger aperture.
                  • labels – The optional output 2d array of labels - the
                    discrete Voronoi diagram; will have type CV_32SC1 and the
                    same size as src . See the discussion

The functions distanceTransform calculate the approximate or precise distance
from every binary image pixel to the nearest zero pixel. (for zero image pixels
the distance will obviously be zero).

When maskSize == CV_DIST_MASK_PRECISE and distanceType == CV_DIST_L2 , the
function runs the algorithm described in Felzenszwalb04 .

In other cases the algorithm Borgefors86 is used, that is, for pixel the
function finds the shortest path to the nearest zero pixel consisting of basic
shifts: horizontal, vertical, diagonal or knight’s move (the latest is
available for a 5\times 5 mask). The overall distance is calculated as a sum of
these basic distances. Because the distance function should be symmetric, all
of the horizontal and vertical shifts must have the same cost (that is denoted
as a ), all the diagonal shifts must have the same cost (denoted b ), and all
knight’s moves must have the same cost (denoted c ). For CV_DIST_C and
CV_DIST_L1 types the distance is calculated precisely, whereas for CV_DIST_L2
(Euclidian distance) the distance can be calculated only with some relative
error (a 5\times 5 mask gives more accurate results). For a , b and c OpenCV
uses the values suggested in the original paper:

┌──────────┬───────────┬────────────────────┐
│CV_DIST_C │(3\times 3)│    a = 1, b = 1    │
├──────────┼───────────┼────────────────────┤
│CV_DIST_L1│(3\times 3)│a = 1, b = 2        │
├──────────┼───────────┼────────────────────┤
│CV_DIST_L2│(3\times 3)│a=0.955, b=1.3693   │
├──────────┼───────────┼────────────────────┤
│CV_DIST_L2│(5\times 5)│a=1, b=1.4, c=2.1969│
└──────────┴───────────┴────────────────────┘

Typically, for a fast, coarse distance estimation CV_DIST_L2 , a 3\times 3 mask
is used, and for a more accurate distance estimation CV_DIST_L2 , a 5\times 5
mask or the precise algorithm is used. Note that both the precise and the
approximate algorithms are linear on the number of pixels.

The second variant of the function does not only compute the minimum distance
for each pixel (x, y) , but it also identifies the nearest the nearest
connected component consisting of zero pixels. Index of the component is stored
in labels(x, y) . The connected components of zero pixels are also
found and marked by the function.

In this mode the complexity is still linear. That is, the function provides a
very fast way to compute Voronoi diagram for the binary image. Currently, this
second variant can only use the approximate distance transform algorithm.

===========================================================================
*cv-floodFill*

int floodFill(Mat& image, Point seed, Scalar newVal, Rect* rect=0, Scalar
    loDiff=Scalar(), Scalar upDiff=Scalar(), int flags=4)~

int floodFill(Mat& image, Mat& mask, Point seed, Scalar newVal, Rect* rect=0,
    Scalar loDiff=Scalar(), Scalar upDiff=Scalar(), int flags=4)

    Fills a connected component with the given color.

                  • image – Input/output 1- or 3-channel, 8-bit or
                    floating-point image. It is modified by the function unless
                    the FLOODFILL_MASK_ONLY flag is set (in the second variant
                    of the function; see below)
                  • mask – (For the second function only) Operation mask,
                    should be a single-channel 8-bit image, 2 pixels wider and
                    2 pixels taller. The function uses and updates the mask, so
                    the user takes responsibility of initializing the mask
                    content. Flood-filling can’t go across non-zero pixels in
                    the mask, for example, an edge detector output can be used
                    as a mask to stop filling at edges. It is possible to use
                    the same mask in multiple calls to the function to make
                    sure the filled area do not overlap. Note : because the
                    mask is larger than the filled image, a pixel (x, y) in
                    image will correspond to the pixel (x+1, y+1) in the mask
                  • seed – The starting point
                  • newVal – New value of the repainted domain pixels
                  • loDiff – Maximal lower brightness/color difference between
                    the currently observed pixel and one of its neighbors
                    belonging to the component, or a seed pixel being added to
    Parameters:     the component
                  • upDiff – Maximal upper brightness/color difference between
                    the currently observed pixel and one of its neighbors
                    belonging to the component, or a seed pixel being added to
                    the component
                  • rect – The optional output parameter that the function sets
                    to the minimum bounding rectangle of the repainted domain
                  • flags –

                    The operation flags. Lower bits contain connectivity value,
                    4 (by default) or 8, used within the function. Connectivity
                    determines which neighbors of a pixel are considered. Upper
                    bits can be 0 or a combination of the following flags:

                      □ FLOODFILL_FIXED_RANGE if set, the difference between
                        the current pixel and seed pixel is considered,
                        otherwise the difference between neighbor pixels is
                        considered (i.e. the range is floating)
                      □ FLOODFILL_MASK_ONLY (for the second variant only) if
                        set, the function does not change the image ( newVal is
                        ignored), but fills the mask

The functions floodFill fill a connected component starting from the seed point
with the specified color. The connectivity is determined by the color/
brightness closeness of the neighbor pixels. The pixel at (x,y) is considered
to belong to the repainted domain if:

  • grayscale image, floating range

        src (x',y')- loDiff \leq src (x,y) \leq \
        texttt{src} (x',y')+ upDiff

  • grayscale image, fixed range

        src ( seed .x, seed .y)- loDiff \
        leq src (x,y) \leq src ( seed .x, \texttt
        {seed} .y)+ upDiff

  • color image, floating range

        src (x',y')_r- loDiff _r \leq src (x,y)_r \
        leq src (x',y')_r+ upDiff _r

        src (x',y')_g- loDiff _g \leq src (x,y)_g \
        leq src (x',y')_g+ upDiff _g

        src (x',y')_b- loDiff _b \leq src (x,y)_b \
        leq src (x',y')_b+ upDiff _b

  • color image, fixed range

        src ( seed .x, seed .y)_r- loDiff
        _r \leq src (x,y)_r \leq src ( seed .x, \
        texttt{seed} .y)_r+ upDiff _r

        src ( seed .x, seed .y)_g- loDiff
        _g \leq src (x,y)_g \leq src ( seed .x, \
        texttt{seed} .y)_g+ upDiff _g

        src ( seed .x, seed .y)_b- loDiff
        _b \leq src (x,y)_b \leq src ( seed .x, \
        texttt{seed} .y)_b+ upDiff _b

where src(x',y') is the value of one of pixel neighbors that is already known
to belong to the component. That is, to be added to the connected component, a
pixel’s color/brightness should be close enough to the:

  • color/brightness of one of its neighbors that are already referred to the
    connected component in the case of floating range
  • color/brightness of the seed point in the case of fixed range.

By using these functions you can either mark a connected component with the
specified color in-place, or build a mask and then extract the contour or copy
the region to another image etc. Various modes of the function are demonstrated
in floodfill.c sample.

See also: findContours()

===========================================================================
*cv-inpaint*

void inpaint(const Mat& src, const Mat& inpaintMask, Mat& dst, double
    inpaintRadius, int flags)~

    Inpaints the selected region in the image.

                  • src – The input 8-bit 1-channel or 3-channel image.
                  • inpaintMask – The inpainting mask, 8-bit 1-channel image.
                    Non-zero pixels indicate the area that needs to be
                    inpainted.
                  • dst – The output image; will have the same size and the
                    same type as src
    Parameters:   • inpaintRadius – The radius of a circlular neighborhood of
                    each point inpainted that is considered by the algorithm.
                  • flags –

                    The inpainting method, one of the following:

                      □ INPAINT_NS Navier-Stokes based method.
                      □ INPAINT_TELEA The method by Alexandru Telea Telea04

The function reconstructs the selected image area from the pixel near the area
boundary. The function may be used to remove dust and scratches from a scanned
photo, or to remove undesirable objects from still images or video. See http://
en.wikipedia.org/wiki/Inpainting for more details.

===========================================================================
*cv-integral*

void integral(const Mat& image, Mat& sum, int sdepth=-1)~

void integral(const Mat& image, Mat& sum, Mat& sqsum, int sdepth=-1)

void integral(const Mat& image, Mat& sum, Mat& sqsum, Mat& tilted, int sdepth=
    -1)

    Calculates the integral of an image.

                  • image – The source image, W \times H , 8-bit or
                    floating-point (32f or 64f)
                  • sum – The integral image, (W+1)\times (H+1) , 32-bit
                    integer or floating-point (32f or 64f)
    Parameters:   • sqsum – The integral image for squared pixel values, (W+1)\
                    times (H+1) , double precision floating-point (64f)
                  • tilted – The integral for the image rotated by 45 degrees,
                    (W+1)\times (H+1) , the same data type as sum
                  • sdepth – The desired depth of the integral and the tilted
                    integral images, CV_32S , CV_32F or CV_64F

The functions integral calculate one or more integral images for the source
image as following:

sum (X,Y) = \sum _{x<X,y<Y} image (x,y)

sqsum (X,Y) = \sum _{x<X,y<Y} image (x,y)^2

tilted (X,Y) = \sum _{y<Y,abs(x-X+1) \leq Y-y-1} image (x,y)

Using these integral images, one may calculate sum, mean and standard deviation
over a specific up-right or rotated rectangular region of the image in a
constant time, for example:

\sum _{x_1 \leq x < x_2, \, y_1 \leq y < y_2} image (x,y) = \texttt
{sum} (x_2,y_2)- sum (x_1,y_2)- sum (x_2,y_1)+ sum
(x_1,x_1)

It makes possible to do a fast blurring or fast block correlation with variable
window size, for example. In the case of multi-channel images, sums for each
channel are accumulated independently.

As a practical example, the next figure shows the calculation of the integral
of a straight rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) .
The selected pixels in the original image are shown, as well as the relative
pixels in the integral images sum and tilted .

begin{center}

_images/integral.png

end{center}

===========================================================================
*cv-threshold*

double threshold(const Mat& src, Mat& dst, double thresh, double maxVal, int
    thresholdType)~

    Applies a fixed-level threshold to each array element

                  • src – Source array (single-channel, 8-bit of 32-bit
                    floating point)
                  • dst – Destination array; will have the same size and the
    Parameters:     same type as src
                  • thresh – Threshold value
                  • maxVal – Maximum value to use with THRESH_BINARY and
                    THRESH_BINARY_INV thresholding types
                  • thresholdType – Thresholding type (see the discussion)

The function applies fixed-level thresholding to a single-channel array. The
function is typically used to get a bi-level (binary) image out of a grayscale
image ( compare() could be also used for this purpose) or for removing a noise,
i.e. filtering out pixels with too small or too large values. There are several
types of thresholding that the function supports that are determined by
thresholdType :

      □ THRESH_BINARY

            dst (x,y) = \fork{maxVal}{if $src(x,y) >
            thresh$}{0}{otherwise}

      □ THRESH_BINARY_INV

            dst (x,y) = \fork{0}{if $src(x,y) > \texttt
            {thresh}$}{maxVal}{otherwise}

      □ THRESH_TRUNC

            dst (x,y) = \fork{threshold}{if $src
            (x,y) > thresh$}{src(x,y)}{otherwise}

      □ THRESH_TOZERO

            dst (x,y) = \fork{src(x,y)}{if $src(x,y)
            > thresh$}{0}{otherwise}

      □ THRESH_TOZERO_INV

            dst (x,y) = \fork{0}{if $src(x,y) > \texttt
            {thresh}$}{src(x,y)}{otherwise}

Also, the special value THRESH_OTSU may be combined with one of the above
values. In this case the function determines the optimal threshold value using
Otsu’s algorithm and uses it instead of the specified thresh . The function
returns the computed threshold value. Currently, Otsu’s method is implemented
only for 8-bit images.

_images/threshold.png

See also: adaptiveThreshold() , findContours() , compare() , min() , max()

===========================================================================
*cv-watershed*

void watershed(const Mat& image, Mat& markers)~

    Does marker-based image segmentation using watershed algrorithm

                  • image – The input 8-bit 3-channel image.
    Parameters:   • markers – The input/output 32-bit single-channel image
                    (map) of markers. It should have the same size as image

The function implements one of the variants of watershed, non-parametric
marker-based segmentation algorithm, described in Meyer92 . Before passing the
image to the function, user has to outline roughly the desired regions in the
image markers with positive ( >0 ) indices, i.e. every region is represented as
one or more connected components with the pixel values 1, 2, 3 etc (such
markers can be retrieved from a binary mask using findContours() and
drawContours() , see watershed.cpp demo). The markers will be “seeds” of the
future image regions. All the other pixels in markers , which relation to the
outlined regions is not known and should be defined by the algorithm, should be
set to 0’s. On the output of the function, each pixel in markers is set to one
of values of the “seed” components, or to -1 at boundaries between the regions.

Note, that it is not necessary that every two neighbor connected components are
separated by a watershed boundary (-1’s pixels), for example, in case when such
tangent components exist in the initial marker image. Visual demonstration and
usage example of the function can be found in OpenCV samples directory; see
watershed.cpp demo.

See also: findContours()

===========================================================================
*cv-grabCut*

void grabCut(const Mat& image, Mat& mask, Rect rect, Mat& bgdModel, Mat&
    fgdModel, int iterCount, int mode)~

    Runs GrabCut algorithm

                  • image – The input 8-bit 3-channel image.
                  • mask –

                    The input/output 8-bit single-channel mask. Its elements
                    may have one of four values. The mask is initialize when
                    mode==GC_INIT_WITH_RECT

                      □ GC_BGD Certainly a background pixel
                      □ GC_FGD Certainly a foreground (object) pixel
                      □ GC_PR_BGD Likely a background pixel
                      □ GC_PR_BGD Likely a foreground pixel
                  • rect – The ROI containing the segmented object. The pixels
                    outside of the ROI are marked as “certainly a background”.
                    The parameter is only used when mode==GC_INIT_WITH_RECT
                  • bgdModel, fgdModel – Temporary arrays used for
                    segmentation. Do not modify them while you are processing
                    the same image
                  • iterCount – The number of iterations the algorithm should
    Parameters:     do before returning the result. Note that the result can be
                    refined with further calls with the mode==GC_INIT_WITH_MASK
                    or mode==GC_EVAL
                  • mode –

                    The operation mode

                      □ GC_INIT_WITH_RECT The function initializes the state
                        and the mask using the provided rectangle. After that
                        it runs iterCount iterations of the algorithm
                      □ GC_INIT_WITH_MASK The function initializes the state
                        using the provided mask. Note that GC_INIT_WITH_RECT
                        and GC_INIT_WITH_MASK can be combined, then all the
                        pixels outside of the ROI are automatically initialized
                        with GC_BGD

                    .

                      □ GC_EVAL The value means that algorithm should just
                        resume.

The function implements the GrabCut image segmentation algorithm. See the
sample grabcut.cpp on how to use the function.

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Miscellaneous Image Transformations
      □ cv::adaptiveThreshold
      □ cv::cvtColor
      □ cv::distanceTransform
      □ cv::floodFill
      □ cv::inpaint
      □ cv::integral
      □ cv::threshold
      □ cv::watershed
      □ cv::grabCut

Previous topic

Geometric Image Transformations

Next topic

Structural Analysis and Shape Descriptors

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Structural Analysis and Shape Descriptors~

===========================================================================
*cv-moments*

Moments moments(const Mat& array, bool binaryImage=false)¶
    Calculates all of the moments up to the third order of a polygon or
    rasterized shape.

where the class Moments is defined as:

class Moments
{
public:
    Moments();
    Moments(double m00, double m10, double m01, double m20, double m11,
            double m02, double m30, double m21, double m12, double m03 );
    Moments( const CvMoments& moments );
    operator CvMoments() const;

    // spatial moments
    double  m00, m10, m01, m20, m11, m02, m30, m21, m12, m03;
    // central moments
    double  mu20, mu11, mu02, mu30, mu21, mu12, mu03;
    // central normalized moments
    double  nu20, nu11, nu02, nu30, nu21, nu12, nu03;
};

    param  A raster image (single-channel, 8-bit or floating-point 2D array) or
    array: an array ( 1 \times N or N \times 1 ) of 2D points ( Point or
           Point2f )
                                param binaryImage:
           (For images only) If it is true, then all the non-zero image pixels
           are treated as 1’s

The function computes moments, up to the 3rd order, of a vector shape or a
rasterized shape. In case of a raster image, the spatial moments \texttt
{Moments::m}_{ji} are computed as:

m _{ji}= \sum _{x,y} \left ( array (x,y) * x^j * y^i
\right ),

the central moments \texttt{Moments::mu}_{ji} are computed as:

mu _{ji}= \sum _{x,y} \left ( array (x,y) * (x - \bar{x}
)^j * (y - \bar{y} )^i \right )

where (\bar{x}, \bar{y}) is the mass center:

\bar{x} = \frac{m_{10}}{m_{00}} , \; \bar{y} = \frac{\texttt
{m}_{01}}{m_{00}}

and the normalized central moments \texttt{Moments::nu}_{ij} are computed as:

nu _{ji}= \frac{mu_{ji}}{m_{00}^{(i+j)/2+1}} .

Note that mu_{00}=m_{00} , nu_{00}=1 nu_
{10}=mu_{10}=mu_{01}=mu_{10}=0 , hence the values
are not stored.

The moments of a contour are defined in the same way, but computed using
Green’s formula (see http://en.wikipedia.org/wiki/Green_theorem ), therefore,
because of a limited raster resolution, the moments computed for a contour will
be slightly different from the moments computed for the same contour
rasterized.

See also: contourArea() , arcLength()

===========================================================================
*cv-HuMoments*

void HuMoments(const Moments& moments, double h[7])~

    Calculates the seven Hu invariants.

    Parameters:   • moments – The input moments, computed with moments()
                  • h – The output Hu invariants

The function calculates the seven Hu invariants, see http://en.wikipedia.org/
wiki/Image_moment , that are defined as:

\begin{array}{l} h[0]= \eta _{20}+ \eta _{02} \\ h[1]=( \eta _{20}- \eta _{02})
^{2}+4 \eta _{11}^{2} \\ h[2]=( \eta _{30}-3 \eta _{12})^{2}+ (3 \eta _{21}- \
eta _{03})^{2} \\ h[3]=( \eta _{30}+ \eta _{12})^{2}+ ( \eta _{21}+ \eta _{03})
^{2} \\ h[4]=( \eta _{30}-3 \eta _{12})( \eta _{30}+ \eta _{12})[( \eta _{30}+
\eta _{12})^{2}-3( \eta _{21}+ \eta _{03})^{2}]+(3 \eta _{21}- \eta _{03})( \
eta _{21}+ \eta _{03})[3( \eta _{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _{03})
^{2}] \\ h[5]=( \eta _{20}- \eta _{02})[( \eta _{30}+ \eta _{12})^{2}- ( \eta _
{21}+ \eta _{03})^{2}]+4 \eta _{11}( \eta _{30}+ \eta _{12})( \eta _{21}+ \eta
_{03}) \\ h[6]=(3 \eta _{21}- \eta _{03})( \eta _{21}+ \eta _{03})[3( \eta _
{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _{03})^{2}]-( \eta _{30}-3 \eta _{12})
( \eta _{21}+ \eta _{03})[3( \eta _{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _
{03})^{2}] \\ \end{array}

where \eta_{ji} stand for \texttt{Moments::nu}_{ji} .

These values are proved to be invariant to the image scale, rotation, and
reflection except the seventh one, whose sign is changed by reflection. Of
course, this invariance was proved with the assumption of infinite image
resolution. In case of a raster images the computed Hu invariants for the
original and transformed images will be a bit different.

See also: matchShapes()

===========================================================================
*cv-findContours*

void findContours(const Mat& image, vector<vector<Point> >& contours, vector
    <Vec4i>& hierarchy, int mode, int method, Point offset=Point())~

void findContours(const Mat& image, vector<vector<Point> >& contours, int mode,
    int method, Point offset=Point())

    Finds the contours in a binary image.

                  • image – The source, an 8-bit single-channel image. Non-zero
                    pixels are treated as 1’s, zero pixels remain 0’s - the
                    image is treated as binary . You can use compare() ,
                    inRange() , threshold() , adaptiveThreshold() , Canny()
                    etc. to create a binary image out of a grayscale or color
                    one. The function modifies the image while extracting the
                    contours
                  • contours – The detected contours. Each contour is stored as
                    a vector of points
                  • hiararchy – The optional output vector that will contain
                    information about the image topology. It will have as many
                    elements as the number of contours. For each contour
                    contours[i] , the elements hierarchy[i][0] , hiearchy[i][1]
                    , hiearchy[i][2] , hiearchy[i][3] will be set to 0-based
                    indices in contours of the next and previous contours at
                    the same hierarchical level, the first child contour and
                    the parent contour, respectively. If for some contour i
                    there is no next, previous, parent or nested contours, the
                    corresponding elements of hierarchy[i] will be negative
                  • mode –

                    The contour retrieval mode

                      □ CV_RETR_EXTERNAL retrieves only the extreme outer
                        contours; It will set hierarchy[i][2]=hierarchy[i][3]=
                        -1 for all the contours
                      □ CV_RETR_LIST retrieves all of the contours without
                        establishing any hierarchical relationships
    Parameters:       □ CV_RETR_CCOMP retrieves all of the contours and
                        organizes them into a two-level hierarchy: on the top
                        level are the external boundaries of the components, on
                        the second level are the boundaries of the holes. If
                        inside a hole of a connected component there is another
                        contour, it will still be put on the top level
                      □ CV_RETR_TREE retrieves all of the contours and
                        reconstructs the full hierarchy of nested contours.
                        This full hierarchy is built and shown in OpenCV
                        contours.c demo
                  • method –

                    The contour approximation method.

                      □ CV_CHAIN_APPROX_NONE stores absolutely all the contour
                        points. That is, every 2 points of a contour stored
                        with this method are 8-connected neighbors of each
                        other
                      □ CV_CHAIN_APPROX_SIMPLE compresses horizontal, vertical,
                        and diagonal segments and leaves only their end points.
                        E.g. an up-right rectangular contour will be encoded
                        with 4 points
                      □ CV_CHAIN_APPROX_TC89_L1,CV_CHAIN_APPROX_TC89_KCOS
                        applies one of the flavors of the Teh-Chin chain
                        approximation algorithm; see TehChin89
                  • offset – The optional offset, by which every contour point
                    is shifted. This is useful if the contours are extracted
                    from the image ROI and then they should be analyzed in the
                    whole image context

The function retrieves contours from the binary image using the algorithm
Suzuki85 . The contours are a useful tool for shape analysis and object
detection and recognition. See squares.c in the OpenCV sample directory.

Note: the source image is modified by this function.

===========================================================================
*cv-drawContours*

void drawContours(Mat& image, const vector<vector<Point> >& contours, int
    contourIdx, const Scalar& color, int thickness=1, int lineType=8, const
    vector<Vec4i>& hierarchy=vector<Vec4i>(), int maxLevel=INT_MAX, Point
    offset=Point())~

    Draws contours’ outlines or filled contours.

                  • image – The destination image
                  • contours – All the input contours. Each contour is stored
                    as a point vector
                  • contourIdx – Indicates the contour to draw. If it is
                    negative, all the contours are drawn
                  • color – The contours’ color
                  • thickness – Thickness of lines the contours are drawn with.
                    If it is negative (e.g. thickness=CV_FILLED ), the contour
                    interiors are drawn.
                  • lineType – The line connectivity; see line() description
    Parameters:   • hierarchy – The optional information about hierarchy. It is
                    only needed if you want to draw only some of the contours
                    (see maxLevel )
                  • maxLevel – Maximal level for drawn contours. If 0, only the
                    specified contour is drawn. If 1, the function draws the
                    contour(s) and all the nested contours. If 2, the function
                    draws the contours, all the nested contours and all the
                    nested into nested contours etc. This parameter is only
                    taken into account when there is hierarchy available.
                  • offset – The optional contour shift parameter. Shift all
                    the drawn contours by the specified offset=(dx,dy)

The function draws contour outlines in the image if thickness \ge 0 or
fills the area bounded by the contours if thickness<0 . Here is the
example on how to retrieve connected components from the binary image and label
them

#include "cv.h"
#include "highgui.h"

using namespace cv;

int main( int argc, char** argv )
{
    Mat src;
    // the first command line parameter must be file name of binary
    // (black-n-white) image
    if( argc != 2 || !(src=imread(argv[1], 0)).data)
        return -1;

    Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);

    src = src > 1;
    namedWindow( "Source", 1 );
    imshow( "Source", src );

    vector<vector<Point> > contours;
    vector<Vec4i> hierarchy;

    findContours( src, contours, hierarchy,
        CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE );

    // iterate through all the top-level contours,
    // draw each connected component with its own random color
    int idx = 0;
    for( ; idx >= 0; idx = hierarchy[idx][0] )
    {
        Scalar color( rand()&255, rand()&255, rand()&255 );
        drawContours( dst, contours, idx, color, CV_FILLED, 8, hierarchy );
    }

    namedWindow( "Components", 1 );
    imshow( "Components", dst );
    waitKey(0);
}

===========================================================================
*cv-approxPolyDP*

void approxPolyDP(const Mat& curve, vector<Point>& approxCurve, double epsilon,
    bool closed)~

void approxPolyDP(const Mat& curve, vector<Point2f>& approxCurve, double
    epsilon, bool closed)

    Approximates polygonal curve(s) with the specified precision.

                  • curve – The polygon or curve to approximate. Must be 1 \
                    times N or N \times 1 matrix of type CV_32SC2 or CV_32FC2 .
                    You can also convert vector<Point> or vector<Point2f to the
                    matrix by calling Mat(const vector<T>&) constructor.
                  • approxCurve – The result of the approximation; The type
    Parameters:     should match the type of the input curve
                  • epsilon – Specifies the approximation accuracy. This is the
                    maximum distance between the original curve and its
                    approximation
                  • closed – If true, the approximated curve is closed (i.e.
                    its first and last vertices are connected), otherwise it’s
                    not

The functions approxPolyDP approximate a curve or a polygon with another curve/
polygon with less vertices, so that the distance between them is less or equal
to the specified precision. It used Douglas-Peucker algorithm http://
en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm

===========================================================================
*cv-arcLength*

double arcLength(const Mat& curve, bool closed)~

    Calculates a contour perimeter or a curve length.

                  • curve – The input vector of 2D points, represented by
                    CV_32SC2 or CV_32FC2 matrix, or by vector<Point> or vector
    Parameters:     <Point2f> converted to a matrix with Mat(const vector<T>&)
                    constructor
                  • closed – Indicates, whether the curve is closed or not

The function computes the curve length or the closed contour perimeter.

===========================================================================
*cv-boundingRect*

Rect boundingRect(const Mat& points)~

    Calculates the up-right bounding rectangle of a point set.

               points – The input 2D point set, represented by CV_32SC2 or
    Parameter: CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
               converted to the matrix using Mat(const vector<T>&) constructor.

The function calculates and returns the minimal up-right bounding rectangle for
the specified point set.

===========================================================================
*cv-estimateRigidTransform*

Mat estimateRigidTransform(const Mat& srcpt, const Mat& dstpt, bool fullAffine)
    ~

    Computes optimal affine transformation between two 2D point sets

                  • srcpt – The first input 2D point set
                  • dst – The second input 2D point set of the same size and
                    the same type as A
                  • fullAffine – If true, the function finds the optimal affine
    Parameters:     transformation with no any additional resrictions (i.e.
                    there are 6 degrees of freedom); otherwise, the class of
                    transformations to choose from is limited to combinations
                    of translation, rotation and uniform scaling (i.e. there
                    are 5 degrees of freedom)

The function finds the optimal affine transform [A|b] (a 2 \times 3
floating-point matrix) that approximates best the transformation from \texttt
{srcpt}_i to dstpt_i :

[A^*|b^*] = arg \min _{[A|b]} \sum _i \| dstpt _i - A { srcpt
_i}^T - b \| ^2

where [A|b] can be either arbitrary (when fullAffine=true ) or have form

\begin{bmatrix} a_{11} & a_{12} & b_1 \\ -a_{12} & a_{11} & b_2 \end{bmatrix}

when fullAffine=false .

See also: getAffineTransform() , getPerspectiveTransform() , findHomography()

===========================================================================
*cv-estimateAffine3D*

int estimateAffine3D(const Mat& srcpt, const Mat& dstpt, Mat& out, vector<uchar
    >& outliers, double ransacThreshold = 3.0, double confidence = 0.99)~

    Computes optimal affine transformation between two 3D point sets

                  • srcpt – The first input 3D point set
                  • dstpt – The second input 3D point set
                  • out – The output 3D affine transformation matrix 3 \times 4
                  • outliers – The output vector indicating which points are
    Parameters:     outliers
                  • ransacThreshold – The maximum reprojection error in RANSAC
                    algorithm to consider a point an inlier
                  • confidence – The confidence level, between 0 and 1, with
                    which the matrix is estimated

The function estimates the optimal 3D affine transformation between two 3D
point sets using RANSAC algorithm.

===========================================================================
*cv-contourArea*

double contourArea(const Mat& contour)~

    Calculates the contour area

               contour – The contour vertices, represented by CV_32SC2 or
    Parameter: CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
               converted to the matrix using Mat(const vector<T>&) constructor.

The function computes the contour area. Similarly to moments() the area is
computed using the Green formula, thus the returned area and the number of
non-zero pixels, if you draw the contour using drawContours() or fillPoly() ,
can be different. Here is a short example:

vector<Point> contour;
contour.push_back(Point2f(0, 0));
contour.push_back(Point2f(10, 0));
contour.push_back(Point2f(10, 10));
contour.push_back(Point2f(5, 4));

double area0 = contourArea(contour);
vector<Point> approx;
approxPolyDP(contour, approx, 5, true);
double area1 = contourArea(approx);

cout << "area0 =" << area0 << endl <<
        "area1 =" << area1 << endl <<
        "approx poly vertices" << approx.size() << endl;

===========================================================================
*cv-convexHull*

void convexHull(const Mat& points, vector<int>& hull, bool clockwise=false)~

void convexHull(const Mat& points, vector<Point>& hull, bool clockwise=false)

void convexHull(const Mat& points, vector<Point2f>& hull, bool clockwise=false)

    Finds the convex hull of a point set.

                  • points – The input 2D point set, represented by CV_32SC2 or
                    CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
                    converted to the matrix using Mat(const vector<T>&)
                    constructor.
                  • hull – The output convex hull. It is either a vector of
                    points that form the hull (must have the same type as the
    Parameters:     input points), or a vector of 0-based point indices of the
                    hull points in the original array (since the set of convex
                    hull points is a subset of the original point set).
                  • clockwise – If true, the output convex hull will be
                    oriented clockwise, otherwise it will be oriented
                    counter-clockwise. Here, the usual screen coordinate system
                    is assumed - the origin is at the top-left corner, x axis
                    is oriented to the right, and y axis is oriented downwards.

The functions find the convex hull of a 2D point set using Sklansky’s algorithm
Sklansky82 that has O(N logN) or O(N) complexity (where N is the number of
input points), depending on how the initial sorting is implemented (currently
it is O(N logN) . See the OpenCV sample convexhull.c that demonstrates the use
of the different function variants.

===========================================================================
*cv-fitEllipse*

RotatedRect fitEllipse(const Mat& points)~

    Fits an ellipse around a set of 2D points.

               points – The input 2D point set, represented by CV_32SC2 or
    Parameter: CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
               converted to the matrix using Mat(const vector<T>&) constructor.

The function calculates the ellipse that fits best (in least-squares sense) a
set of 2D points. It returns the rotated rectangle in which the ellipse is
inscribed.

===========================================================================
*cv-fitLine*

void fitLine(const Mat& points, Vec4f& line, int distType, double param, double
    reps, double aeps)~

void fitLine(const Mat& points, Vec6f& line, int distType, double param, double
    reps, double aeps)

    Fits a line to a 2D or 3D point set.

                  • points – The input 2D point set, represented by CV_32SC2 or
                    CV_32FC2 matrix, or by vector<Point> , vector<Point2f> ,
                    vector<Point3i> or vector<Point3f> converted to the matrix
                    by Mat(const vector<T>&) constructor
                  • line –

                    The output line parameters. In the case of a 2d fitting, it
                    is a vector of 4 floats ``(vx, vy,

                        x0, y0)`` where (vx, vy) is a normalized vector
                        collinear to the
    Parameters:
                    line and (x0, y0) is some point on the line. in the case of
                    a 3D fitting it is vector of 6 floats (vx, vy, vz, x0, y0,
                    z0) where (vx, vy, vz) is a normalized vector collinear to
                    the line and (x0, y0, z0) is some point on the line

                  • distType – The distance used by the M-estimator (see the
                    discussion)
                  • param – Numerical parameter ( C ) for some types of
                    distances, if 0 then some optimal value is chosen
                  • reps, aeps – Sufficient accuracy for the radius (distance
                    between the coordinate origin and the line) and angle,
                    respectively; 0.01 would be a good default value for both.

The functions fitLine fit a line to a 2D or 3D point set by minimizing \sum_i \
rho(r_i) where r_i is the distance between the i^{th} point and the line and \
rho(r) is a distance function, one of:

  • distType=CV_DIST_L2

        \rho (r) = r^2/2 \quad \text{(the simplest and the fastest
        least-squares method)}

  • distType=CV_DIST_L1

        \rho (r) = r

  • distType=CV_DIST_L12

        \rho (r) = 2 * ( \sqrt{1 + \frac{r^2}{2}} - 1)

  • distType=CV_DIST_FAIR

        \rho \left (r \right ) = C^2 * \left ( \frac{r}{C} - \log{\left(1 +
        \frac{r}{C}\right)} \right ) \quad \text{where} \quad C=1.3998

  • distType=CV_DIST_WELSCH

        \rho \left (r \right ) = \frac{C^2}{2} * \left ( 1 - \exp{\left(-\
        left(\frac{r}{C}\right)^2\right)} \right ) \quad \text{where} \quad C=
        2.9846

  • distType=CV_DIST_HUBER

        \rho (r) = \fork{r^2/2}{if $r < C$}{C * (r-C/2)}{otherwise} \quad \
        text{where} \quad C=1.345

The algorithm is based on the M-estimator ( http://en.wikipedia.org/wiki/
M-estimator ) technique, that iteratively fits the line using weighted
least-squares algorithm and after each iteration the weights w_i are adjusted
to beinversely proportional to \rho(r_i) .

===========================================================================
*cv-isContourConvex*

bool isContourConvex(const Mat& contour)~

    Tests contour convexity.

               contour – The tested contour, a matrix of type CV_32SC2 or
    Parameter: CV_32FC2 , or vector<Point> or vector<Point2f> converted to the
               matrix using Mat(const vector<T>&) constructor.

The function tests whether the input contour is convex or not. The contour must
be simple, i.e. without self-intersections, otherwise the function output is
undefined.

===========================================================================
*cv-minAreaRect*

RotatedRect minAreaRect(const Mat& points)~

    Finds the minimum area rotated rectangle enclosing a 2D point set.

               points – The input 2D point set, represented by CV_32SC2 or
    Parameter: CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
               converted to the matrix using Mat(const vector<T>&) constructor.

The function calculates and returns the minimum area bounding rectangle
(possibly rotated) for the specified point set. See the OpenCV sample minarea.c

===========================================================================
*cv-minEnclosingCircle*

void minEnclosingCircle(const Mat& points, Point2f& center, float& radius)~

    Finds the minimum area circle enclosing a 2D point set.

                  • points – The input 2D point set, represented by CV_32SC2 or
                    CV_32FC2 matrix, or by vector<Point> or vector<Point2f>
    Parameters:     converted to the matrix using Mat(const vector<T>&)
                    constructor.
                  • center – The output center of the circle
                  • radius – The output radius of the circle

The function finds the minimal enclosing circle of a 2D point set using
iterative algorithm. See the OpenCV sample minarea.c

===========================================================================
*cv-matchShapes*

double matchShapes(const Mat& object1, const Mat& object2, int method, double
    parameter=0)~

    Compares two shapes.

                  • object1 – The first contour or grayscale image
                  • object2 – The second contour or grayscale image
                  • method –

    Parameters:     Comparison method:
                        CV_CONTOUR_MATCH_I1 , CV_CONTOURS_MATCH_I2
                    or
                        CV_CONTOURS_MATCH_I3 (see the discussion below)

                  • parameter – Method-specific parameter (is not used now)

The function compares two shapes. The 3 implemented methods all use Hu
invariants (see HuMoments() ) as following ( A denotes object1 , B denotes
object2 ):

  • method=CV_CONTOUR_MATCH_I1

        I_1(A,B) = \sum _{i=1...7} \left | \frac{1}{m^A_i} - \frac{1}{m^B_i} \
        right |

  • method=CV_CONTOUR_MATCH_I2

        I_2(A,B) = \sum _{i=1...7} \left | m^A_i - m^B_i \right |

  • method=CV_CONTOUR_MATCH_I3

        I_3(A,B) = \sum _{i=1...7} \frac{ \left| m^A_i - m^B_i \right| }{ \left
        | m^A_i \right| }

where

\begin{array}{l} m^A_i = sign (h^A_i) * \log{h^A_i} \\ m^B_i = \
mathrm{sign} (h^B_i) * \log{h^B_i} \end{array}

and h^A_i, h^B_i are the Hu moments of A and B respectively.

===========================================================================
*cv-pointPolygonTest*

double pointPolygonTest(const Mat& contour, Point2f pt, bool measureDist)~

    Performs point-in-contour test.

                  • contour – The input contour
                  • pt – The point tested against the contour
    Parameters:   • measureDist – If true, the function estimates the signed
                    distance from the point to the nearest contour edge;
                    otherwise, the function only checks if the point is inside
                    or not.

The function determines whether the point is inside a contour, outside, or lies
on an edge (or coincides with a vertex). It returns positive (inside), negative
(outside) or zero (on an edge) value, correspondingly. When measureDist=false ,
the return value is +1, -1 and 0, respectively. Otherwise, the return value it
is a signed distance between the point and the nearest contour edge.

Here is the sample output of the function, where each image pixel is tested
against the contour.

_images/pointpolygon.png

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Structural Analysis and Shape Descriptors
      □ cv::moments
      □ cv::HuMoments
      □ cv::findContours
      □ cv::drawContours
      □ cv::approxPolyDP
      □ cv::arcLength
      □ cv::boundingRect
      □ cv::estimateRigidTransform
      □ cv::estimateAffine3D
      □ cv::contourArea
      □ cv::convexHull
      □ cv::fitEllipse
      □ cv::fitLine
      □ cv::isContourConvex
      □ cv::minAreaRect
      □ cv::minEnclosingCircle
      □ cv::matchShapes
      □ cv::pointPolygonTest

Previous topic

Miscellaneous Image Transformations

Next topic

Planar Subdivisions

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Planar Subdivisions~

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Previous topic

Structural Analysis and Shape Descriptors

Next topic

Motion Analysis and Object Tracking

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Motion Analysis and Object Tracking~

===========================================================================
*cv-accumulate*

void accumulate(const Mat& src, Mat& dst, const Mat& mask=Mat())~

    Adds image to the accumulator.

                  • src – The input image, 1- or 3-channel, 8-bit or 32-bit
                    floating point
    Parameters:   • dst – The accumulator image with the same number of
                    channels as input image, 32-bit or 64-bit floating-point
                  • mask – Optional operation mask

The function adds src , or some of its elements, to dst :

dst (x,y) \leftarrow dst (x,y) + src (x,y) \quad \
text{if} \quad mask (x,y) \ne 0

The function supports multi-channel images; each channel is processed
independently.

The functions accumulate* can be used, for example, to collect statistic of
background of a scene, viewed by a still camera, for the further
foreground-background segmentation.

See also: accumulateSquare() , accumulateProduct() , accumulateWeighted()

===========================================================================
*cv-accumulateSquare*

void accumulateSquare(const Mat& src, Mat& dst, const Mat& mask=Mat())~

    Adds the square of the source image to the accumulator.

                  • src – The input image, 1- or 3-channel, 8-bit or 32-bit
                    floating point
    Parameters:   • dst – The accumulator image with the same number of
                    channels as input image, 32-bit or 64-bit floating-point
                  • mask – Optional operation mask

The function adds the input image src or its selected region, raised to power
2, to the accumulator dst :

dst (x,y) \leftarrow dst (x,y) + src (x,y)^2 \quad \
text{if} \quad mask (x,y) \ne 0

The function supports multi-channel images; each channel is processed
independently.

See also: accumulateSquare() , accumulateProduct() , accumulateWeighted()

===========================================================================
*cv-accumulateProduct*

void accumulateProduct(const Mat& src1, const Mat& src2, Mat& dst, const Mat&
    mask=Mat())~

    Adds the per-element product of two input images to the accumulator.

                  • src1 – The first input image, 1- or 3-channel, 8-bit or
                    32-bit floating point
                  • src2 – The second input image of the same type and the same
    Parameters:     size as src1
                  • dst – Accumulator with the same number of channels as input
                    images, 32-bit or 64-bit floating-point
                  • mask – Optional operation mask

The function adds the product of 2 images or their selected regions to the
accumulator dst :

dst (x,y) \leftarrow dst (x,y) + src1 (x,y) * \
texttt{src2} (x,y) \quad \text{if} \quad mask (x,y) \ne 0

The function supports multi-channel images; each channel is processed
independently.

See also: accumulate() , accumulateSquare() , accumulateWeighted()

===========================================================================
*cv-accumulateWeighted*

void accumulateWeighted(const Mat& src, Mat& dst, double alpha, const Mat& mask
    =Mat())~

    Updates the running average.

                  • src – The input image, 1- or 3-channel, 8-bit or 32-bit
                    floating point
    Parameters:   • dst – The accumulator image with the same number of
                    channels as input image, 32-bit or 64-bit floating-point
                  • alpha – Weight of the input image
                  • mask – Optional operation mask

The function calculates the weighted sum of the input image src and the
accumulator dst so that dst becomes a running average of frame sequence:

dst (x,y) \leftarrow (1- alpha ) * dst (x,y) + \
texttt{alpha} * src (x,y) \quad \text{if} \quad mask
(x,y) \ne 0

that is, alpha regulates the update speed (how fast the accumulator “forgets”
about earlier images). The function supports multi-channel images; each channel
is processed independently.

See also: accumulate() , accumulateSquare() , accumulateProduct()

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Motion Analysis and Object Tracking
      □ cv::accumulate
      □ cv::accumulateSquare
      □ cv::accumulateProduct
      □ cv::accumulateWeighted

Previous topic

Planar Subdivisions

Next topic

Feature Detection

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Feature Detection~

===========================================================================
*cv-Canny*

void Canny(const Mat& image, Mat& edges, double threshold1, double threshold2,
    int apertureSize=3, bool L2gradient=false)~

    Finds edges in an image using Canny algorithm.

                  • image – Single-channel 8-bit input image
                  • edges – The output edge map. It will have the same size and
                    the same type as image
                  • threshold1 – The first threshold for the hysteresis
                    procedure
                  • threshold2 – The second threshold for the hysteresis
    Parameters:     procedure
                  • apertureSize – Aperture size for the Sobel() operator
                  • L2gradient – Indicates, whether the more accurate L_2 norm
                    =\sqrt{(dI/dx)^2 + (dI/dy)^2} should be used to compute the
                    image gradient magnitude ( L2gradient=true ), or a faster
                    default L_1 norm =|dI/dx|+|dI/dy| is enough ( L2gradient=
                    false )

The function finds edges in the input image image and marks them in the output
map edges using the Canny algorithm. The smallest value between threshold1 and
threshold2 is used for edge linking, the largest value is used to find the
initial segments of strong edges, see http://en.wikipedia.org/wiki/
Canny_edge_detector

===========================================================================
*cv-cornerEigenValsAndVecs*

void cornerEigenValsAndVecs(const Mat& src, Mat& dst, int blockSize, int
    apertureSize, int borderType=BORDER_DEFAULT)~

    Calculates eigenvalues and eigenvectors of image blocks for corner
    detection.

                  • src – Input single-channel 8-bit or floating-point image
                  • dst – Image to store the results. It will have the same
                    size as src and the type CV_32FC(6)
    Parameters:   • blockSize – Neighborhood size (see discussion)
                  • apertureSize – Aperture parameter for the Sobel() operator
                  • boderType – Pixel extrapolation method; see
                    borderInterpolate()

For every pixel p , the function cornerEigenValsAndVecs considers a blockSize \
times blockSize neigborhood S(p) . It calculates the covariation matrix of
derivatives over the neighborhood as:

M = \begin{bmatrix} \sum _{S(p)}(dI/dx)^2 & \sum _{S(p)}(dI/dx dI/dy)^2 \\ \sum
_{S(p)}(dI/dx dI/dy)^2 & \sum _{S(p)}(dI/dy)^2 \end{bmatrix}

Where the derivatives are computed using Sobel() operator.

After that it finds eigenvectors and eigenvalues of M and stores them into
destination image in the form (\lambda_1, \lambda_2, x_1, y_1, x_2, y_2) where

  • 
    \lambda_1, \lambda_2

        are the eigenvalues of M ; not sorted

  • 
    x_1, y_1

        are the eigenvectors corresponding to \lambda_1

  • 
    x_2, y_2

        are the eigenvectors corresponding to \lambda_2

The output of the function can be used for robust edge or corner detection.

See also: cornerMinEigenVal() , cornerHarris() , preCornerDetect()

===========================================================================
*cv-cornerHarris*

void cornerHarris(const Mat& src, Mat& dst, int blockSize, int apertureSize,
    double k, int borderType=BORDER_DEFAULT)~

    Harris edge detector.

                  • src – Input single-channel 8-bit or floating-point image
                  • dst – Image to store the Harris detector responses; will
                    have type CV_32FC1 and the same size as src
                  • blockSize – Neighborhood size (see the discussion of
    Parameters:     cornerEigenValsAndVecs() )
                  • apertureSize – Aperture parameter for the Sobel() operator
                  • k – Harris detector free parameter. See the formula below
                  • boderType – Pixel extrapolation method; see
                    borderInterpolate()

The function runs the Harris edge detector on the image. Similarly to
cornerMinEigenVal() and cornerEigenValsAndVecs() , for each pixel (x, y) it
calculates a 2\times2 gradient covariation matrix M^{(x,y)} over a \texttt
{blockSize} \times blockSize neighborhood. Then, it computes the
following characteristic:

dst (x,y) = det M^{(x,y)} - k * \left ( tr M^
{(x,y)} \right )^2

Corners in the image can be found as the local maxima of this response map.

===========================================================================
*cv-cornerMinEigenVal*

void cornerMinEigenVal(const Mat& src, Mat& dst, int blockSize, int
    apertureSize=3, int borderType=BORDER_DEFAULT)~

    Calculates the minimal eigenvalue of gradient matrices for corner
    detection.

                  • src – Input single-channel 8-bit or floating-point image
                  • dst – Image to store the minimal eigenvalues; will have
                    type CV_32FC1 and the same size as src
    Parameters:   • blockSize – Neighborhood size (see the discussion of
                    cornerEigenValsAndVecs() )
                  • apertureSize – Aperture parameter for the Sobel() operator
                  • boderType – Pixel extrapolation method; see
                    borderInterpolate()

The function is similar to cornerEigenValsAndVecs() but it calculates and
stores only the minimal eigenvalue of the covariation matrix of derivatives,
i.e. \min(\lambda_1, \lambda_2) in terms of the formulae in
cornerEigenValsAndVecs() description.

===========================================================================
*cv-cornerSubPix*

void cornerSubPix(const Mat& image, vector<Point2f>& corners, Size winSize,
    Size zeroZone, TermCriteria criteria)~

    Refines the corner locations.

                  • image – Input image
                  • corners – Initial coordinates of the input corners; refined
                    coordinates on output
                  • winSize – Half of the side length of the search window. For
                    example, if winSize=Size(5,5) , then a 5*2+1 \times 5*2+1 =
                    11 \times 11 search window would be used
                  • zeroZone – Half of the size of the dead region in the
                    middle of the search zone over which the summation in the
    Parameters:     formula below is not done. It is used sometimes to avoid
                    possible singularities of the autocorrelation matrix. The
                    value of (-1,-1) indicates that there is no such size
                  • criteria – Criteria for termination of the iterative
                    process of corner refinement. That is, the process of
                    corner position refinement stops either after a certain
                    number of iterations or when a required accuracy is
                    achieved. The criteria may specify either of or both the
                    maximum number of iteration and the required accuracy

The function iterates to find the sub-pixel accurate location of corners, or
radial saddle points, as shown in on the picture below.

_images/cornersubpix.png

Sub-pixel accurate corner locator is based on the observation that every vector
from the center q to a point p located within a neighborhood of q is orthogonal
to the image gradient at p subject to image and measurement noise. Consider the
expression:

\epsilon _i = {DI_{p_i}}^T * (q - p_i)

where {DI_{p_i}} is the image gradient at the one of the points p_i in a
neighborhood of q . The value of q is to be found such that \epsilon_i is
minimized. A system of equations may be set up with \epsilon_i set to zero:

\sum _i(DI_{p_i} * {DI_{p_i}}^T) - \sum _i(DI_{p_i} * {DI_{p_i}}^T \
cdot p_i)

where the gradients are summed within a neighborhood (“search window”) of q .
Calling the first gradient term G and the second gradient term b gives:

q = G^{-1} * b

The algorithm sets the center of the neighborhood window at this new center q
and then iterates until the center keeps within a set threshold.

===========================================================================
*cv-goodFeaturesToTrack*

void goodFeaturesToTrack(const Mat& image, vector<Point2f>& corners, int
    maxCorners, double qualityLevel, double minDistance, const Mat& mask=Mat(),
    int blockSize=3, bool useHarrisDetector=false, double k=0.04)~

    Determines strong corners on an image.

                  • image – The input 8-bit or floating-point 32-bit,
                    single-channel image
                  • corners – The output vector of detected corners
                  • maxCorners – The maximum number of corners to return. If
                    there are more corners than that will be found, the
                    strongest of them will be returned
                  • qualityLevel – Characterizes the minimal accepted quality
                    of image corners; the value of the parameter is multiplied
                    by the by the best corner quality measure (which is the min
                    eigenvalue, see cornerMinEigenVal() , or the Harris
                    function response, see cornerHarris() ). The corners, which
                    quality measure is less than the product, will be rejected.
                    For example, if the best corner has the quality measure =
    Parameters:     1500, and the qualityLevel=0.01 , then all the corners
                    which quality measure is less than 15 will be rejected.
                  • minDistance – The minimum possible Euclidean distance
                    between the returned corners
                  • mask – The optional region of interest. If the image is not
                    empty (then it needs to have the type CV_8UC1 and the same
                    size as image ), it will specify the region in which the
                    corners are detected
                  • blockSize – Size of the averaging block for computing
                    derivative covariation matrix over each pixel neighborhood,
                    see cornerEigenValsAndVecs()
                  • useHarrisDetector – Indicates, whether to use operator or
                    cornerMinEigenVal()
                  • k – Free parameter of Harris detector

The function finds the most prominent corners in the image or in the specified
image region, as described in Shi94 :

 1. the function first calculates the corner quality measure at every source
    image pixel using the cornerMinEigenVal() or cornerHarris()
 2. then it performs non-maxima suppression (the local maxima in 3\times 3
    neighborhood are retained).
 3. the next step rejects the corners with the minimal eigenvalue less than \
    texttt{qualityLevel} * \max_{x,y} qualityMeasureMap(x,y) .
 4. the remaining corners are then sorted by the quality measure in the
    descending order.
 5. finally, the function throws away each corner pt_j if there is a stronger
    corner pt_i ( i < j ) such that the distance between them is less than
    minDistance

The function can be used to initialize a point-based tracker of an object.

Note that the if the function is called with different values A and B of the
parameter qualityLevel , and A > {B}, the vector of returned corners with
qualityLevel=A will be the prefix of the output vector with qualityLevel=B .

See also: cornerMinEigenVal() , cornerHarris() , calcOpticalFlowPyrLK() ,
estimateRigidMotion() , PlanarObjectDetector() , OneWayDescriptor()

===========================================================================
*cv-HoughCircles*

void HoughCircles(Mat& image, vector<Vec3f>& circles, int method, double dp,
    double minDist, double param1=100, double param2=100, int minRadius=0, int
    maxRadius=0)~

    Finds circles in a grayscale image using a Hough transform.

                  • image – The 8-bit, single-channel, grayscale input image
                  • circles – The output vector of found circles. Each vector
                    is encoded as 3-element floating-point vector (x, y,
                    radius)
                  • method – Currently, the only implemented method is
                    CV_HOUGH_GRADIENT , which is basically 21HT , described in
                    Yuen90 .
                  • dp – The inverse ratio of the accumulator resolution to the
                    image resolution. For example, if dp=1 , the accumulator
                    will have the same resolution as the input image, if dp=2 -
                    accumulator will have half as big width and height, etc
                  • minDist – Minimum distance between the centers of the
    Parameters:     detected circles. If the parameter is too small, multiple
                    neighbor circles may be falsely detected in addition to a
                    true one. If it is too large, some circles may be missed
                  • param1 – The first method-specific parameter. in the case
                    of CV_HOUGH_GRADIENT it is the higher threshold of the two
                    passed to Canny() edge detector (the lower one will be
                    twice smaller)
                  • param2 – The second method-specific parameter. in the case
                    of CV_HOUGH_GRADIENT it is the accumulator threshold at the
                    center detection stage. The smaller it is, the more false
                    circles may be detected. Circles, corresponding to the
                    larger accumulator values, will be returned first
                  • minRadius – Minimum circle radius
                  • maxRadius – Maximum circle radius

The function finds circles in a grayscale image using some modification of
Hough transform. Here is a short usage example:

#include <cv.h>
#include <highgui.h>
#include <math.h>

using namespace cv;

int main(int argc, char** argv)
{
    Mat img, gray;
    if( argc != 2 && !(img=imread(argv[1], 1)).data)
        return -1;
    cvtColor(img, gray, CV_BGR2GRAY);
    // smooth it, otherwise a lot of false circles may be detected
    GaussianBlur( gray, gray, Size(9, 9), 2, 2 );
    vector<Vec3f> circles;
    HoughCircles(gray, circles, CV_HOUGH_GRADIENT,
                 2, gray->rows/4, 200, 100 );
    for( size_t i = 0; i < circles.size(); i++ )
    {
         Point center(cvRound(circles[i][0]), cvRound(circles[i][1]));
         int radius = cvRound(circles[i][2]);
         // draw the circle center
         circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );
         // draw the circle outline
         circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 );
    }
    namedWindow( "circles", 1 );
    imshow( "circles", img );
    return 0;
}

Note that usually the function detects the circles’ centers well, however it
may fail to find the correct radii. You can assist the function by specifying
the radius range ( minRadius and maxRadius ) if you know it, or you may ignore
the returned radius, use only the center and find the correct radius using some
additional procedure.

See also: fitEllipse() , minEnclosingCircle()

===========================================================================
*cv-HoughLines*

void HoughLines(Mat& image, vector<Vec2f>& lines, double rho, double theta, int
    threshold, double srn=0, double stn=0)~

    Finds lines in a binary image using standard Hough transform.

                  • image – The 8-bit, single-channel, binary source image. The
                    image may be modified by the function
                  • lines – The output vector of lines. Each line is
                    represented by a two-element vector (\rho, \theta) . \rho
                    is the distance from the coordinate origin (0,0) (top-left
                    corner of the image) and \theta is the line rotation angle
                    in radians ( 0 \sim \textrm{vertical line}, \pi/2 \sim \
                    textrm{horizontal line} )
                  • rho – Distance resolution of the accumulator in pixels
                  • theta – Angle resolution of the accumulator in radians
    Parameters:   • threshold – The accumulator threshold parameter. Only those
                    lines are returned that get enough votes ( >\texttt
                    {threshold} )
                  • srn – For the multi-scale Hough transform it is the divisor
                    for the distance resolution rho . The coarse accumulator
                    distance resolution will be rho and the accurate
                    accumulator resolution will be rho/srn . If both srn=0 and
                    stn=0 then the classical Hough transform is used, otherwise
                    both these parameters should be positive.
                  • stn – For the multi-scale Hough transform it is the divisor
                    for the distance resolution theta

The function implements standard or standard multi-scale Hough transform
algorithm for line detection. See HoughLinesP() for the code example.

===========================================================================
*cv-HoughLinesP*

void HoughLinesP(Mat& image, vector<Vec4i>& lines, double rho, double theta,
    int threshold, double minLineLength=0, double maxLineGap=0)~

    Finds lines segments in a binary image using probabilistic Hough transform.

                  • image – The 8-bit, single-channel, binary source image. The
                    image may be modified by the function
                  • lines – The output vector of lines. Each line is
                    represented by a 4-element vector (x_1, y_1, x_2, y_2) ,
                    where (x_1,y_1) and (x_2, y_2) are the ending points of
                    each line segment detected.
                  • rho – Distance resolution of the accumulator in pixels
    Parameters:   • theta – Angle resolution of the accumulator in radians
                  • threshold – The accumulator threshold parameter. Only those
                    lines are returned that get enough votes ( >\texttt
                    {threshold} )
                  • minLineLength – The minimum line length. Line segments
                    shorter than that will be rejected
                  • maxLineGap – The maximum allowed gap between points on the
                    same line to link them.

The function implements probabilistic Hough transform algorithm for line
detection, described in Matas00 . Below is line detection example:

/* This is a standalone program. Pass an image name as a first parameter
of the program.  Switch between standard and probabilistic Hough transform
by changing "#if 1" to "#if 0" and back */
#include <cv.h>
#include <highgui.h>
#include <math.h>

using namespace cv;

int main(int argc, char** argv)
{
    Mat src, dst, color_dst;
    if( argc != 2 || !(src=imread(argv[1], 0)).data)
        return -1;

    Canny( src, dst, 50, 200, 3 );
    cvtColor( dst, color_dst, CV_GRAY2BGR );

#if 0
    vector<Vec2f> lines;
    HoughLines( dst, lines, 1, CV_PI/180, 100 );

    for( size_t i = 0; i < lines.size(); i++ )
    {
        float rho = lines[i][0];
        float theta = lines[i][1];
        double a = cos(theta), b = sin(theta);
        double x0 = a*rho, y0 = b*rho;
        Point pt1(cvRound(x0 + 1000*(-b)),
                  cvRound(y0 + 1000*(a)));
        Point pt2(cvRound(x0 - 1000*(-b)),
                  cvRound(y0 - 1000*(a)));
        line( color_dst, pt1, pt2, Scalar(0,0,255), 3, 8 );
    }
#else
    vector<Vec4i> lines;
    HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 );
    for( size_t i = 0; i < lines.size(); i++ )
    {
        line( color_dst, Point(lines[i][0], lines[i][1]),
            Point(lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 );
    }
#endif
    namedWindow( "Source", 1 );
    imshow( "Source", src );

    namedWindow( "Detected Lines", 1 );
    imshow( "Detected Lines", color_dst );

    waitKey(0);
    return 0;
}

This is the sample picture the function parameters have been tuned for:

_images/building.jpg

And this is the output of the above program in the case of probabilistic Hough
transform

_images/houghp.png

===========================================================================
*cv-preCornerDetect*

void preCornerDetect(const Mat& src, Mat& dst, int apertureSize, int borderType
    =BORDER_DEFAULT)~

    Calculates the feature map for corner detection

                  • src – The source single-channel 8-bit of floating-point
                    image
                  • dst – The output image; will have type CV_32F and the same
    Parameters:     size as src
                  • apertureSize – Aperture size of Sobel()
                  • borderType – The pixel extrapolation method; see
                    borderInterpolate()

The function calculates the complex spatial derivative-based function of the
source image

dst = (D_x src )^2 * D_{yy} src + (D_y \texttt
{src} )^2 * D_{xx} src - 2 D_x src * D_y src
* D_{xy} src

where D_x , D_y are the first image derivatives, D_{xx} , D_{yy} are the second
image derivatives and D_{xy} is the mixed derivative.

The corners can be found as local maximums of the functions, as shown below:

Mat corners, dilated_corners;
preCornerDetect(image, corners, 3);
// dilation with 3x3 rectangular structuring element
dilate(corners, dilated_corners, Mat(), 1);
Mat corner_mask = corners == dilated_corners;

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Feature Detection
      □ cv::Canny
      □ cv::cornerEigenValsAndVecs
      □ cv::cornerHarris
      □ cv::cornerMinEigenVal
      □ cv::cornerSubPix
      □ cv::goodFeaturesToTrack
      □ cv::HoughCircles
      □ cv::HoughLines
      □ cv::HoughLinesP
      □ cv::preCornerDetect

Previous topic

Motion Analysis and Object Tracking

Next topic

Object Detection

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.
Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

Object Detection~

===========================================================================
*cv-matchTemplate*

void matchTemplate(const Mat& image, const Mat& templ, Mat& result, int method)
    ~

    Compares a template against overlapped image regions.

                  • image – Image where the search is running; should be 8-bit
                    or 32-bit floating-point
                  • templ – Searched template; must be not greater than the
                    source image and have the same data type
    Parameters:   • result – A map of comparison results; will be
                    single-channel 32-bit floating-point. If image is W \times
                    H and templ is w \times h then result will be (W-w+1) \
                    times (H-h+1)
                  • method – Specifies the comparison method (see below)

The function slides through image , compares the overlapped patches of size w \
times h against templ using the specified method and stores the comparison
results to result . Here are the formulas for the available comparison methods
( I denotes image , T template , R result ). The summation is done over
template and/or the image patch: x' = 0...w-1, y' = 0...h-1

  • method=CV_TM_SQDIFF

        R(x,y)= \sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2

  • method=CV_TM_SQDIFF_NORMED

        R(x,y)= \frac{\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\sqrt{\sum_
        {x',y'}T(x',y')^2 * \sum_{x',y'} I(x+x',y+y')^2}}

  • method=CV_TM_CCORR

        R(x,y)= \sum _{x',y'} (T(x',y') * I(x+x',y+y'))

  • method=CV_TM_CCORR_NORMED

        R(x,y)= \frac{\sum_{x',y'} (T(x',y') * I'(x+x',y+y'))}{\sqrt{\sum_
        {x',y'}T(x',y')^2 * \sum_{x',y'} I(x+x',y+y')^2}}

  • method=CV_TM_CCOEFF

        R(x,y)= \sum _{x',y'} (T'(x',y') * I(x+x',y+y'))

        where

        \begin{array}{l} T'(x',y')=T(x',y') - 1/(w * h) * \sum _
        {x'',y''} T(x'',y'') \\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w * h) \
        cdot \sum _{x'',y''} I(x+x'',y+y'') \end{array}

  • method=CV_TM_CCOEFF_NORMED

        R(x,y)= \frac{ \sum_{x',y'} (T'(x',y') * I'(x+x',y+y')) }{ \sqrt{\
        sum_{x',y'}T'(x',y')^2 * \sum_{x',y'} I'(x+x',y+y')^2} }

After the function finishes the comparison, the best matches can be found as
global minimums (when CV_TM_SQDIFF was used) or maximums (when CV_TM_CCORR or
CV_TM_CCOEFF was used) using the minMaxLoc() function. In the case of a color
image, template summation in the numerator and each sum in the denominator is
done over all of the channels (and separate mean values are used for each
channel). That is, the function can take a color template and a color image;
the result will still be a single-channel image, which is easier to analyze.

Help and Feedback

You did not find what you were looking for?

  • Try the Cheatsheet.
  • Ask a question in the user group/mailing list.
  • If you think something is missing or wrong in the documentation, please
    file a bug report.

Logo

Table Of Contents

  • Object Detection
      □ cv::matchTemplate

Previous topic

Feature Detection

Next topic

features2d. Feature Detection and Descriptor Extraction

This Page

  • Show Source

Quick search

[                  ] [Go] 
Enter search terms or a module, class or function name.

Navigation

  • index
  • next |
  • previous |
  • opencv v2.1 documentation »
  • imgproc. Image Processing »

© Copyright 2010, authors. Created using Sphinx 0.6.2.


vim:tw=78:ts=4:ft=help:norl: